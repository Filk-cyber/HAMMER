{
  "metrics": {
    "weight_w": 1.0,
    "input_file": "/home/jiangjp/trace-idea/data/hotpotqa/hotpotqa_dev100ideal_with_reasoning_chains_modify_w10.json",
    "total_samples": 100,
    "exact_match": 0.43,
    "f1": 0.5218214285714285,
    "precision": 0.5215995670995671,
    "recall": 0.5386666666666666,
    "avg_max_log_prob": -7.536178683046756,
    "avg_max_prob": 0.3795294683874054,
    "avg_total_prob": 0.3795294683874054,
    "valid_max_prob_samples": 100,
    "invalid_max_prob_samples": 0,
    "avg_answer_count": 1.0,
    "avg_valid_answer_count": 1.0
  },
  "sample_info": [
    {
      "sample_idx": 0,
      "generated_answer": "Yes",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "no": 1.1829738745095463e-06
      },
      "individual_log_probs": {
        "no": -13.647479057312012
      },
      "max_prob": 1.1829738745095463e-06,
      "max_log_prob": -13.647479057312012,
      "best_answer": "no",
      "total_prob": 1.1829738745095463e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 1,
      "generated_answer": "Boston College.",
      "gold_answers": [
        "University of Delaware"
      ],
      "reference_answer": "University of Delaware",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "University of Delaware": 0.11690055622808034
      },
      "individual_log_probs": {
        "University of Delaware": -2.1464316523624802
      },
      "max_prob": 0.11690055622808034,
      "max_log_prob": -2.1464316523624802,
      "best_answer": "University of Delaware",
      "total_prob": 0.11690055622808034,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 2,
      "generated_answer": "ABC",
      "gold_answers": [
        "NBC"
      ],
      "reference_answer": "NBC",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "NBC": 8.576518632837114e-07
      },
      "individual_log_probs": {
        "NBC": -13.969067573547363
      },
      "max_prob": 8.576518632837114e-07,
      "max_log_prob": -13.969067573547363,
      "best_answer": "NBC",
      "total_prob": 8.576518632837114e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 3,
      "generated_answer": "Monocotyledonous flowering plants.",
      "gold_answers": [
        "genus"
      ],
      "reference_answer": "genus",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "genus": 0.0009072139951712214
      },
      "individual_log_probs": {
        "genus": -7.00513219833374
      },
      "max_prob": 0.0009072139951712214,
      "max_log_prob": -7.00513219833374,
      "best_answer": "genus",
      "total_prob": 0.0009072139951712214,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 4,
      "generated_answer": "Kirk Fogg",
      "gold_answers": [
        "Jeff Sutphen"
      ],
      "reference_answer": "Jeff Sutphen",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Jeff Sutphen": 0.05174029469797806
      },
      "individual_log_probs": {
        "Jeff Sutphen": -2.961518406492928
      },
      "max_prob": 0.05174029469797806,
      "max_log_prob": -2.961518406492928,
      "best_answer": "Jeff Sutphen",
      "total_prob": 0.05174029469797806,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 5,
      "generated_answer": "The Forest of Bowland.",
      "gold_answers": [
        "the Bowland Fells"
      ],
      "reference_answer": "the Bowland Fells",
      "em_score": 0.0,
      "f1_score": 0.4,
      "precision": 0.3333333333333333,
      "recall": 0.5,
      "individual_probs": {
        "the Bowland Fells": 4.142289430044097e-06
      },
      "individual_log_probs": {
        "the Bowland Fells": -12.394261920550434
      },
      "max_prob": 4.142289430044097e-06,
      "max_log_prob": -12.394261920550434,
      "best_answer": "the Bowland Fells",
      "total_prob": 4.142289430044097e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 6,
      "generated_answer": "Daft Punk",
      "gold_answers": [
        "Daft Punk"
      ],
      "reference_answer": "Daft Punk",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Daft Punk": 0.9997272013072596
      },
      "individual_log_probs": {
        "Daft Punk": -0.00027283590907245525
      },
      "max_prob": 0.9997272013072596,
      "max_log_prob": -0.00027283590907245525,
      "best_answer": "Daft Punk",
      "total_prob": 0.9997272013072596,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 7,
      "generated_answer": "3rd Rock from the Sun",
      "gold_answers": [
        "3rd Rock from the Sun"
      ],
      "reference_answer": "3rd Rock from the Sun",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "3rd Rock from the Sun": 0.9933453637743715
      },
      "individual_log_probs": {
        "3rd Rock from the Sun": -0.006676877041883245
      },
      "max_prob": 0.9933453637743715,
      "max_log_prob": -0.006676877041883245,
      "best_answer": "3rd Rock from the Sun",
      "total_prob": 0.9933453637743715,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 8,
      "generated_answer": "The Island (2005 film)",
      "gold_answers": [
        "The Island"
      ],
      "reference_answer": "The Island",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.3333333333333333,
      "recall": 1.0,
      "individual_probs": {
        "The Island": 0.706032725977095
      },
      "individual_log_probs": {
        "The Island": -0.3480936884880066
      },
      "max_prob": 0.706032725977095,
      "max_log_prob": -0.3480936884880066,
      "best_answer": "The Island",
      "total_prob": 0.706032725977095,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 9,
      "generated_answer": "The Great Mouse Detective",
      "gold_answers": [
        "The Great Mouse Detective"
      ],
      "reference_answer": "The Great Mouse Detective",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Great Mouse Detective": 0.9418620558257579
      },
      "individual_log_probs": {
        "The Great Mouse Detective": -0.05989645268107324
      },
      "max_prob": 0.9418620558257579,
      "max_log_prob": -0.05989645268107324,
      "best_answer": "The Great Mouse Detective",
      "total_prob": 0.9418620558257579,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 10,
      "generated_answer": "20 March to 1 May 2003",
      "gold_answers": [
        "20 March to 1 May 2003"
      ],
      "reference_answer": "20 March to 1 May 2003",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "20 March to 1 May 2003": 0.4766125899255349
      },
      "individual_log_probs": {
        "20 March to 1 May 2003": -0.7410512985406399
      },
      "max_prob": 0.4766125899255349,
      "max_log_prob": -0.7410512985406399,
      "best_answer": "20 March to 1 May 2003",
      "total_prob": 0.4766125899255349,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 11,
      "generated_answer": "A type of flowering plant.",
      "gold_answers": [
        "plant"
      ],
      "reference_answer": "plant",
      "em_score": 0.0,
      "f1_score": 0.4,
      "precision": 0.25,
      "recall": 1.0,
      "individual_probs": {
        "plant": 0.00046588988914400943
      },
      "individual_log_probs": {
        "plant": -7.671561241149902
      },
      "max_prob": 0.00046588988914400943,
      "max_log_prob": -7.671561241149902,
      "best_answer": "plant",
      "total_prob": 0.00046588988914400943,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 12,
      "generated_answer": "Montclair State University.",
      "gold_answers": [
        "Montclair State University"
      ],
      "reference_answer": "Montclair State University",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Montclair State University": 0.9994231873084419
      },
      "individual_log_probs": {
        "Montclair State University": -0.0005769791119973888
      },
      "max_prob": 0.9994231873084419,
      "max_log_prob": -0.0005769791119973888,
      "best_answer": "Montclair State University",
      "total_prob": 0.9994231873084419,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 13,
      "generated_answer": "Preity Zinta",
      "gold_answers": [
        "Soha Ali Khan"
      ],
      "reference_answer": "Soha Ali Khan",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Soha Ali Khan": 1.3205084939543588e-07
      },
      "individual_log_probs": {
        "Soha Ali Khan": -15.840078765846556
      },
      "max_prob": 1.3205084939543588e-07,
      "max_log_prob": -15.840078765846556,
      "best_answer": "Soha Ali Khan",
      "total_prob": 1.3205084939543588e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 14,
      "generated_answer": "No",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "yes": 6.411770721562124e-07
      },
      "individual_log_probs": {
        "yes": -14.259960174560547
      },
      "max_prob": 6.411770721562124e-07,
      "max_log_prob": -14.259960174560547,
      "best_answer": "yes",
      "total_prob": 6.411770721562124e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 15,
      "generated_answer": "Taejon Massacre",
      "gold_answers": [
        "Bodo League massacre"
      ],
      "reference_answer": "Bodo League massacre",
      "em_score": 0.0,
      "f1_score": 0.4,
      "precision": 0.5,
      "recall": 0.3333333333333333,
      "individual_probs": {
        "Bodo League massacre": 6.3031513544694364e-12
      },
      "individual_log_probs": {
        "Bodo League massacre": -25.78997139260173
      },
      "max_prob": 6.3031513544694364e-12,
      "max_log_prob": -25.78997139260173,
      "best_answer": "Bodo League massacre",
      "total_prob": 6.3031513544694364e-12,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 16,
      "generated_answer": "Ariocarpus",
      "gold_answers": [
        "Ariocarpus"
      ],
      "reference_answer": "Ariocarpus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ariocarpus": 0.9964347618081253
      },
      "individual_log_probs": {
        "Ariocarpus": -0.0035716087998878265
      },
      "max_prob": 0.9964347618081253,
      "max_log_prob": -0.0035716087998878265,
      "best_answer": "Ariocarpus",
      "total_prob": 0.9964347618081253,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 17,
      "generated_answer": "761",
      "gold_answers": [
        "761"
      ],
      "reference_answer": "761",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "761": 0.9976519772540228
      },
      "individual_log_probs": {
        "761": -0.002350783674046397
      },
      "max_prob": 0.9976519772540228,
      "max_log_prob": -0.002350783674046397,
      "best_answer": "761",
      "total_prob": 0.9976519772540228,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 18,
      "generated_answer": "Cnut the Great",
      "gold_answers": [
        "Cnut the Great"
      ],
      "reference_answer": "Cnut the Great",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Cnut the Great": 0.9402247471017111
      },
      "individual_log_probs": {
        "Cnut the Great": -0.06163633963478787
      },
      "max_prob": 0.9402247471017111,
      "max_log_prob": -0.06163633963478787,
      "best_answer": "Cnut the Great",
      "total_prob": 0.9402247471017111,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 19,
      "generated_answer": "MTV India.",
      "gold_answers": [
        "Channel V"
      ],
      "reference_answer": "Channel V",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Channel V": 0.00011800097320670421
      },
      "individual_log_probs": {
        "Channel V": -9.044817686018177
      },
      "max_prob": 0.00011800097320670421,
      "max_log_prob": -9.044817686018177,
      "best_answer": "Channel V",
      "total_prob": 0.00011800097320670421,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 20,
      "generated_answer": "Albert Camus",
      "gold_answers": [
        "Albert Camus"
      ],
      "reference_answer": "Albert Camus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Albert Camus": 0.9930937057142121
      },
      "individual_log_probs": {
        "Albert Camus": -0.006930253111022466
      },
      "max_prob": 0.9930937057142121,
      "max_log_prob": -0.006930253111022466,
      "best_answer": "Albert Camus",
      "total_prob": 0.9930937057142121,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 21,
      "generated_answer": "Cardinal Francis Spellman",
      "gold_answers": [
        "Paul Marcinkus"
      ],
      "reference_answer": "Paul Marcinkus",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Paul Marcinkus": 0.0005250587081853566
      },
      "individual_log_probs": {
        "Paul Marcinkus": -7.552000476509647
      },
      "max_prob": 0.0005250587081853566,
      "max_log_prob": -7.552000476509647,
      "best_answer": "Paul Marcinkus",
      "total_prob": 0.0005250587081853566,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 22,
      "generated_answer": "1852",
      "gold_answers": [
        "1904"
      ],
      "reference_answer": "1904",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1904": 9.890414780503849e-09
      },
      "individual_log_probs": {
        "1904": -18.431699752807617
      },
      "max_prob": 9.890414780503849e-09,
      "max_log_prob": -18.431699752807617,
      "best_answer": "1904",
      "total_prob": 9.890414780503849e-09,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 23,
      "generated_answer": "Ralph Fiennes",
      "gold_answers": [
        "Ralph Fiennes"
      ],
      "reference_answer": "Ralph Fiennes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ralph Fiennes": 0.9993720911795382
      },
      "individual_log_probs": {
        "Ralph Fiennes": -0.0006281060377659742
      },
      "max_prob": 0.9993720911795382,
      "max_log_prob": -0.0006281060377659742,
      "best_answer": "Ralph Fiennes",
      "total_prob": 0.9993720911795382,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 24,
      "generated_answer": "Champion",
      "gold_answers": [
        "runner-up"
      ],
      "reference_answer": "runner-up",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "runner-up": 6.993909298199336e-06
      },
      "individual_log_probs": {
        "runner-up": -11.870470887923148
      },
      "max_prob": 6.993909298199336e-06,
      "max_log_prob": -11.870470887923148,
      "best_answer": "runner-up",
      "total_prob": 6.993909298199336e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 25,
      "generated_answer": "Swedish",
      "gold_answers": [
        "Swedish"
      ],
      "reference_answer": "Swedish",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Swedish": 0.9903906669143839
      },
      "individual_log_probs": {
        "Swedish": -0.009655800647919932
      },
      "max_prob": 0.9903906669143839,
      "max_log_prob": -0.009655800647919932,
      "best_answer": "Swedish",
      "total_prob": 0.9903906669143839,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 26,
      "generated_answer": "Vice",
      "gold_answers": [
        "Vice"
      ],
      "reference_answer": "Vice",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Vice": 0.994543695945955
      },
      "individual_log_probs": {
        "Vice": -0.0054712440505682025
      },
      "max_prob": 0.994543695945955,
      "max_log_prob": -0.0054712440505682025,
      "best_answer": "Vice",
      "total_prob": 0.994543695945955,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 27,
      "generated_answer": "The Preppie Murder",
      "gold_answers": [
        "The Preppie Murder"
      ],
      "reference_answer": "The Preppie Murder",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Preppie Murder": 0.9808982476373264
      },
      "individual_log_probs": {
        "The Preppie Murder": -0.019286547897991113
      },
      "max_prob": 0.9808982476373264,
      "max_log_prob": -0.019286547897991113,
      "best_answer": "The Preppie Murder",
      "total_prob": 0.9808982476373264,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 28,
      "generated_answer": "Isolde Kostner",
      "gold_answers": [
        "Isolde Kostner"
      ],
      "reference_answer": "Isolde Kostner",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Isolde Kostner": 0.9992572614435863
      },
      "individual_log_probs": {
        "Isolde Kostner": -0.0007430145233513485
      },
      "max_prob": 0.9992572614435863,
      "max_log_prob": -0.0007430145233513485,
      "best_answer": "Isolde Kostner",
      "total_prob": 0.9992572614435863,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 29,
      "generated_answer": "Hollywood High School",
      "gold_answers": [
        "Beverly Hills High School"
      ],
      "reference_answer": "Beverly Hills High School",
      "em_score": 0.0,
      "f1_score": 0.5714285714285715,
      "precision": 0.6666666666666666,
      "recall": 0.5,
      "individual_probs": {
        "Beverly Hills High School": 1.193939933633687e-05
      },
      "individual_log_probs": {
        "Beverly Hills High School": -11.335666758104708
      },
      "max_prob": 1.193939933633687e-05,
      "max_log_prob": -11.335666758104708,
      "best_answer": "Beverly Hills High School",
      "total_prob": 1.193939933633687e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 30,
      "generated_answer": "1989",
      "gold_answers": [
        "fifth album"
      ],
      "reference_answer": "fifth album",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "fifth album": 6.980412416462179e-10
      },
      "individual_log_probs": {
        "fifth album": -21.082742929458618
      },
      "max_prob": 6.980412416462179e-10,
      "max_log_prob": -21.082742929458618,
      "best_answer": "fifth album",
      "total_prob": 6.980412416462179e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 31,
      "generated_answer": "Middleweight",
      "gold_answers": [
        "Welterweight"
      ],
      "reference_answer": "Welterweight",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Welterweight": 4.7558675555676986e-07
      },
      "individual_log_probs": {
        "Welterweight": -14.558716520379676
      },
      "max_prob": 4.7558675555676986e-07,
      "max_log_prob": -14.558716520379676,
      "best_answer": "Welterweight",
      "total_prob": 4.7558675555676986e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 32,
      "generated_answer": "Times Beach",
      "gold_answers": [
        "Times Beach"
      ],
      "reference_answer": "Times Beach",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Times Beach": 0.6551298384870425
      },
      "individual_log_probs": {
        "Times Beach": -0.42292183629342617
      },
      "max_prob": 0.6551298384870425,
      "max_log_prob": -0.42292183629342617,
      "best_answer": "Times Beach",
      "total_prob": 0.6551298384870425,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 33,
      "generated_answer": "Beverly Hills Supper Club",
      "gold_answers": [
        "Beverly Hills Supper Club fire"
      ],
      "reference_answer": "Beverly Hills Supper Club fire",
      "em_score": 0.0,
      "f1_score": 0.888888888888889,
      "precision": 1.0,
      "recall": 0.8,
      "individual_probs": {
        "Beverly Hills Supper Club fire": 0.2734701674461769
      },
      "individual_log_probs": {
        "Beverly Hills Supper Club fire": -1.296562740048671
      },
      "max_prob": 0.2734701674461769,
      "max_log_prob": -1.296562740048671,
      "best_answer": "Beverly Hills Supper Club fire",
      "total_prob": 0.2734701674461769,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 34,
      "generated_answer": "Wide receiver.",
      "gold_answers": [
        "quarterback"
      ],
      "reference_answer": "quarterback",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "quarterback": 0.030845411490795265
      },
      "individual_log_probs": {
        "quarterback": -3.4787672758099006
      },
      "max_prob": 0.030845411490795265,
      "max_log_prob": -3.4787672758099006,
      "best_answer": "quarterback",
      "total_prob": 0.030845411490795265,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 35,
      "generated_answer": "Dusty Dvoracek",
      "gold_answers": [
        "Dusty Dvoracek"
      ],
      "reference_answer": "Dusty Dvoracek",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Dusty Dvoracek": 0.9996707000276124
      },
      "individual_log_probs": {
        "Dusty Dvoracek": -0.00032935420352941946
      },
      "max_prob": 0.9996707000276124,
      "max_log_prob": -0.00032935420352941946,
      "best_answer": "Dusty Dvoracek",
      "total_prob": 0.9996707000276124,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 36,
      "generated_answer": "No.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "no": 0.004203138259261959
      },
      "individual_log_probs": {
        "no": -5.471923828125
      },
      "max_prob": 0.004203138259261959,
      "max_log_prob": -5.471923828125,
      "best_answer": "no",
      "total_prob": 0.004203138259261959,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 37,
      "generated_answer": "New York Yankees",
      "gold_answers": [
        "Texas Rangers"
      ],
      "reference_answer": "Texas Rangers",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Texas Rangers": 2.1167414268096827e-06
      },
      "individual_log_probs": {
        "Texas Rangers": -13.065632714598905
      },
      "max_prob": 2.1167414268096827e-06,
      "max_log_prob": -13.065632714598905,
      "best_answer": "Texas Rangers",
      "total_prob": 2.1167414268096827e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 38,
      "generated_answer": "Slavic, Hungarian, and Romanian groups.",
      "gold_answers": [
        "Germanic, Baltic and Thracian tribes"
      ],
      "reference_answer": "Germanic, Baltic and Thracian tribes",
      "em_score": 0.0,
      "f1_score": 0.20000000000000004,
      "precision": 0.2,
      "recall": 0.2,
      "individual_probs": {
        "Germanic, Baltic and Thracian tribes": 3.4517689775000596e-09
      },
      "individual_log_probs": {
        "Germanic, Baltic and Thracian tribes": -19.484378990211553
      },
      "max_prob": 3.4517689775000596e-09,
      "max_log_prob": -19.484378990211553,
      "best_answer": "Germanic, Baltic and Thracian tribes",
      "total_prob": 3.4517689775000596e-09,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 39,
      "generated_answer": "American",
      "gold_answers": [
        "American"
      ],
      "reference_answer": "American",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "American": 0.9994787312391036
      },
      "individual_log_probs": {
        "American": -0.0005214046686887741
      },
      "max_prob": 0.9994787312391036,
      "max_log_prob": -0.0005214046686887741,
      "best_answer": "American",
      "total_prob": 0.9994787312391036,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 40,
      "generated_answer": "Mario Lemieux",
      "gold_answers": [
        "Mario Lemieux"
      ],
      "reference_answer": "Mario Lemieux",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mario Lemieux": 0.9751150925256064
      },
      "individual_log_probs": {
        "Mario Lemieux": -0.025199771334882826
      },
      "max_prob": 0.9751150925256064,
      "max_log_prob": -0.025199771334882826,
      "best_answer": "Mario Lemieux",
      "total_prob": 0.9751150925256064,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 41,
      "generated_answer": "Mumbai",
      "gold_answers": [
        "Mumbai"
      ],
      "reference_answer": "Mumbai",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mumbai": 0.9993146505389481
      },
      "individual_log_probs": {
        "Mumbai": -0.000685584420352825
      },
      "max_prob": 0.9993146505389481,
      "max_log_prob": -0.000685584420352825,
      "best_answer": "Mumbai",
      "total_prob": 0.9993146505389481,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 42,
      "generated_answer": "Sergei Rachmaninoff",
      "gold_answers": [
        "Alexander Scriabin"
      ],
      "reference_answer": "Alexander Scriabin",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Alexander Scriabin": 0.31199061716263926
      },
      "individual_log_probs": {
        "Alexander Scriabin": -1.1647821648215313
      },
      "max_prob": 0.31199061716263926,
      "max_log_prob": -1.1647821648215313,
      "best_answer": "Alexander Scriabin",
      "total_prob": 0.31199061716263926,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 43,
      "generated_answer": "11",
      "gold_answers": [
        "Their biggest hit was \"Tall Cool One\", first released in 1959"
      ],
      "reference_answer": "Their biggest hit was \"Tall Cool One\", first released in 1959",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Their biggest hit was \"Tall Cool One\", first released in 1959": 6.095509469003856e-22
      },
      "individual_log_probs": {
        "Their biggest hit was \"Tall Cool One\", first released in 1959": -48.84931969840545
      },
      "max_prob": 6.095509469003856e-22,
      "max_log_prob": -48.84931969840545,
      "best_answer": "Their biggest hit was \"Tall Cool One\", first released in 1959",
      "total_prob": 6.095509469003856e-22,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 44,
      "generated_answer": "Jawaharlal Nehru",
      "gold_answers": [
        "first Prime Minister of India"
      ],
      "reference_answer": "first Prime Minister of India",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "first Prime Minister of India": 4.744796746572176e-12
      },
      "individual_log_probs": {
        "first Prime Minister of India": -26.07397252006922
      },
      "max_prob": 4.744796746572176e-12,
      "max_log_prob": -26.07397252006922,
      "best_answer": "first Prime Minister of India",
      "total_prob": 4.744796746572176e-12,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 45,
      "generated_answer": "United Kingdom",
      "gold_answers": [
        "Ireland"
      ],
      "reference_answer": "Ireland",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Ireland": 1.6122167475952654e-08
      },
      "individual_log_probs": {
        "Ireland": -17.943070650100708
      },
      "max_prob": 1.6122167475952654e-08,
      "max_log_prob": -17.943070650100708,
      "best_answer": "Ireland",
      "total_prob": 1.6122167475952654e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 46,
      "generated_answer": "72-78 passengers",
      "gold_answers": [
        "72-78"
      ],
      "reference_answer": "72-78",
      "em_score": 0.0,
      "f1_score": 0.6666666666666666,
      "precision": 0.5,
      "recall": 1.0,
      "individual_probs": {
        "72-78": 0.9869727497586329
      },
      "individual_log_probs": {
        "72-78": -0.013112849090248346
      },
      "max_prob": 0.9869727497586329,
      "max_log_prob": -0.013112849090248346,
      "best_answer": "72-78",
      "total_prob": 0.9869727497586329,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 47,
      "generated_answer": "Heroica Matamoros",
      "gold_answers": [
        "Heroica Matamoros"
      ],
      "reference_answer": "Heroica Matamoros",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Heroica Matamoros": 0.9852478108000285
      },
      "individual_log_probs": {
        "Heroica Matamoros": -0.014862084883581872
      },
      "max_prob": 0.9852478108000285,
      "max_log_prob": -0.014862084883581872,
      "best_answer": "Heroica Matamoros",
      "total_prob": 0.9852478108000285,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 48,
      "generated_answer": "General Sir Henry Clinton",
      "gold_answers": [
        "Benedict Arnold"
      ],
      "reference_answer": "Benedict Arnold",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Benedict Arnold": 3.388829936625079e-08
      },
      "individual_log_probs": {
        "Benedict Arnold": -17.200196033576503
      },
      "max_prob": 3.388829936625079e-08,
      "max_log_prob": -17.200196033576503,
      "best_answer": "Benedict Arnold",
      "total_prob": 3.388829936625079e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 49,
      "generated_answer": "Wembley Stadium",
      "gold_answers": [
        "South Kesteven Sports Stadium"
      ],
      "reference_answer": "South Kesteven Sports Stadium",
      "em_score": 0.0,
      "f1_score": 0.3333333333333333,
      "precision": 0.5,
      "recall": 0.25,
      "individual_probs": {
        "South Kesteven Sports Stadium": 0.02312365892737744
      },
      "individual_log_probs": {
        "South Kesteven Sports Stadium": -3.7668989895948926
      },
      "max_prob": 0.02312365892737744,
      "max_log_prob": -3.7668989895948926,
      "best_answer": "South Kesteven Sports Stadium",
      "total_prob": 0.02312365892737744,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 50,
      "generated_answer": "Aiko, Princess Toshi",
      "gold_answers": [
        "Aiko, Princess Toshi"
      ],
      "reference_answer": "Aiko, Princess Toshi",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Aiko, Princess Toshi": 0.9224206173544279
      },
      "individual_log_probs": {
        "Aiko, Princess Toshi": -0.08075395841842692
      },
      "max_prob": 0.9224206173544279,
      "max_log_prob": -0.08075395841842692,
      "best_answer": "Aiko, Princess Toshi",
      "total_prob": 0.9224206173544279,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 51,
      "generated_answer": "Marco Antonio Barrera",
      "gold_answers": [
        "Marco Antonio Barrera"
      ],
      "reference_answer": "Marco Antonio Barrera",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Marco Antonio Barrera": 0.996651688851964
      },
      "individual_log_probs": {
        "Marco Antonio Barrera": -0.003353929286163293
      },
      "max_prob": 0.996651688851964,
      "max_log_prob": -0.003353929286163293,
      "best_answer": "Marco Antonio Barrera",
      "total_prob": 0.996651688851964,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 52,
      "generated_answer": "Iberian Peninsula",
      "gold_answers": [
        "Iberian Peninsula"
      ],
      "reference_answer": "Iberian Peninsula",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Iberian Peninsula": 0.7514720429211377
      },
      "individual_log_probs": {
        "Iberian Peninsula": -0.2857212721828546
      },
      "max_prob": 0.7514720429211377,
      "max_log_prob": -0.2857212721828546,
      "best_answer": "Iberian Peninsula",
      "total_prob": 0.7514720429211377,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 53,
      "generated_answer": "Princes Highway",
      "gold_answers": [
        "Hume Highway"
      ],
      "reference_answer": "Hume Highway",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Hume Highway": 1.7879903401330823e-11
      },
      "individual_log_probs": {
        "Hume Highway": -24.747343748807907
      },
      "max_prob": 1.7879903401330823e-11,
      "max_log_prob": -24.747343748807907,
      "best_answer": "Hume Highway",
      "total_prob": 1.7879903401330823e-11,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 54,
      "generated_answer": "Lawrence Technological University.",
      "gold_answers": [
        "University of North Texas"
      ],
      "reference_answer": "University of North Texas",
      "em_score": 0.0,
      "f1_score": 0.28571428571428575,
      "precision": 0.3333333333333333,
      "recall": 0.25,
      "individual_probs": {
        "University of North Texas": 8.588186662028871e-05
      },
      "individual_log_probs": {
        "University of North Texas": -9.362537849974615
      },
      "max_prob": 8.588186662028871e-05,
      "max_log_prob": -9.362537849974615,
      "best_answer": "University of North Texas",
      "total_prob": 8.588186662028871e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 55,
      "generated_answer": "Humber Refinery.",
      "gold_answers": [
        "the A160"
      ],
      "reference_answer": "the A160",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "the A160": 6.313110038178165e-09
      },
      "individual_log_probs": {
        "the A160": -18.880637407302856
      },
      "max_prob": 6.313110038178165e-09,
      "max_log_prob": -18.880637407302856,
      "best_answer": "the A160",
      "total_prob": 6.313110038178165e-09,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 56,
      "generated_answer": "American",
      "gold_answers": [
        "American"
      ],
      "reference_answer": "American",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "American": 0.8959588238762916
      },
      "individual_log_probs": {
        "American": -0.10986082255840302
      },
      "max_prob": 0.8959588238762916,
      "max_log_prob": -0.10986082255840302,
      "best_answer": "American",
      "total_prob": 0.8959588238762916,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 57,
      "generated_answer": "*Capsicum chinense*",
      "gold_answers": [
        "\"Capsicum chinense\""
      ],
      "reference_answer": "\"Capsicum chinense\"",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "\"Capsicum chinense\"": 0.0023056689294965993
      },
      "individual_log_probs": {
        "\"Capsicum chinense\"": -6.072384436611173
      },
      "max_prob": 0.0023056689294965993,
      "max_log_prob": -6.072384436611173,
      "best_answer": "\"Capsicum chinense\"",
      "total_prob": 0.0023056689294965993,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 58,
      "generated_answer": "Barack Obama",
      "gold_answers": [
        "Hillary Clinton"
      ],
      "reference_answer": "Hillary Clinton",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Hillary Clinton": 0.012811640175569312
      },
      "individual_log_probs": {
        "Hillary Clinton": -4.357401132583618
      },
      "max_prob": 0.012811640175569312,
      "max_log_prob": -4.357401132583618,
      "best_answer": "Hillary Clinton",
      "total_prob": 0.012811640175569312,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 59,
      "generated_answer": "Lynyrd Skynyrd",
      "gold_answers": [
        "Lynyrd Skynyrd"
      ],
      "reference_answer": "Lynyrd Skynyrd",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Lynyrd Skynyrd": 0.8273132827377462
      },
      "individual_log_probs": {
        "Lynyrd Skynyrd": -0.18957183738757521
      },
      "max_prob": 0.8273132827377462,
      "max_log_prob": -0.18957183738757521,
      "best_answer": "Lynyrd Skynyrd",
      "total_prob": 0.8273132827377462,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 60,
      "generated_answer": "Sehnsucht",
      "gold_answers": [
        "Herzeleid"
      ],
      "reference_answer": "Herzeleid",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Herzeleid": 1.3212660283475172e-05
      },
      "individual_log_probs": {
        "Herzeleid": -11.234335075641866
      },
      "max_prob": 1.3212660283475172e-05,
      "max_log_prob": -11.234335075641866,
      "best_answer": "Herzeleid",
      "total_prob": 1.3212660283475172e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 61,
      "generated_answer": "1897",
      "gold_answers": [
        "1897"
      ],
      "reference_answer": "1897",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "1897": 0.9879772950842832
      },
      "individual_log_probs": {
        "1897": -0.012095562182253161
      },
      "max_prob": 0.9879772950842832,
      "max_log_prob": -0.012095562182253161,
      "best_answer": "1897",
      "total_prob": 0.9879772950842832,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 62,
      "generated_answer": "Ahmed Elmohamady",
      "gold_answers": [
        "Ahmed Elmohamady"
      ],
      "reference_answer": "Ahmed Elmohamady",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ahmed Elmohamady": 0.9586860080312354
      },
      "individual_log_probs": {
        "Ahmed Elmohamady": -0.04219167373685195
      },
      "max_prob": 0.9586860080312354,
      "max_log_prob": -0.04219167373685195,
      "best_answer": "Ahmed Elmohamady",
      "total_prob": 0.9586860080312354,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 63,
      "generated_answer": "live-action",
      "gold_answers": [
        "animation"
      ],
      "reference_answer": "animation",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "animation": 5.99085335390426e-07
      },
      "individual_log_probs": {
        "animation": -14.327861785888672
      },
      "max_prob": 5.99085335390426e-07,
      "max_log_prob": -14.327861785888672,
      "best_answer": "animation",
      "total_prob": 5.99085335390426e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 64,
      "generated_answer": "Jeff Healey",
      "gold_answers": [
        "Jeff Healey"
      ],
      "reference_answer": "Jeff Healey",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Jeff Healey": 0.9727975171059356
      },
      "individual_log_probs": {
        "Jeff Healey": -0.02757932009058095
      },
      "max_prob": 0.9727975171059356,
      "max_log_prob": -0.02757932009058095,
      "best_answer": "Jeff Healey",
      "total_prob": 0.9727975171059356,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 65,
      "generated_answer": "Royal Air Force",
      "gold_answers": [
        "British Armed Forces"
      ],
      "reference_answer": "British Armed Forces",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "British Armed Forces": 0.0002772540600763185
      },
      "individual_log_probs": {
        "British Armed Forces": -8.190576287742488
      },
      "max_prob": 0.0002772540600763185,
      "max_log_prob": -8.190576287742488,
      "best_answer": "British Armed Forces",
      "total_prob": 0.0002772540600763185,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 66,
      "generated_answer": "Kadhalan",
      "gold_answers": [
        "Kadhalan"
      ],
      "reference_answer": "Kadhalan",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Kadhalan": 0.9964559285859337
      },
      "individual_log_probs": {
        "Kadhalan": -0.0035503665130818263
      },
      "max_prob": 0.9964559285859337,
      "max_log_prob": -0.0035503665130818263,
      "best_answer": "Kadhalan",
      "total_prob": 0.9964559285859337,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 67,
      "generated_answer": "Vydubychi Hill",
      "gold_answers": [
        "Cherepanov Hill"
      ],
      "reference_answer": "Cherepanov Hill",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Cherepanov Hill": 8.133562354180648e-05
      },
      "individual_log_probs": {
        "Cherepanov Hill": -9.41692646343472
      },
      "max_prob": 8.133562354180648e-05,
      "max_log_prob": -9.41692646343472,
      "best_answer": "Cherepanov Hill",
      "total_prob": 8.133562354180648e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 68,
      "generated_answer": "Arantxa Sánchez Vicario",
      "gold_answers": [
        "\"Arantxa\" Sánchez Vicario"
      ],
      "reference_answer": "\"Arantxa\" Sánchez Vicario",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "\"Arantxa\" Sánchez Vicario": 4.863374036601695e-16
      },
      "individual_log_probs": {
        "\"Arantxa\" Sánchez Vicario": -35.25962904465541
      },
      "max_prob": 4.863374036601695e-16,
      "max_log_prob": -35.25962904465541,
      "best_answer": "\"Arantxa\" Sánchez Vicario",
      "total_prob": 4.863374036601695e-16,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 69,
      "generated_answer": "University of Oklahoma",
      "gold_answers": [
        "University of Alabama"
      ],
      "reference_answer": "University of Alabama",
      "em_score": 0.0,
      "f1_score": 0.6666666666666666,
      "precision": 0.6666666666666666,
      "recall": 0.6666666666666666,
      "individual_probs": {
        "University of Alabama": 2.278792339172042e-07
      },
      "individual_log_probs": {
        "University of Alabama": -15.294450024121488
      },
      "max_prob": 2.278792339172042e-07,
      "max_log_prob": -15.294450024121488,
      "best_answer": "University of Alabama",
      "total_prob": 2.278792339172042e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 70,
      "generated_answer": "Pizza Hut",
      "gold_answers": [
        "Pizza Hut"
      ],
      "reference_answer": "Pizza Hut",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Pizza Hut": 0.975196593425827
      },
      "individual_log_probs": {
        "Pizza Hut": -0.02511619402685028
      },
      "max_prob": 0.975196593425827,
      "max_log_prob": -0.02511619402685028,
      "best_answer": "Pizza Hut",
      "total_prob": 0.975196593425827,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 71,
      "generated_answer": "1945",
      "gold_answers": [
        "1939"
      ],
      "reference_answer": "1939",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1939": 0.02340838032206691
      },
      "individual_log_probs": {
        "1939": -3.754661187311285
      },
      "max_prob": 0.02340838032206691,
      "max_log_prob": -3.754661187311285,
      "best_answer": "1939",
      "total_prob": 0.02340838032206691,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 72,
      "generated_answer": "1964",
      "gold_answers": [
        "1962"
      ],
      "reference_answer": "1962",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1962": 0.14857806800469164
      },
      "individual_log_probs": {
        "1962": -1.906644748407416
      },
      "max_prob": 0.14857806800469164,
      "max_log_prob": -1.906644748407416,
      "best_answer": "1962",
      "total_prob": 0.14857806800469164,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 73,
      "generated_answer": "Aquatium",
      "gold_answers": [
        "Metreon"
      ],
      "reference_answer": "Metreon",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Metreon": 2.9223877497637297e-10
      },
      "individual_log_probs": {
        "Metreon": -21.953449925291352
      },
      "max_prob": 2.9223877497637297e-10,
      "max_log_prob": -21.953449925291352,
      "best_answer": "Metreon",
      "total_prob": 2.9223877497637297e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 74,
      "generated_answer": "Viktor Uspaskich",
      "gold_answers": [
        "Viktor Uspaskich"
      ],
      "reference_answer": "Viktor Uspaskich",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Viktor Uspaskich": 0.5512805004091142
      },
      "individual_log_probs": {
        "Viktor Uspaskich": -0.5955115242113607
      },
      "max_prob": 0.5512805004091142,
      "max_log_prob": -0.5955115242113607,
      "best_answer": "Viktor Uspaskich",
      "total_prob": 0.5512805004091142,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 75,
      "generated_answer": "Spacemen 3",
      "gold_answers": [
        "Spacemen 3"
      ],
      "reference_answer": "Spacemen 3",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Spacemen 3": 0.9827289570575055
      },
      "individual_log_probs": {
        "Spacemen 3": -0.017421927214165578
      },
      "max_prob": 0.9827289570575055,
      "max_log_prob": -0.017421927214165578,
      "best_answer": "Spacemen 3",
      "total_prob": 0.9827289570575055,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 76,
      "generated_answer": "Stephen King.",
      "gold_answers": [
        "Stephen King"
      ],
      "reference_answer": "Stephen King",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Stephen King": 0.9980871331231733
      },
      "individual_log_probs": {
        "Stephen King": -0.0019146987431213347
      },
      "max_prob": 0.9980871331231733,
      "max_log_prob": -0.0019146987431213347,
      "best_answer": "Stephen King",
      "total_prob": 0.9980871331231733,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 77,
      "generated_answer": "bringing together electricity, magnetism, and light as manifestations of the same phenomenon",
      "gold_answers": [
        "classical theory of electromagnetic radiation"
      ],
      "reference_answer": "classical theory of electromagnetic radiation",
      "em_score": 0.0,
      "f1_score": 0.12500000000000003,
      "precision": 0.09090909090909091,
      "recall": 0.2,
      "individual_probs": {
        "classical theory of electromagnetic radiation": 2.1979969351585582e-07
      },
      "individual_log_probs": {
        "classical theory of electromagnetic radiation": -15.330549189355224
      },
      "max_prob": 2.1979969351585582e-07,
      "max_log_prob": -15.330549189355224,
      "best_answer": "classical theory of electromagnetic radiation",
      "total_prob": 2.1979969351585582e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 78,
      "generated_answer": "The Country Bears",
      "gold_answers": [
        "The London Connection"
      ],
      "reference_answer": "The London Connection",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "The London Connection": 8.145838323571338e-05
      },
      "individual_log_probs": {
        "The London Connection": -9.415418303280603
      },
      "max_prob": 8.145838323571338e-05,
      "max_log_prob": -9.415418303280603,
      "best_answer": "The London Connection",
      "total_prob": 8.145838323571338e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 79,
      "generated_answer": "Mexican",
      "gold_answers": [
        "Mexican"
      ],
      "reference_answer": "Mexican",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mexican": 0.6227503104790751
      },
      "individual_log_probs": {
        "Mexican": -0.47360962627863046
      },
      "max_prob": 0.6227503104790751,
      "max_log_prob": -0.47360962627863046,
      "best_answer": "Mexican",
      "total_prob": 0.6227503104790751,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 80,
      "generated_answer": "September 23, 2001",
      "gold_answers": [
        "December 7, 2008"
      ],
      "reference_answer": "December 7, 2008",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "December 7, 2008": 1.1125125360309118e-10
      },
      "individual_log_probs": {
        "December 7, 2008": -22.919229926603293
      },
      "max_prob": 1.1125125360309118e-10,
      "max_log_prob": -22.919229926603293,
      "best_answer": "December 7, 2008",
      "total_prob": 1.1125125360309118e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 81,
      "generated_answer": "Yes",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "yes": 0.0018789468985630658
      },
      "individual_log_probs": {
        "yes": -6.27704381942749
      },
      "max_prob": 0.0018789468985630658,
      "max_log_prob": -6.27704381942749,
      "best_answer": "yes",
      "total_prob": 0.0018789468985630658,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 82,
      "generated_answer": "Football",
      "gold_answers": [
        "Football"
      ],
      "reference_answer": "Football",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Football": 0.9372190295571898
      },
      "individual_log_probs": {
        "Football": -0.06483826786279678
      },
      "max_prob": 0.9372190295571898,
      "max_log_prob": -0.06483826786279678,
      "best_answer": "Football",
      "total_prob": 0.9372190295571898,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 83,
      "generated_answer": "Betty Shabazz",
      "gold_answers": [
        "Betty X"
      ],
      "reference_answer": "Betty X",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Betty X": 0.2908711766622062
      },
      "individual_log_probs": {
        "Betty X": -1.2348748017100775
      },
      "max_prob": 0.2908711766622062,
      "max_log_prob": -1.2348748017100775,
      "best_answer": "Betty X",
      "total_prob": 0.2908711766622062,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 84,
      "generated_answer": "Pizza Inn",
      "gold_answers": [
        "Extreme Pizza"
      ],
      "reference_answer": "Extreme Pizza",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Extreme Pizza": 2.617941836198711e-05
      },
      "individual_log_probs": {
        "Extreme Pizza": -10.550537014612928
      },
      "max_prob": 2.617941836198711e-05,
      "max_log_prob": -10.550537014612928,
      "best_answer": "Extreme Pizza",
      "total_prob": 2.617941836198711e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 85,
      "generated_answer": "Diana Ernestine Earle Ross",
      "gold_answers": [
        "Anna Mae Bullock"
      ],
      "reference_answer": "Anna Mae Bullock",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Anna Mae Bullock": 1.098045632374742e-08
      },
      "individual_log_probs": {
        "Anna Mae Bullock": -18.327148842188763
      },
      "max_prob": 1.098045632374742e-08,
      "max_log_prob": -18.327148842188763,
      "best_answer": "Anna Mae Bullock",
      "total_prob": 1.098045632374742e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 86,
      "generated_answer": "Blaik Field",
      "gold_answers": [
        "Michie Stadium"
      ],
      "reference_answer": "Michie Stadium",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Michie Stadium": 6.050337679568561e-08
      },
      "individual_log_probs": {
        "Michie Stadium": -16.620566658662483
      },
      "max_prob": 6.050337679568561e-08,
      "max_log_prob": -16.620566658662483,
      "best_answer": "Michie Stadium",
      "total_prob": 6.050337679568561e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 87,
      "generated_answer": "Pacific Ocean",
      "gold_answers": [
        "the Pacific Ocean"
      ],
      "reference_answer": "the Pacific Ocean",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "the Pacific Ocean": 0.0011554092482124587
      },
      "individual_log_probs": {
        "the Pacific Ocean": -6.763300670310855
      },
      "max_prob": 0.0011554092482124587,
      "max_log_prob": -6.763300670310855,
      "best_answer": "the Pacific Ocean",
      "total_prob": 0.0011554092482124587,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 88,
      "generated_answer": "Angela Nissel",
      "gold_answers": [
        "Angela R. Nissel"
      ],
      "reference_answer": "Angela R. Nissel",
      "em_score": 0.0,
      "f1_score": 0.8,
      "precision": 1.0,
      "recall": 0.6666666666666666,
      "individual_probs": {
        "Angela R. Nissel": 4.219898578567404e-09
      },
      "individual_log_probs": {
        "Angela R. Nissel": -19.283454742702133
      },
      "max_prob": 4.219898578567404e-09,
      "max_log_prob": -19.283454742702133,
      "best_answer": "Angela R. Nissel",
      "total_prob": 4.219898578567404e-09,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 89,
      "generated_answer": "LyondellBasell.",
      "gold_answers": [
        "Netherlands"
      ],
      "reference_answer": "Netherlands",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Netherlands": 0.019086240753390608
      },
      "individual_log_probs": {
        "Netherlands": -3.9587875829311088
      },
      "max_prob": 0.019086240753390608,
      "max_log_prob": -3.9587875829311088,
      "best_answer": "Netherlands",
      "total_prob": 0.019086240753390608,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 90,
      "generated_answer": "Michael Schumacher",
      "gold_answers": [
        "Alessandro Nannini"
      ],
      "reference_answer": "Alessandro Nannini",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Alessandro Nannini": 0.04186653588522367
      },
      "individual_log_probs": {
        "Alessandro Nannini": -3.1732684373816085
      },
      "max_prob": 0.04186653588522367,
      "max_log_prob": -3.1732684373816085,
      "best_answer": "Alessandro Nannini",
      "total_prob": 0.04186653588522367,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 91,
      "generated_answer": "Vincent Willem van Gogh",
      "gold_answers": [
        "Theodorus \"Theo\" van Gogh"
      ],
      "reference_answer": "Theodorus \"Theo\" van Gogh",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Theodorus \"Theo\" van Gogh": 0.0004316633715187947
      },
      "individual_log_probs": {
        "Theodorus \"Theo\" van Gogh": -7.747864506075075
      },
      "max_prob": 0.0004316633715187947,
      "max_log_prob": -7.747864506075075,
      "best_answer": "Theodorus \"Theo\" van Gogh",
      "total_prob": 0.0004316633715187947,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 92,
      "generated_answer": "Indian Rebellion of 1857",
      "gold_answers": [
        "Indian Rebellion of 1857"
      ],
      "reference_answer": "Indian Rebellion of 1857",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Indian Rebellion of 1857": 0.812302313032924
      },
      "individual_log_probs": {
        "Indian Rebellion of 1857": -0.20788270141860465
      },
      "max_prob": 0.812302313032924,
      "max_log_prob": -0.20788270141860465,
      "best_answer": "Indian Rebellion of 1857",
      "total_prob": 0.812302313032924,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 93,
      "generated_answer": "Yes.",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "yes": 0.0007763435825475865
      },
      "individual_log_probs": {
        "yes": -7.160915374755859
      },
      "max_prob": 0.0007763435825475865,
      "max_log_prob": -7.160915374755859,
      "best_answer": "yes",
      "total_prob": 0.0007763435825475865,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 94,
      "generated_answer": "Ludovico Einaudi has performed with Canzoniere Grecanico Salentino.",
      "gold_answers": [
        "Italian pianist and composer"
      ],
      "reference_answer": "Italian pianist and composer",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Italian pianist and composer": 4.1283124865069235e-06
      },
      "individual_log_probs": {
        "Italian pianist and composer": -12.397641833399348
      },
      "max_prob": 4.1283124865069235e-06,
      "max_log_prob": -12.397641833399348,
      "best_answer": "Italian pianist and composer",
      "total_prob": 4.1283124865069235e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 95,
      "generated_answer": "dancer",
      "gold_answers": [
        "actress"
      ],
      "reference_answer": "actress",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "actress": 1.0417682991065135e-07
      },
      "individual_log_probs": {
        "actress": -16.077176094055176
      },
      "max_prob": 1.0417682991065135e-07,
      "max_log_prob": -16.077176094055176,
      "best_answer": "actress",
      "total_prob": 1.0417682991065135e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 96,
      "generated_answer": "London",
      "gold_answers": [
        "Amsterdam"
      ],
      "reference_answer": "Amsterdam",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Amsterdam": 2.204000409512612e-06
      },
      "individual_log_probs": {
        "Amsterdam": -13.025236480869353
      },
      "max_prob": 2.204000409512612e-06,
      "max_log_prob": -13.025236480869353,
      "best_answer": "Amsterdam",
      "total_prob": 2.204000409512612e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 97,
      "generated_answer": "Yes",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "no": 0.0008050488790930579
      },
      "individual_log_probs": {
        "no": -7.124607563018799
      },
      "max_prob": 0.0008050488790930579,
      "max_log_prob": -7.124607563018799,
      "best_answer": "no",
      "total_prob": 0.0008050488790930579,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 98,
      "generated_answer": "Nickelodeon",
      "gold_answers": [
        "Nickelodeon"
      ],
      "reference_answer": "Nickelodeon",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Nickelodeon": 0.9978068709431759
      },
      "individual_log_probs": {
        "Nickelodeon": -0.002195537486329613
      },
      "max_prob": 0.9978068709431759,
      "max_log_prob": -0.002195537486329613,
      "best_answer": "Nickelodeon",
      "total_prob": 0.9978068709431759,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    },
    {
      "sample_idx": 99,
      "generated_answer": "Project Runway and America's Next Top Model.",
      "gold_answers": [
        "Project Runway"
      ],
      "reference_answer": "Project Runway",
      "em_score": 0.0,
      "f1_score": 0.4444444444444445,
      "precision": 0.2857142857142857,
      "recall": 1.0,
      "individual_probs": {
        "Project Runway": 0.797175509632743
      },
      "individual_log_probs": {
        "Project Runway": -0.22668041159613495
      },
      "max_prob": 0.797175509632743,
      "max_log_prob": -0.22668041159613495,
      "best_answer": "Project Runway",
      "total_prob": 0.797175509632743,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 1.0
    }
  ]
}