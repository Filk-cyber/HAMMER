{
  "metrics": {
    "weight_w": 0.0,
    "input_file": "/home/jiangjp/trace-idea/data/hotpotqa/hotpotqa_dev100ideal_with_reasoning_chains_modify_w00.json",
    "total_samples": 100,
    "exact_match": 0.37,
    "f1": 0.4329318362796623,
    "precision": 0.44353968253968257,
    "recall": 0.4429090909090909,
    "avg_max_log_prob": -9.852692761614737,
    "avg_max_prob": 0.28144430463761344,
    "avg_total_prob": 0.28144430463761344,
    "valid_max_prob_samples": 100,
    "invalid_max_prob_samples": 0,
    "avg_answer_count": 1.0,
    "avg_valid_answer_count": 1.0
  },
  "sample_info": [
    {
      "sample_idx": 0,
      "generated_answer": "No.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "no": 0.0045774863529331085
      },
      "individual_log_probs": {
        "no": -5.386605262756348
      },
      "max_prob": 0.0045774863529331085,
      "max_log_prob": -5.386605262756348,
      "best_answer": "no",
      "total_prob": 0.0045774863529331085,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 1,
      "generated_answer": "University of Delaware",
      "gold_answers": [
        "University of Delaware"
      ],
      "reference_answer": "University of Delaware",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "University of Delaware": 0.7281633985813188
      },
      "individual_log_probs": {
        "University of Delaware": -0.3172298073695572
      },
      "max_prob": 0.7281633985813188,
      "max_log_prob": -0.3172298073695572,
      "best_answer": "University of Delaware",
      "total_prob": 0.7281633985813188,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 2,
      "generated_answer": "Le Building",
      "gold_answers": [
        "NBC"
      ],
      "reference_answer": "NBC",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "NBC": 9.416391852993833e-05
      },
      "individual_log_probs": {
        "NBC": -9.27047348022461
      },
      "max_prob": 9.416391852993833e-05,
      "max_log_prob": -9.27047348022461,
      "best_answer": "NBC",
      "total_prob": 9.416391852993833e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 3,
      "generated_answer": "plant genus",
      "gold_answers": [
        "genus"
      ],
      "reference_answer": "genus",
      "em_score": 0.0,
      "f1_score": 0.6666666666666666,
      "precision": 0.5,
      "recall": 1.0,
      "individual_probs": {
        "genus": 0.0007515773047348512
      },
      "individual_log_probs": {
        "genus": -7.193336486816406
      },
      "max_prob": 0.0007515773047348512,
      "max_log_prob": -7.193336486816406,
      "best_answer": "genus",
      "total_prob": 0.0007515773047348512,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 4,
      "generated_answer": "Bob Bergen",
      "gold_answers": [
        "Jeff Sutphen"
      ],
      "reference_answer": "Jeff Sutphen",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Jeff Sutphen": 4.0279936359596513e-07
      },
      "individual_log_probs": {
        "Jeff Sutphen": -14.724827256053686
      },
      "max_prob": 4.0279936359596513e-07,
      "max_log_prob": -14.724827256053686,
      "best_answer": "Jeff Sutphen",
      "total_prob": 4.0279936359596513e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 5,
      "generated_answer": "Karri Forest Region.",
      "gold_answers": [
        "the Bowland Fells"
      ],
      "reference_answer": "the Bowland Fells",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "the Bowland Fells": 1.1031582688660122e-16
      },
      "individual_log_probs": {
        "the Bowland Fells": -36.74318426847458
      },
      "max_prob": 1.1031582688660122e-16,
      "max_log_prob": -36.74318426847458,
      "best_answer": "the Bowland Fells",
      "total_prob": 1.1031582688660122e-16,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 6,
      "generated_answer": "Avi Kaplan",
      "gold_answers": [
        "Daft Punk"
      ],
      "reference_answer": "Daft Punk",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Daft Punk": 0.00047125403428490824
      },
      "individual_log_probs": {
        "Daft Punk": -7.660113258421916
      },
      "max_prob": 0.00047125403428490824,
      "max_log_prob": -7.660113258421916,
      "best_answer": "Daft Punk",
      "total_prob": 0.00047125403428490824,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 7,
      "generated_answer": "3rd Rock from the Sun",
      "gold_answers": [
        "3rd Rock from the Sun"
      ],
      "reference_answer": "3rd Rock from the Sun",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "3rd Rock from the Sun": 0.9665571236636716
      },
      "individual_log_probs": {
        "3rd Rock from the Sun": -0.03401487844484308
      },
      "max_prob": 0.9665571236636716,
      "max_log_prob": -0.03401487844484308,
      "best_answer": "3rd Rock from the Sun",
      "total_prob": 0.9665571236636716,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 8,
      "generated_answer": "I Am Legend",
      "gold_answers": [
        "The Island"
      ],
      "reference_answer": "The Island",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "The Island": 0.00513609752000417
      },
      "individual_log_probs": {
        "The Island": -5.271461725234985
      },
      "max_prob": 0.00513609752000417,
      "max_log_prob": -5.271461725234985,
      "best_answer": "The Island",
      "total_prob": 0.00513609752000417,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 9,
      "generated_answer": "The Great Mouse Detective",
      "gold_answers": [
        "The Great Mouse Detective"
      ],
      "reference_answer": "The Great Mouse Detective",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Great Mouse Detective": 0.3372580316132929
      },
      "individual_log_probs": {
        "The Great Mouse Detective": -1.0869069691643745
      },
      "max_prob": 0.3372580316132929,
      "max_log_prob": -1.0869069691643745,
      "best_answer": "The Great Mouse Detective",
      "total_prob": 0.3372580316132929,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 10,
      "generated_answer": "2003",
      "gold_answers": [
        "20 March to 1 May 2003"
      ],
      "reference_answer": "20 March to 1 May 2003",
      "em_score": 0.0,
      "f1_score": 0.2857142857142857,
      "precision": 1.0,
      "recall": 0.16666666666666666,
      "individual_probs": {
        "20 March to 1 May 2003": 4.81542600861564e-12
      },
      "individual_log_probs": {
        "20 March to 1 May 2003": -26.059196599282586
      },
      "max_prob": 4.81542600861564e-12,
      "max_log_prob": -26.059196599282586,
      "best_answer": "20 March to 1 May 2003",
      "total_prob": 4.81542600861564e-12,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 11,
      "generated_answer": "Both Borassodendron and Chelidonium are types of flowering plants.",
      "gold_answers": [
        "plant"
      ],
      "reference_answer": "plant",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "plant": 0.008169260718204981
      },
      "individual_log_probs": {
        "plant": -4.807376861572266
      },
      "max_prob": 0.008169260718204981,
      "max_log_prob": -4.807376861572266,
      "best_answer": "plant",
      "total_prob": 0.008169260718204981,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 12,
      "generated_answer": "Montclair State University.",
      "gold_answers": [
        "Montclair State University"
      ],
      "reference_answer": "Montclair State University",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Montclair State University": 0.9992479352387347
      },
      "individual_log_probs": {
        "Montclair State University": -0.0007523477038375859
      },
      "max_prob": 0.9992479352387347,
      "max_log_prob": -0.0007523477038375859,
      "best_answer": "Montclair State University",
      "total_prob": 0.9992479352387347,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 13,
      "generated_answer": "Soha Ali Khan",
      "gold_answers": [
        "Soha Ali Khan"
      ],
      "reference_answer": "Soha Ali Khan",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Soha Ali Khan": 0.9997544776664423
      },
      "individual_log_probs": {
        "Soha Ali Khan": -0.0002455524791002972
      },
      "max_prob": 0.9997544776664423,
      "max_log_prob": -0.0002455524791002972,
      "best_answer": "Soha Ali Khan",
      "total_prob": 0.9997544776664423,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 14,
      "generated_answer": "No",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "yes": 0.00035031746872389764
      },
      "individual_log_probs": {
        "yes": -7.956670761108398
      },
      "max_prob": 0.00035031746872389764,
      "max_log_prob": -7.956670761108398,
      "best_answer": "yes",
      "total_prob": 0.00035031746872389764,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 15,
      "generated_answer": "Bodo League massacre.",
      "gold_answers": [
        "Bodo League massacre"
      ],
      "reference_answer": "Bodo League massacre",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Bodo League massacre": 0.9633155590827219
      },
      "individual_log_probs": {
        "Bodo League massacre": -0.03737423749316804
      },
      "max_prob": 0.9633155590827219,
      "max_log_prob": -0.03737423749316804,
      "best_answer": "Bodo League massacre",
      "total_prob": 0.9633155590827219,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 16,
      "generated_answer": "Ariocarpus.",
      "gold_answers": [
        "Ariocarpus"
      ],
      "reference_answer": "Ariocarpus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ariocarpus": 0.9996870294616559
      },
      "individual_log_probs": {
        "Ariocarpus": -0.0003130195238441047
      },
      "max_prob": 0.9996870294616559,
      "max_log_prob": -0.0003130195238441047,
      "best_answer": "Ariocarpus",
      "total_prob": 0.9996870294616559,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 17,
      "generated_answer": "Hibbing, Minnesota",
      "gold_answers": [
        "761"
      ],
      "reference_answer": "761",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "761": 9.240137546833961e-07
      },
      "individual_log_probs": {
        "761": -13.894538879394531
      },
      "max_prob": 9.240137546833961e-07,
      "max_log_prob": -13.894538879394531,
      "best_answer": "761",
      "total_prob": 9.240137546833961e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 18,
      "generated_answer": "Cnut the Great.",
      "gold_answers": [
        "Cnut the Great"
      ],
      "reference_answer": "Cnut the Great",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Cnut the Great": 0.9864138836000288
      },
      "individual_log_probs": {
        "Cnut the Great": -0.013679252210749837
      },
      "max_prob": 0.9864138836000288,
      "max_log_prob": -0.013679252210749837,
      "best_answer": "Cnut the Great",
      "total_prob": 0.9864138836000288,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 19,
      "generated_answer": "Channel V",
      "gold_answers": [
        "Channel V"
      ],
      "reference_answer": "Channel V",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Channel V": 0.9987831624106887
      },
      "individual_log_probs": {
        "Channel V": -0.0012175785373074177
      },
      "max_prob": 0.9987831624106887,
      "max_log_prob": -0.0012175785373074177,
      "best_answer": "Channel V",
      "total_prob": 0.9987831624106887,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 20,
      "generated_answer": "Oscar Hammerstein II",
      "gold_answers": [
        "Albert Camus"
      ],
      "reference_answer": "Albert Camus",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Albert Camus": 2.448298054266102e-10
      },
      "individual_log_probs": {
        "Albert Camus": -22.130457818508148
      },
      "max_prob": 2.448298054266102e-10,
      "max_log_prob": -22.130457818508148,
      "best_answer": "Albert Camus",
      "total_prob": 2.448298054266102e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 21,
      "generated_answer": "Paul Marcinkus",
      "gold_answers": [
        "Paul Marcinkus"
      ],
      "reference_answer": "Paul Marcinkus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Paul Marcinkus": 0.9991778614771301
      },
      "individual_log_probs": {
        "Paul Marcinkus": -0.0008224766640907433
      },
      "max_prob": 0.9991778614771301,
      "max_log_prob": -0.0008224766640907433,
      "best_answer": "Paul Marcinkus",
      "total_prob": 0.9991778614771301,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 22,
      "generated_answer": "1886",
      "gold_answers": [
        "1904"
      ],
      "reference_answer": "1904",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1904": 1.1085494870022502e-07
      },
      "individual_log_probs": {
        "1904": -16.015043258666992
      },
      "max_prob": 1.1085494870022502e-07,
      "max_log_prob": -16.015043258666992,
      "best_answer": "1904",
      "total_prob": 1.1085494870022502e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 23,
      "generated_answer": "Ralph Fiennes",
      "gold_answers": [
        "Ralph Fiennes"
      ],
      "reference_answer": "Ralph Fiennes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ralph Fiennes": 0.9997280162166681
      },
      "individual_log_probs": {
        "Ralph Fiennes": -0.0002720207776292227
      },
      "max_prob": 0.9997280162166681,
      "max_log_prob": -0.0002720207776292227,
      "best_answer": "Ralph Fiennes",
      "total_prob": 0.9997280162166681,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 24,
      "generated_answer": "Tenth place.",
      "gold_answers": [
        "runner-up"
      ],
      "reference_answer": "runner-up",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "runner-up": 1.8644403510678256e-08
      },
      "individual_log_probs": {
        "runner-up": -17.79771981574595
      },
      "max_prob": 1.8644403510678256e-08,
      "max_log_prob": -17.79771981574595,
      "best_answer": "runner-up",
      "total_prob": 1.8644403510678256e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 25,
      "generated_answer": "Swedish",
      "gold_answers": [
        "Swedish"
      ],
      "reference_answer": "Swedish",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Swedish": 0.9936187990839561
      },
      "individual_log_probs": {
        "Swedish": -0.006401647808843336
      },
      "max_prob": 0.9936187990839561,
      "max_log_prob": -0.006401647808843336,
      "best_answer": "Swedish",
      "total_prob": 0.9936187990839561,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 26,
      "generated_answer": "Kenny Hotz is a contributor for Vice Magazine.",
      "gold_answers": [
        "Vice"
      ],
      "reference_answer": "Vice",
      "em_score": 0.0,
      "f1_score": 0.25,
      "precision": 0.14285714285714285,
      "recall": 1.0,
      "individual_probs": {
        "Vice": 0.031301071318184735
      },
      "individual_log_probs": {
        "Vice": -3.4641029546037316
      },
      "max_prob": 0.031301071318184735,
      "max_log_prob": -3.4641029546037316,
      "best_answer": "Vice",
      "total_prob": 0.031301071318184735,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 27,
      "generated_answer": "The Preppie Murder.",
      "gold_answers": [
        "The Preppie Murder"
      ],
      "reference_answer": "The Preppie Murder",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Preppie Murder": 0.986897099837969
      },
      "individual_log_probs": {
        "The Preppie Murder": -0.013189500466950221
      },
      "max_prob": 0.986897099837969,
      "max_log_prob": -0.013189500466950221,
      "best_answer": "The Preppie Murder",
      "total_prob": 0.986897099837969,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 28,
      "generated_answer": "Carolina Kostner",
      "gold_answers": [
        "Isolde Kostner"
      ],
      "reference_answer": "Isolde Kostner",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Isolde Kostner": 1.7023939111543088e-07
      },
      "individual_log_probs": {
        "Isolde Kostner": -15.586060207424111
      },
      "max_prob": 1.7023939111543088e-07,
      "max_log_prob": -15.586060207424111,
      "best_answer": "Isolde Kostner",
      "total_prob": 1.7023939111543088e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 29,
      "generated_answer": "Northwestern University",
      "gold_answers": [
        "Beverly Hills High School"
      ],
      "reference_answer": "Beverly Hills High School",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Beverly Hills High School": 0.00022123466423210747
      },
      "individual_log_probs": {
        "Beverly Hills High School": -8.41628659068374
      },
      "max_prob": 0.00022123466423210747,
      "max_log_prob": -8.41628659068374,
      "best_answer": "Beverly Hills High School",
      "total_prob": 0.00022123466423210747,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 30,
      "generated_answer": "Fearless",
      "gold_answers": [
        "fifth album"
      ],
      "reference_answer": "fifth album",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "fifth album": 1.0761593447885466e-09
      },
      "individual_log_probs": {
        "fifth album": -20.649867296218872
      },
      "max_prob": 1.0761593447885466e-09,
      "max_log_prob": -20.649867296218872,
      "best_answer": "fifth album",
      "total_prob": 1.0761593447885466e-09,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 31,
      "generated_answer": "Welterweight",
      "gold_answers": [
        "Welterweight"
      ],
      "reference_answer": "Welterweight",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Welterweight": 0.9908458233628368
      },
      "individual_log_probs": {
        "Welterweight": -0.009196333584100103
      },
      "max_prob": 0.9908458233628368,
      "max_log_prob": -0.009196333584100103,
      "best_answer": "Welterweight",
      "total_prob": 0.9908458233628368,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 32,
      "generated_answer": "Times Beach",
      "gold_answers": [
        "Times Beach"
      ],
      "reference_answer": "Times Beach",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Times Beach": 0.9997301812319309
      },
      "individual_log_probs": {
        "Times Beach": -0.00026985517570210504
      },
      "max_prob": 0.9997301812319309,
      "max_log_prob": -0.00026985517570210504,
      "best_answer": "Times Beach",
      "total_prob": 0.9997301812319309,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 33,
      "generated_answer": "Beverly Hills Supper Club fire",
      "gold_answers": [
        "Beverly Hills Supper Club fire"
      ],
      "reference_answer": "Beverly Hills Supper Club fire",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Beverly Hills Supper Club fire": 0.6380529723930078
      },
      "individual_log_probs": {
        "Beverly Hills Supper Club fire": -0.44933397025488375
      },
      "max_prob": 0.6380529723930078,
      "max_log_prob": -0.44933397025488375,
      "best_answer": "Beverly Hills Supper Club fire",
      "total_prob": 0.6380529723930078,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 34,
      "generated_answer": "Defensive end.",
      "gold_answers": [
        "quarterback"
      ],
      "reference_answer": "quarterback",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "quarterback": 1.1032621015402473e-05
      },
      "individual_log_probs": {
        "quarterback": -11.414654126871028
      },
      "max_prob": 1.1032621015402473e-05,
      "max_log_prob": -11.414654126871028,
      "best_answer": "quarterback",
      "total_prob": 1.1032621015402473e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 35,
      "generated_answer": "Kelly Gregg",
      "gold_answers": [
        "Dusty Dvoracek"
      ],
      "reference_answer": "Dusty Dvoracek",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Dusty Dvoracek": 1.7510556622997158e-10
      },
      "individual_log_probs": {
        "Dusty Dvoracek": -22.465632088278653
      },
      "max_prob": 1.7510556622997158e-10,
      "max_log_prob": -22.465632088278653,
      "best_answer": "Dusty Dvoracek",
      "total_prob": 1.7510556622997158e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 36,
      "generated_answer": "No.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "no": 0.007467148577485998
      },
      "individual_log_probs": {
        "no": -4.897242069244385
      },
      "max_prob": 0.007467148577485998,
      "max_log_prob": -4.897242069244385,
      "best_answer": "no",
      "total_prob": 0.007467148577485998,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 37,
      "generated_answer": "Texas Rangers",
      "gold_answers": [
        "Texas Rangers"
      ],
      "reference_answer": "Texas Rangers",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Texas Rangers": 0.7140163687809841
      },
      "individual_log_probs": {
        "Texas Rangers": -0.33684939144177406
      },
      "max_prob": 0.7140163687809841,
      "max_log_prob": -0.33684939144177406,
      "best_answer": "Texas Rangers",
      "total_prob": 0.7140163687809841,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 38,
      "generated_answer": "Slavs.",
      "gold_answers": [
        "Germanic, Baltic and Thracian tribes"
      ],
      "reference_answer": "Germanic, Baltic and Thracian tribes",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Germanic, Baltic and Thracian tribes": 3.3797893351804507e-22
      },
      "individual_log_probs": {
        "Germanic, Baltic and Thracian tribes": -49.43905866518617
      },
      "max_prob": 3.3797893351804507e-22,
      "max_log_prob": -49.43905866518617,
      "best_answer": "Germanic, Baltic and Thracian tribes",
      "total_prob": 3.3797893351804507e-22,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 39,
      "generated_answer": "American",
      "gold_answers": [
        "American"
      ],
      "reference_answer": "American",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "American": 0.9916470316329581
      },
      "individual_log_probs": {
        "American": -0.008388049900531769
      },
      "max_prob": 0.9916470316329581,
      "max_log_prob": -0.008388049900531769,
      "best_answer": "American",
      "total_prob": 0.9916470316329581,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 40,
      "generated_answer": "Brian Smith",
      "gold_answers": [
        "Mario Lemieux"
      ],
      "reference_answer": "Mario Lemieux",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Mario Lemieux": 2.078266092737061e-06
      },
      "individual_log_probs": {
        "Mario Lemieux": -13.08397662115749
      },
      "max_prob": 2.078266092737061e-06,
      "max_log_prob": -13.08397662115749,
      "best_answer": "Mario Lemieux",
      "total_prob": 2.078266092737061e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 41,
      "generated_answer": "Mumbai",
      "gold_answers": [
        "Mumbai"
      ],
      "reference_answer": "Mumbai",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mumbai": 0.7731214615074443
      },
      "individual_log_probs": {
        "Mumbai": -0.257319112715777
      },
      "max_prob": 0.7731214615074443,
      "max_log_prob": -0.257319112715777,
      "best_answer": "Mumbai",
      "total_prob": 0.7731214615074443,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 42,
      "generated_answer": "Alexander Scriabin",
      "gold_answers": [
        "Alexander Scriabin"
      ],
      "reference_answer": "Alexander Scriabin",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Alexander Scriabin": 0.865907907730518
      },
      "individual_log_probs": {
        "Alexander Scriabin": -0.1439767181869911
      },
      "max_prob": 0.865907907730518,
      "max_log_prob": -0.1439767181869911,
      "best_answer": "Alexander Scriabin",
      "total_prob": 0.865907907730518,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 43,
      "generated_answer": "1959 (Little Queenie) to 1963 (Chuck Berry on Stage) is 4 years.",
      "gold_answers": [
        "Their biggest hit was \"Tall Cool One\", first released in 1959"
      ],
      "reference_answer": "Their biggest hit was \"Tall Cool One\", first released in 1959",
      "em_score": 0.0,
      "f1_score": 0.08695652173913043,
      "precision": 0.08333333333333333,
      "recall": 0.09090909090909091,
      "individual_probs": {
        "Their biggest hit was \"Tall Cool One\", first released in 1959": 2.733285994260259e-18
      },
      "individual_log_probs": {
        "Their biggest hit was \"Tall Cool One\", first released in 1959": -40.44102712761378
      },
      "max_prob": 2.733285994260259e-18,
      "max_log_prob": -40.44102712761378,
      "best_answer": "Their biggest hit was \"Tall Cool One\", first released in 1959",
      "total_prob": 2.733285994260259e-18,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 44,
      "generated_answer": "Jawaharlal Nehru Stadium",
      "gold_answers": [
        "first Prime Minister of India"
      ],
      "reference_answer": "first Prime Minister of India",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "first Prime Minister of India": 8.244286576486224e-14
      },
      "individual_log_probs": {
        "first Prime Minister of India": -30.126670877667493
      },
      "max_prob": 8.244286576486224e-14,
      "max_log_prob": -30.126670877667493,
      "best_answer": "first Prime Minister of India",
      "total_prob": 8.244286576486224e-14,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 45,
      "generated_answer": "The United Kingdom",
      "gold_answers": [
        "Ireland"
      ],
      "reference_answer": "Ireland",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Ireland": 0.012130337468534716
      },
      "individual_log_probs": {
        "Ireland": -4.412045735429274
      },
      "max_prob": 0.012130337468534716,
      "max_log_prob": -4.412045735429274,
      "best_answer": "Ireland",
      "total_prob": 0.012130337468534716,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 46,
      "generated_answer": "Not provided in the context.",
      "gold_answers": [
        "72-78"
      ],
      "reference_answer": "72-78",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "72-78": 1.4442300054545856e-09
      },
      "individual_log_probs": {
        "72-78": -20.355689525604248
      },
      "max_prob": 1.4442300054545856e-09,
      "max_log_prob": -20.355689525604248,
      "best_answer": "72-78",
      "total_prob": 1.4442300054545856e-09,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 47,
      "generated_answer": "Heroica Matamoros",
      "gold_answers": [
        "Heroica Matamoros"
      ],
      "reference_answer": "Heroica Matamoros",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Heroica Matamoros": 0.5793929279924646
      },
      "individual_log_probs": {
        "Heroica Matamoros": -0.5457743994644488
      },
      "max_prob": 0.5793929279924646,
      "max_log_prob": -0.5457743994644488,
      "best_answer": "Heroica Matamoros",
      "total_prob": 0.5793929279924646,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 48,
      "generated_answer": "General Benedict Arnold",
      "gold_answers": [
        "Benedict Arnold"
      ],
      "reference_answer": "Benedict Arnold",
      "em_score": 0.0,
      "f1_score": 0.8,
      "precision": 0.6666666666666666,
      "recall": 1.0,
      "individual_probs": {
        "Benedict Arnold": 0.15271476083727228
      },
      "individual_log_probs": {
        "Benedict Arnold": -1.8791834058283712
      },
      "max_prob": 0.15271476083727228,
      "max_log_prob": -1.8791834058283712,
      "best_answer": "Benedict Arnold",
      "total_prob": 0.15271476083727228,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 49,
      "generated_answer": "Campbell Field (Colorado)",
      "gold_answers": [
        "South Kesteven Sports Stadium"
      ],
      "reference_answer": "South Kesteven Sports Stadium",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "South Kesteven Sports Stadium": 1.6771462342263808e-18
      },
      "individual_log_probs": {
        "South Kesteven Sports Stadium": -40.92943799495697
      },
      "max_prob": 1.6771462342263808e-18,
      "max_log_prob": -40.92943799495697,
      "best_answer": "South Kesteven Sports Stadium",
      "total_prob": 1.6771462342263808e-18,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 50,
      "generated_answer": "None, since Princess Ōku was born in 661, not 1963.",
      "gold_answers": [
        "Aiko, Princess Toshi"
      ],
      "reference_answer": "Aiko, Princess Toshi",
      "em_score": 0.0,
      "f1_score": 0.15384615384615383,
      "precision": 0.1,
      "recall": 0.3333333333333333,
      "individual_probs": {
        "Aiko, Princess Toshi": 2.2257831600271336e-10
      },
      "individual_log_probs": {
        "Aiko, Princess Toshi": -22.22574209421873
      },
      "max_prob": 2.2257831600271336e-10,
      "max_log_prob": -22.22574209421873,
      "best_answer": "Aiko, Princess Toshi",
      "total_prob": 2.2257831600271336e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 51,
      "generated_answer": "Wayne Elcock",
      "gold_answers": [
        "Marco Antonio Barrera"
      ],
      "reference_answer": "Marco Antonio Barrera",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Marco Antonio Barrera": 3.758676912073162e-08
      },
      "individual_log_probs": {
        "Marco Antonio Barrera": -17.096613733563572
      },
      "max_prob": 3.758676912073162e-08,
      "max_log_prob": -17.096613733563572,
      "best_answer": "Marco Antonio Barrera",
      "total_prob": 3.758676912073162e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 52,
      "generated_answer": "southwestern Europe",
      "gold_answers": [
        "Iberian Peninsula"
      ],
      "reference_answer": "Iberian Peninsula",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Iberian Peninsula": 0.03470601905546822
      },
      "individual_log_probs": {
        "Iberian Peninsula": -3.360842147259973
      },
      "max_prob": 0.03470601905546822,
      "max_log_prob": -3.360842147259973,
      "best_answer": "Iberian Peninsula",
      "total_prob": 0.03470601905546822,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 53,
      "generated_answer": "None.",
      "gold_answers": [
        "Hume Highway"
      ],
      "reference_answer": "Hume Highway",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Hume Highway": 3.1957067453662704e-07
      },
      "individual_log_probs": {
        "Hume Highway": -14.956287384033203
      },
      "max_prob": 3.1957067453662704e-07,
      "max_log_prob": -14.956287384033203,
      "best_answer": "Hume Highway",
      "total_prob": 3.1957067453662704e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 54,
      "generated_answer": "Lawrence Technological University",
      "gold_answers": [
        "University of North Texas"
      ],
      "reference_answer": "University of North Texas",
      "em_score": 0.0,
      "f1_score": 0.28571428571428575,
      "precision": 0.3333333333333333,
      "recall": 0.25,
      "individual_probs": {
        "University of North Texas": 0.011224526181905519
      },
      "individual_log_probs": {
        "University of North Texas": -4.489654057210828
      },
      "max_prob": 0.011224526181905519,
      "max_log_prob": -4.489654057210828,
      "best_answer": "University of North Texas",
      "total_prob": 0.011224526181905519,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 55,
      "generated_answer": "Killingholme B Power Station.",
      "gold_answers": [
        "the A160"
      ],
      "reference_answer": "the A160",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "the A160": 3.681395323106075e-06
      },
      "individual_log_probs": {
        "the A160": -12.512218713760376
      },
      "max_prob": 3.681395323106075e-06,
      "max_log_prob": -12.512218713760376,
      "best_answer": "the A160",
      "total_prob": 3.681395323106075e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 56,
      "generated_answer": "Italian",
      "gold_answers": [
        "American"
      ],
      "reference_answer": "American",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "American": 0.01146286370814587
      },
      "individual_log_probs": {
        "American": -4.468642711639404
      },
      "max_prob": 0.01146286370814587,
      "max_log_prob": -4.468642711639404,
      "best_answer": "American",
      "total_prob": 0.01146286370814587,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 57,
      "generated_answer": "Capsicum annuum",
      "gold_answers": [
        "\"Capsicum chinense\""
      ],
      "reference_answer": "\"Capsicum chinense\"",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "\"Capsicum chinense\"": 4.4903964656485056e-05
      },
      "individual_log_probs": {
        "\"Capsicum chinense\"": -10.010984467415255
      },
      "max_prob": 4.4903964656485056e-05,
      "max_log_prob": -10.010984467415255,
      "best_answer": "\"Capsicum chinense\"",
      "total_prob": 4.4903964656485056e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 58,
      "generated_answer": "\"Animal\"",
      "gold_answers": [
        "Hillary Clinton"
      ],
      "reference_answer": "Hillary Clinton",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Hillary Clinton": 2.457242435261196e-07
      },
      "individual_log_probs": {
        "Hillary Clinton": -15.219055891036987
      },
      "max_prob": 2.457242435261196e-07,
      "max_log_prob": -15.219055891036987,
      "best_answer": "Hillary Clinton",
      "total_prob": 2.457242435261196e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 59,
      "generated_answer": "Lynyrd Skynyrd",
      "gold_answers": [
        "Lynyrd Skynyrd"
      ],
      "reference_answer": "Lynyrd Skynyrd",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Lynyrd Skynyrd": 0.9943701509057257
      },
      "individual_log_probs": {
        "Lynyrd Skynyrd": -0.005645756426702064
      },
      "max_prob": 0.9943701509057257,
      "max_log_prob": -0.005645756426702064,
      "best_answer": "Lynyrd Skynyrd",
      "total_prob": 0.9943701509057257,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 60,
      "generated_answer": "My Bloody Valentine",
      "gold_answers": [
        "Herzeleid"
      ],
      "reference_answer": "Herzeleid",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Herzeleid": 1.8201410596499908e-07
      },
      "individual_log_probs": {
        "Herzeleid": -15.519181647570804
      },
      "max_prob": 1.8201410596499908e-07,
      "max_log_prob": -15.519181647570804,
      "best_answer": "Herzeleid",
      "total_prob": 1.8201410596499908e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 61,
      "generated_answer": "There is no mention of CareFusion in the given context. The context only talks about Exit Tunes, Inc. and",
      "gold_answers": [
        "1897"
      ],
      "reference_answer": "1897",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1897": 1.3715112238168694e-07
      },
      "individual_log_probs": {
        "1897": -15.80218243598938
      },
      "max_prob": 1.3715112238168694e-07,
      "max_log_prob": -15.80218243598938,
      "best_answer": "1897",
      "total_prob": 1.3715112238168694e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 62,
      "generated_answer": "None, as Mohamed Salah is from Egypt, not Basyoun.",
      "gold_answers": [
        "Ahmed Elmohamady"
      ],
      "reference_answer": "Ahmed Elmohamady",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Ahmed Elmohamady": 5.5161734883671805e-06
      },
      "individual_log_probs": {
        "Ahmed Elmohamady": -12.107826146684602
      },
      "max_prob": 5.5161734883671805e-06,
      "max_log_prob": -12.107826146684602,
      "best_answer": "Ahmed Elmohamady",
      "total_prob": 5.5161734883671805e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 63,
      "generated_answer": "computer graphics and animatronics",
      "gold_answers": [
        "animation"
      ],
      "reference_answer": "animation",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "animation": 0.002632058366703298
      },
      "individual_log_probs": {
        "animation": -5.93998908996582
      },
      "max_prob": 0.002632058366703298,
      "max_log_prob": -5.93998908996582,
      "best_answer": "animation",
      "total_prob": 0.002632058366703298,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 64,
      "generated_answer": "Jeff Healey",
      "gold_answers": [
        "Jeff Healey"
      ],
      "reference_answer": "Jeff Healey",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Jeff Healey": 0.9995716044518489
      },
      "individual_log_probs": {
        "Jeff Healey": -0.0004284873357391916
      },
      "max_prob": 0.9995716044518489,
      "max_log_prob": -0.0004284873357391916,
      "best_answer": "Jeff Healey",
      "total_prob": 0.9995716044518489,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 65,
      "generated_answer": "The Royal Navy.",
      "gold_answers": [
        "British Armed Forces"
      ],
      "reference_answer": "British Armed Forces",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "British Armed Forces": 0.00235446717364431
      },
      "individual_log_probs": {
        "British Armed Forces": -6.051440830342472
      },
      "max_prob": 0.00235446717364431,
      "max_log_prob": -6.051440830342472,
      "best_answer": "British Armed Forces",
      "total_prob": 0.00235446717364431,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 66,
      "generated_answer": "Roja",
      "gold_answers": [
        "Kadhalan"
      ],
      "reference_answer": "Kadhalan",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Kadhalan": 0.002464755414817092
      },
      "individual_log_probs": {
        "Kadhalan": -6.005662699608365
      },
      "max_prob": 0.002464755414817092,
      "max_log_prob": -6.005662699608365,
      "best_answer": "Kadhalan",
      "total_prob": 0.002464755414817092,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 67,
      "generated_answer": "Dnipro.",
      "gold_answers": [
        "Cherepanov Hill"
      ],
      "reference_answer": "Cherepanov Hill",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Cherepanov Hill": 3.772533498002559e-08
      },
      "individual_log_probs": {
        "Cherepanov Hill": -17.09293395280838
      },
      "max_prob": 3.772533498002559e-08,
      "max_log_prob": -17.09293395280838,
      "best_answer": "Cherepanov Hill",
      "total_prob": 3.772533498002559e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 68,
      "generated_answer": "Arantxa Sánchez Vicario",
      "gold_answers": [
        "\"Arantxa\" Sánchez Vicario"
      ],
      "reference_answer": "\"Arantxa\" Sánchez Vicario",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "\"Arantxa\" Sánchez Vicario": 1.3576931730818307e-13
      },
      "individual_log_probs": {
        "\"Arantxa\" Sánchez Vicario": -29.627819145607646
      },
      "max_prob": 1.3576931730818307e-13,
      "max_log_prob": -29.627819145607646,
      "best_answer": "\"Arantxa\" Sánchez Vicario",
      "total_prob": 1.3576931730818307e-13,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 69,
      "generated_answer": "Alabama",
      "gold_answers": [
        "University of Alabama"
      ],
      "reference_answer": "University of Alabama",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 1.0,
      "recall": 0.3333333333333333,
      "individual_probs": {
        "University of Alabama": 0.18128368965739716
      },
      "individual_log_probs": {
        "University of Alabama": -1.707692128547933
      },
      "max_prob": 0.18128368965739716,
      "max_log_prob": -1.707692128547933,
      "best_answer": "University of Alabama",
      "total_prob": 0.18128368965739716,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 70,
      "generated_answer": "KFC",
      "gold_answers": [
        "Pizza Hut"
      ],
      "reference_answer": "Pizza Hut",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Pizza Hut": 9.29117052406405e-05
      },
      "individual_log_probs": {
        "Pizza Hut": -9.283860921796986
      },
      "max_prob": 9.29117052406405e-05,
      "max_log_prob": -9.283860921796986,
      "best_answer": "Pizza Hut",
      "total_prob": 9.29117052406405e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 71,
      "generated_answer": "1987",
      "gold_answers": [
        "1939"
      ],
      "reference_answer": "1939",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1939": 2.747020749188521e-05
      },
      "individual_log_probs": {
        "1939": -10.502408504486084
      },
      "max_prob": 2.747020749188521e-05,
      "max_log_prob": -10.502408504486084,
      "best_answer": "1939",
      "total_prob": 2.747020749188521e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 72,
      "generated_answer": "1957",
      "gold_answers": [
        "1962"
      ],
      "reference_answer": "1962",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1962": 0.18627452398800226
      },
      "individual_log_probs": {
        "1962": -1.6805337579717161
      },
      "max_prob": 0.18627452398800226,
      "max_log_prob": -1.6805337579717161,
      "best_answer": "1962",
      "total_prob": 0.18627452398800226,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 73,
      "generated_answer": "Harbour Square Park",
      "gold_answers": [
        "Metreon"
      ],
      "reference_answer": "Metreon",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Metreon": 1.6456473233460522e-10
      },
      "individual_log_probs": {
        "Metreon": -22.527717113494873
      },
      "max_prob": 1.6456473233460522e-10,
      "max_log_prob": -22.527717113494873,
      "best_answer": "Metreon",
      "total_prob": 1.6456473233460522e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 74,
      "generated_answer": "Robert van Tonder",
      "gold_answers": [
        "Viktor Uspaskich"
      ],
      "reference_answer": "Viktor Uspaskich",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Viktor Uspaskich": 1.122392428311743e-12
      },
      "individual_log_probs": {
        "Viktor Uspaskich": -27.51555861212546
      },
      "max_prob": 1.122392428311743e-12,
      "max_log_prob": -27.51555861212546,
      "best_answer": "Viktor Uspaskich",
      "total_prob": 1.122392428311743e-12,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 75,
      "generated_answer": "Spacemen 3",
      "gold_answers": [
        "Spacemen 3"
      ],
      "reference_answer": "Spacemen 3",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Spacemen 3": 0.9989727942986546
      },
      "individual_log_probs": {
        "Spacemen 3": -0.0010277336386863567
      },
      "max_prob": 0.9989727942986546,
      "max_log_prob": -0.0010277336386863567,
      "best_answer": "Spacemen 3",
      "total_prob": 0.9989727942986546,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 76,
      "generated_answer": "Stephen King.",
      "gold_answers": [
        "Stephen King"
      ],
      "reference_answer": "Stephen King",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Stephen King": 0.9916373977525998
      },
      "individual_log_probs": {
        "Stephen King": -0.008397764977416955
      },
      "max_prob": 0.9916373977525998,
      "max_log_prob": -0.008397764977416955,
      "best_answer": "Stephen King",
      "total_prob": 0.9916373977525998,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 77,
      "generated_answer": "James Clerk Maxwell helped to develop the concept of coherence.",
      "gold_answers": [
        "classical theory of electromagnetic radiation"
      ],
      "reference_answer": "classical theory of electromagnetic radiation",
      "em_score": 0.0,
      "f1_score": 0.14285714285714285,
      "precision": 0.1111111111111111,
      "recall": 0.2,
      "individual_probs": {
        "classical theory of electromagnetic radiation": 3.96733131848783e-08
      },
      "individual_log_probs": {
        "classical theory of electromagnetic radiation": -17.042587087256834
      },
      "max_prob": 3.96733131848783e-08,
      "max_log_prob": -17.042587087256834,
      "best_answer": "classical theory of electromagnetic radiation",
      "total_prob": 3.96733131848783e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 78,
      "generated_answer": "The London Connection",
      "gold_answers": [
        "The London Connection"
      ],
      "reference_answer": "The London Connection",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The London Connection": 0.6512430241814137
      },
      "individual_log_probs": {
        "The London Connection": -0.4288723974016193
      },
      "max_prob": 0.6512430241814137,
      "max_log_prob": -0.4288723974016193,
      "best_answer": "The London Connection",
      "total_prob": 0.6512430241814137,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 79,
      "generated_answer": "Spanish",
      "gold_answers": [
        "Mexican"
      ],
      "reference_answer": "Mexican",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Mexican": 8.357532850590143e-06
      },
      "individual_log_probs": {
        "Mexican": -11.692347287988014
      },
      "max_prob": 8.357532850590143e-06,
      "max_log_prob": -11.692347287988014,
      "best_answer": "Mexican",
      "total_prob": 8.357532850590143e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 80,
      "generated_answer": "February 16, 2012",
      "gold_answers": [
        "December 7, 2008"
      ],
      "reference_answer": "December 7, 2008",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "December 7, 2008": 2.142190768184679e-12
      },
      "individual_log_probs": {
        "December 7, 2008": -26.869192087091506
      },
      "max_prob": 2.142190768184679e-12,
      "max_log_prob": -26.869192087091506,
      "best_answer": "December 7, 2008",
      "total_prob": 2.142190768184679e-12,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 81,
      "generated_answer": "Yes.",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "yes": 0.001192835033285851
      },
      "individual_log_probs": {
        "yes": -6.731422424316406
      },
      "max_prob": 0.001192835033285851,
      "max_log_prob": -6.731422424316406,
      "best_answer": "yes",
      "total_prob": 0.001192835033285851,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 82,
      "generated_answer": "Football.",
      "gold_answers": [
        "Football"
      ],
      "reference_answer": "Football",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Football": 0.9764783322109999
      },
      "individual_log_probs": {
        "Football": -0.02380271814763546
      },
      "max_prob": 0.9764783322109999,
      "max_log_prob": -0.02380271814763546,
      "best_answer": "Football",
      "total_prob": 0.9764783322109999,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 83,
      "generated_answer": "Betty Shabazz.",
      "gold_answers": [
        "Betty X"
      ],
      "reference_answer": "Betty X",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Betty X": 0.0001728265312567293
      },
      "individual_log_probs": {
        "Betty X": -8.663222176014642
      },
      "max_prob": 0.0001728265312567293,
      "max_log_prob": -8.663222176014642,
      "best_answer": "Betty X",
      "total_prob": 0.0001728265312567293,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 84,
      "generated_answer": "Golden Rax Pizzabuffet",
      "gold_answers": [
        "Extreme Pizza"
      ],
      "reference_answer": "Extreme Pizza",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Extreme Pizza": 0.37050500557221244
      },
      "individual_log_probs": {
        "Extreme Pizza": -0.9928883240208961
      },
      "max_prob": 0.37050500557221244,
      "max_log_prob": -0.9928883240208961,
      "best_answer": "Extreme Pizza",
      "total_prob": 0.37050500557221244,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 85,
      "generated_answer": "Howard Jones",
      "gold_answers": [
        "Anna Mae Bullock"
      ],
      "reference_answer": "Anna Mae Bullock",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Anna Mae Bullock": 2.998606380599256e-09
      },
      "individual_log_probs": {
        "Anna Mae Bullock": -19.62511819601059
      },
      "max_prob": 2.998606380599256e-09,
      "max_log_prob": -19.62511819601059,
      "best_answer": "Anna Mae Bullock",
      "total_prob": 2.998606380599256e-09,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 86,
      "generated_answer": "Michie Stadium",
      "gold_answers": [
        "Michie Stadium"
      ],
      "reference_answer": "Michie Stadium",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Michie Stadium": 0.9988794925859443
      },
      "individual_log_probs": {
        "Michie Stadium": -0.0011211356518288085
      },
      "max_prob": 0.9988794925859443,
      "max_log_prob": -0.0011211356518288085,
      "best_answer": "Michie Stadium",
      "total_prob": 0.9988794925859443,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 87,
      "generated_answer": "Pacific Ocean",
      "gold_answers": [
        "the Pacific Ocean"
      ],
      "reference_answer": "the Pacific Ocean",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "the Pacific Ocean": 0.0010581540829964595
      },
      "individual_log_probs": {
        "the Pacific Ocean": -6.851229320047423
      },
      "max_prob": 0.0010581540829964595,
      "max_log_prob": -6.851229320047423,
      "best_answer": "the Pacific Ocean",
      "total_prob": 0.0010581540829964595,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 88,
      "generated_answer": "Angela Nissel",
      "gold_answers": [
        "Angela R. Nissel"
      ],
      "reference_answer": "Angela R. Nissel",
      "em_score": 0.0,
      "f1_score": 0.8,
      "precision": 1.0,
      "recall": 0.6666666666666666,
      "individual_probs": {
        "Angela R. Nissel": 1.3666676205378802e-08
      },
      "individual_log_probs": {
        "Angela R. Nissel": -18.10830536095591
      },
      "max_prob": 1.3666676205378802e-08,
      "max_log_prob": -18.10830536095591,
      "best_answer": "Angela R. Nissel",
      "total_prob": 1.3666676205378802e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 89,
      "generated_answer": "E. LyondellBasell",
      "gold_answers": [
        "Netherlands"
      ],
      "reference_answer": "Netherlands",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Netherlands": 1.1837197933130141e-07
      },
      "individual_log_probs": {
        "Netherlands": -15.94943380355835
      },
      "max_prob": 1.1837197933130141e-07,
      "max_log_prob": -15.94943380355835,
      "best_answer": "Netherlands",
      "total_prob": 1.1837197933130141e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 90,
      "generated_answer": "Alessandro Nannini",
      "gold_answers": [
        "Alessandro Nannini"
      ],
      "reference_answer": "Alessandro Nannini",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Alessandro Nannini": 0.998907327378797
      },
      "individual_log_probs": {
        "Alessandro Nannini": -0.001093270023147852
      },
      "max_prob": 0.998907327378797,
      "max_log_prob": -0.001093270023147852,
      "best_answer": "Alessandro Nannini",
      "total_prob": 0.998907327378797,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 91,
      "generated_answer": "Vincent van Gogh",
      "gold_answers": [
        "Theodorus \"Theo\" van Gogh"
      ],
      "reference_answer": "Theodorus \"Theo\" van Gogh",
      "em_score": 0.0,
      "f1_score": 0.5714285714285715,
      "precision": 0.6666666666666666,
      "recall": 0.5,
      "individual_probs": {
        "Theodorus \"Theo\" van Gogh": 3.696584581272618e-10
      },
      "individual_log_probs": {
        "Theodorus \"Theo\" van Gogh": -21.718441622739192
      },
      "max_prob": 3.696584581272618e-10,
      "max_log_prob": -21.718441622739192,
      "best_answer": "Theodorus \"Theo\" van Gogh",
      "total_prob": 3.696584581272618e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 92,
      "generated_answer": "Indian National Army trials",
      "gold_answers": [
        "Indian Rebellion of 1857"
      ],
      "reference_answer": "Indian Rebellion of 1857",
      "em_score": 0.0,
      "f1_score": 0.25,
      "precision": 0.25,
      "recall": 0.25,
      "individual_probs": {
        "Indian Rebellion of 1857": 5.135703155584967e-07
      },
      "individual_log_probs": {
        "Indian Rebellion of 1857": -14.48187888305074
      },
      "max_prob": 5.135703155584967e-07,
      "max_log_prob": -14.48187888305074,
      "best_answer": "Indian Rebellion of 1857",
      "total_prob": 5.135703155584967e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 93,
      "generated_answer": "Yes",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "yes": 0.0007165174613336859
      },
      "individual_log_probs": {
        "yes": -7.241107940673828
      },
      "max_prob": 0.0007165174613336859,
      "max_log_prob": -7.241107940673828,
      "best_answer": "yes",
      "total_prob": 0.0007165174613336859,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 94,
      "generated_answer": "Not mentioned in the context.",
      "gold_answers": [
        "Italian pianist and composer"
      ],
      "reference_answer": "Italian pianist and composer",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Italian pianist and composer": 1.5922373353874604e-06
      },
      "individual_log_probs": {
        "Italian pianist and composer": -13.350370401636155
      },
      "max_prob": 1.5922373353874604e-06,
      "max_log_prob": -13.350370401636155,
      "best_answer": "Italian pianist and composer",
      "total_prob": 1.5922373353874604e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 95,
      "generated_answer": "Red Velvet's Joy.",
      "gold_answers": [
        "actress"
      ],
      "reference_answer": "actress",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "actress": 0.0053025769930498436
      },
      "individual_log_probs": {
        "actress": -5.23956235148944
      },
      "max_prob": 0.0053025769930498436,
      "max_log_prob": -5.23956235148944,
      "best_answer": "actress",
      "total_prob": 0.0053025769930498436,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 96,
      "generated_answer": "Buenos Aires",
      "gold_answers": [
        "Amsterdam"
      ],
      "reference_answer": "Amsterdam",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Amsterdam": 0.000807828622596889
      },
      "individual_log_probs": {
        "Amsterdam": -7.121160622686148
      },
      "max_prob": 0.000807828622596889,
      "max_log_prob": -7.121160622686148,
      "best_answer": "Amsterdam",
      "total_prob": 0.000807828622596889,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 97,
      "generated_answer": "No.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "no": 0.001325206046920754
      },
      "individual_log_probs": {
        "no": -6.626187324523926
      },
      "max_prob": 0.001325206046920754,
      "max_log_prob": -6.626187324523926,
      "best_answer": "no",
      "total_prob": 0.001325206046920754,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 98,
      "generated_answer": "Nickelodeon",
      "gold_answers": [
        "Nickelodeon"
      ],
      "reference_answer": "Nickelodeon",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Nickelodeon": 0.9794670806535297
      },
      "individual_log_probs": {
        "Nickelodeon": -0.020746650479281925
      },
      "max_prob": 0.9794670806535297,
      "max_log_prob": -0.020746650479281925,
      "best_answer": "Nickelodeon",
      "total_prob": 0.9794670806535297,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    },
    {
      "sample_idx": 99,
      "generated_answer": "Glee.",
      "gold_answers": [
        "Project Runway"
      ],
      "reference_answer": "Project Runway",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Project Runway": 0.006516820288625662
      },
      "individual_log_probs": {
        "Project Runway": -5.03336870779458
      },
      "max_prob": 0.006516820288625662,
      "max_log_prob": -5.03336870779458,
      "best_answer": "Project Runway",
      "total_prob": 0.006516820288625662,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.0
    }
  ]
}