{
  "metrics": {
    "weight_w": 0.8,
    "input_file": "/home/jiangjp/trace-idea/data/hotpotqa/hotpotqa_dev100ideal_with_reasoning_chains_modify_w08.json",
    "total_samples": 100,
    "exact_match": 0.71,
    "f1": 0.7573399933399934,
    "precision": 0.7581111111111111,
    "recall": 0.7741666666666667,
    "avg_max_log_prob": -3.2102486070661724,
    "avg_max_prob": 0.5479184433983094,
    "avg_total_prob": 0.5479184433983094,
    "valid_max_prob_samples": 100,
    "invalid_max_prob_samples": 0,
    "avg_answer_count": 1.0,
    "avg_valid_answer_count": 1.0
  },
  "sample_info": [
    {
      "sample_idx": 0,
      "generated_answer": "No.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "no": 0.0033053874029831407
      },
      "individual_log_probs": {
        "no": -5.7122015953063965
      },
      "max_prob": 0.0033053874029831407,
      "max_log_prob": -5.7122015953063965,
      "best_answer": "no",
      "total_prob": 0.0033053874029831407,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 1,
      "generated_answer": "The University of Delaware has more undergraduate students, with approximately 18,500, compared to Boston College's 9,100",
      "gold_answers": [
        "University of Delaware"
      ],
      "reference_answer": "University of Delaware",
      "em_score": 0.0,
      "f1_score": 0.33333333333333337,
      "precision": 0.2,
      "recall": 1.0,
      "individual_probs": {
        "University of Delaware": 0.009819687320023444
      },
      "individual_log_probs": {
        "University of Delaware": -4.623365998260844
      },
      "max_prob": 0.009819687320023444,
      "max_log_prob": -4.623365998260844,
      "best_answer": "University of Delaware",
      "total_prob": 0.009819687320023444,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 2,
      "generated_answer": "None, the question is not answered in the provided context.",
      "gold_answers": [
        "NBC"
      ],
      "reference_answer": "NBC",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "NBC": 3.623879047862834e-06
      },
      "individual_log_probs": {
        "NBC": -12.527965545654297
      },
      "max_prob": 3.623879047862834e-06,
      "max_log_prob": -12.527965545654297,
      "best_answer": "NBC",
      "total_prob": 3.623879047862834e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 3,
      "generated_answer": "Flowering plants.",
      "gold_answers": [
        "genus"
      ],
      "reference_answer": "genus",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "genus": 0.002820820330307104
      },
      "individual_log_probs": {
        "genus": -5.8707275390625
      },
      "max_prob": 0.002820820330307104,
      "max_log_prob": -5.8707275390625,
      "best_answer": "genus",
      "total_prob": 0.002820820330307104,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 4,
      "generated_answer": "Kirk Fogg",
      "gold_answers": [
        "Jeff Sutphen"
      ],
      "reference_answer": "Jeff Sutphen",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Jeff Sutphen": 0.28821653295168936
      },
      "individual_log_probs": {
        "Jeff Sutphen": -1.2440432308175104
      },
      "max_prob": 0.28821653295168936,
      "max_log_prob": -1.2440432308175104,
      "best_answer": "Jeff Sutphen",
      "total_prob": 0.28821653295168936,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 5,
      "generated_answer": "The Forest of Bowland.",
      "gold_answers": [
        "the Bowland Fells"
      ],
      "reference_answer": "the Bowland Fells",
      "em_score": 0.0,
      "f1_score": 0.4,
      "precision": 0.3333333333333333,
      "recall": 0.5,
      "individual_probs": {
        "the Bowland Fells": 0.09278224087298031
      },
      "individual_log_probs": {
        "the Bowland Fells": -2.377500027406768
      },
      "max_prob": 0.09278224087298031,
      "max_log_prob": -2.377500027406768,
      "best_answer": "the Bowland Fells",
      "total_prob": 0.09278224087298031,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 6,
      "generated_answer": "Daft Punk",
      "gold_answers": [
        "Daft Punk"
      ],
      "reference_answer": "Daft Punk",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Daft Punk": 0.9997471004690458
      },
      "individual_log_probs": {
        "Daft Punk": -0.00025293151543337444
      },
      "max_prob": 0.9997471004690458,
      "max_log_prob": -0.00025293151543337444,
      "best_answer": "Daft Punk",
      "total_prob": 0.9997471004690458,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 7,
      "generated_answer": "John Lithgow in the NBC sitcom \"3rd Rock from the Sun\".",
      "gold_answers": [
        "3rd Rock from the Sun"
      ],
      "reference_answer": "3rd Rock from the Sun",
      "em_score": 0.0,
      "f1_score": 0.6153846153846153,
      "precision": 0.4444444444444444,
      "recall": 1.0,
      "individual_probs": {
        "3rd Rock from the Sun": 0.2735811488041396
      },
      "individual_log_probs": {
        "3rd Rock from the Sun": -1.2961569961771602
      },
      "max_prob": 0.2735811488041396,
      "max_log_prob": -1.2961569961771602,
      "best_answer": "3rd Rock from the Sun",
      "total_prob": 0.2735811488041396,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 8,
      "generated_answer": "Ewan McGregor",
      "gold_answers": [
        "The Island"
      ],
      "reference_answer": "The Island",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "The Island": 0.09146382060820576
      },
      "individual_log_probs": {
        "The Island": -2.391811788082123
      },
      "max_prob": 0.09146382060820576,
      "max_log_prob": -2.391811788082123,
      "best_answer": "The Island",
      "total_prob": 0.09146382060820576,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 9,
      "generated_answer": "The Great Mouse Detective",
      "gold_answers": [
        "The Great Mouse Detective"
      ],
      "reference_answer": "The Great Mouse Detective",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Great Mouse Detective": 0.9850015852282528
      },
      "individual_log_probs": {
        "The Great Mouse Detective": -0.015112028442558767
      },
      "max_prob": 0.9850015852282528,
      "max_log_prob": -0.015112028442558767,
      "best_answer": "The Great Mouse Detective",
      "total_prob": 0.9850015852282528,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 10,
      "generated_answer": "2003",
      "gold_answers": [
        "20 March to 1 May 2003"
      ],
      "reference_answer": "20 March to 1 May 2003",
      "em_score": 0.0,
      "f1_score": 0.2857142857142857,
      "precision": 1.0,
      "recall": 0.16666666666666666,
      "individual_probs": {
        "20 March to 1 May 2003": 0.004968969541273631
      },
      "individual_log_probs": {
        "20 March to 1 May 2003": -5.3045427961304625
      },
      "max_prob": 0.004968969541273631,
      "max_log_prob": -5.3045427961304625,
      "best_answer": "20 March to 1 May 2003",
      "total_prob": 0.004968969541273631,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 11,
      "generated_answer": "Both Borassodendron and Chelidonium are genera of flowering plants.",
      "gold_answers": [
        "plant"
      ],
      "reference_answer": "plant",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "plant": 0.0024784507948980863
      },
      "individual_log_probs": {
        "plant": -6.000121593475342
      },
      "max_prob": 0.0024784507948980863,
      "max_log_prob": -6.000121593475342,
      "best_answer": "plant",
      "total_prob": 0.0024784507948980863,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 12,
      "generated_answer": "Montclair State University.",
      "gold_answers": [
        "Montclair State University"
      ],
      "reference_answer": "Montclair State University",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Montclair State University": 0.9995539524427105
      },
      "individual_log_probs": {
        "Montclair State University": -0.00044614706609280574
      },
      "max_prob": 0.9995539524427105,
      "max_log_prob": -0.00044614706609280574,
      "best_answer": "Montclair State University",
      "total_prob": 0.9995539524427105,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 13,
      "generated_answer": "Soha Ali Khan",
      "gold_answers": [
        "Soha Ali Khan"
      ],
      "reference_answer": "Soha Ali Khan",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Soha Ali Khan": 0.999754363948421
      },
      "individual_log_probs": {
        "Soha Ali Khan": -0.00024566622505517444
      },
      "max_prob": 0.999754363948421,
      "max_log_prob": -0.00024566622505517444,
      "best_answer": "Soha Ali Khan",
      "total_prob": 0.999754363948421,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 14,
      "generated_answer": "Yes.",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "yes": 0.0010211493062343498
      },
      "individual_log_probs": {
        "yes": -6.886826515197754
      },
      "max_prob": 0.0010211493062343498,
      "max_log_prob": -6.886826515197754,
      "best_answer": "yes",
      "total_prob": 0.0010211493062343498,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 15,
      "generated_answer": "Bodo League massacre",
      "gold_answers": [
        "Bodo League massacre"
      ],
      "reference_answer": "Bodo League massacre",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Bodo League massacre": 0.6196757570861184
      },
      "individual_log_probs": {
        "Bodo League massacre": -0.47855891018252805
      },
      "max_prob": 0.6196757570861184,
      "max_log_prob": -0.47855891018252805,
      "best_answer": "Bodo League massacre",
      "total_prob": 0.6196757570861184,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 16,
      "generated_answer": "Ariocarpus.",
      "gold_answers": [
        "Ariocarpus"
      ],
      "reference_answer": "Ariocarpus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ariocarpus": 0.9950741316861447
      },
      "individual_log_probs": {
        "Ariocarpus": -0.0049380403916643445
      },
      "max_prob": 0.9950741316861447,
      "max_log_prob": -0.0049380403916643445,
      "best_answer": "Ariocarpus",
      "total_prob": 0.9950741316861447,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 17,
      "generated_answer": "761",
      "gold_answers": [
        "761"
      ],
      "reference_answer": "761",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "761": 0.9975860121862685
      },
      "individual_log_probs": {
        "761": -0.0024169061798602343
      },
      "max_prob": 0.9975860121862685,
      "max_log_prob": -0.0024169061798602343,
      "best_answer": "761",
      "total_prob": 0.9975860121862685,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 18,
      "generated_answer": "Cnut the Great",
      "gold_answers": [
        "Cnut the Great"
      ],
      "reference_answer": "Cnut the Great",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Cnut the Great": 0.9998297940625362
      },
      "individual_log_probs": {
        "Cnut the Great": -0.00017022042413827876
      },
      "max_prob": 0.9998297940625362,
      "max_log_prob": -0.00017022042413827876,
      "best_answer": "Cnut the Great",
      "total_prob": 0.9998297940625362,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 19,
      "generated_answer": "Channel V.",
      "gold_answers": [
        "Channel V"
      ],
      "reference_answer": "Channel V",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Channel V": 0.9960983006710674
      },
      "individual_log_probs": {
        "Channel V": -0.0039093308147357675
      },
      "max_prob": 0.9960983006710674,
      "max_log_prob": -0.0039093308147357675,
      "best_answer": "Channel V",
      "total_prob": 0.9960983006710674,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 20,
      "generated_answer": "Albert Camus",
      "gold_answers": [
        "Albert Camus"
      ],
      "reference_answer": "Albert Camus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Albert Camus": 0.9620168291369338
      },
      "individual_log_probs": {
        "Albert Camus": -0.0387233345641107
      },
      "max_prob": 0.9620168291369338,
      "max_log_prob": -0.0387233345641107,
      "best_answer": "Albert Camus",
      "total_prob": 0.9620168291369338,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 21,
      "generated_answer": "Paul Marcinkus",
      "gold_answers": [
        "Paul Marcinkus"
      ],
      "reference_answer": "Paul Marcinkus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Paul Marcinkus": 0.9916822331447417
      },
      "individual_log_probs": {
        "Paul Marcinkus": -0.008352552504902633
      },
      "max_prob": 0.9916822331447417,
      "max_log_prob": -0.008352552504902633,
      "best_answer": "Paul Marcinkus",
      "total_prob": 0.9916822331447417,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 22,
      "generated_answer": "1852",
      "gold_answers": [
        "1904"
      ],
      "reference_answer": "1904",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1904": 1.0929096055669674e-10
      },
      "individual_log_probs": {
        "1904": -22.937007427215576
      },
      "max_prob": 1.0929096055669674e-10,
      "max_log_prob": -22.937007427215576,
      "best_answer": "1904",
      "total_prob": 1.0929096055669674e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 23,
      "generated_answer": "Ralph Fiennes",
      "gold_answers": [
        "Ralph Fiennes"
      ],
      "reference_answer": "Ralph Fiennes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ralph Fiennes": 0.9996982280235365
      },
      "individual_log_probs": {
        "Ralph Fiennes": -0.00030181751878899377
      },
      "max_prob": 0.9996982280235365,
      "max_log_prob": -0.00030181751878899377,
      "best_answer": "Ralph Fiennes",
      "total_prob": 0.9996982280235365,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 24,
      "generated_answer": "runner-up",
      "gold_answers": [
        "runner-up"
      ],
      "reference_answer": "runner-up",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "runner-up": 0.7378673608906452
      },
      "individual_log_probs": {
        "runner-up": -0.30399119832509314
      },
      "max_prob": 0.7378673608906452,
      "max_log_prob": -0.30399119832509314,
      "best_answer": "runner-up",
      "total_prob": 0.7378673608906452,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 25,
      "generated_answer": "Swedish",
      "gold_answers": [
        "Swedish"
      ],
      "reference_answer": "Swedish",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Swedish": 0.9997811794237288
      },
      "individual_log_probs": {
        "Swedish": -0.00021884452098674956
      },
      "max_prob": 0.9997811794237288,
      "max_log_prob": -0.00021884452098674956,
      "best_answer": "Swedish",
      "total_prob": 0.9997811794237288,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 26,
      "generated_answer": "Vice",
      "gold_answers": [
        "Vice"
      ],
      "reference_answer": "Vice",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Vice": 0.9982749036189045
      },
      "individual_log_probs": {
        "Vice": -0.0017265860733459704
      },
      "max_prob": 0.9982749036189045,
      "max_log_prob": -0.0017265860733459704,
      "best_answer": "Vice",
      "total_prob": 0.9982749036189045,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 27,
      "generated_answer": "The Preppie Murder.",
      "gold_answers": [
        "The Preppie Murder"
      ],
      "reference_answer": "The Preppie Murder",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Preppie Murder": 0.983020480247809
      },
      "individual_log_probs": {
        "The Preppie Murder": -0.017125324618831428
      },
      "max_prob": 0.983020480247809,
      "max_log_prob": -0.017125324618831428,
      "best_answer": "The Preppie Murder",
      "total_prob": 0.983020480247809,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 28,
      "generated_answer": "Isolde Kostner",
      "gold_answers": [
        "Isolde Kostner"
      ],
      "reference_answer": "Isolde Kostner",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Isolde Kostner": 0.996780817594993
      },
      "individual_log_probs": {
        "Isolde Kostner": -0.0032243751198777204
      },
      "max_prob": 0.996780817594993,
      "max_log_prob": -0.0032243751198777204,
      "best_answer": "Isolde Kostner",
      "total_prob": 0.996780817594993,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 29,
      "generated_answer": "Beverly Hills High School.",
      "gold_answers": [
        "Beverly Hills High School"
      ],
      "reference_answer": "Beverly Hills High School",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Beverly Hills High School": 0.9918150864278441
      },
      "individual_log_probs": {
        "Beverly Hills High School": -0.008218593883441372
      },
      "max_prob": 0.9918150864278441,
      "max_log_prob": -0.008218593883441372,
      "best_answer": "Beverly Hills High School",
      "total_prob": 0.9918150864278441,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 30,
      "generated_answer": "1989",
      "gold_answers": [
        "fifth album"
      ],
      "reference_answer": "fifth album",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "fifth album": 3.279005486375955e-08
      },
      "individual_log_probs": {
        "fifth album": -17.23314057290554
      },
      "max_prob": 3.279005486375955e-08,
      "max_log_prob": -17.23314057290554,
      "best_answer": "fifth album",
      "total_prob": 3.279005486375955e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 31,
      "generated_answer": "Welterweight",
      "gold_answers": [
        "Welterweight"
      ],
      "reference_answer": "Welterweight",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Welterweight": 0.9954617362477883
      },
      "individual_log_probs": {
        "Welterweight": -0.004548592934035867
      },
      "max_prob": 0.9954617362477883,
      "max_log_prob": -0.004548592934035867,
      "best_answer": "Welterweight",
      "total_prob": 0.9954617362477883,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 32,
      "generated_answer": "Times Beach.",
      "gold_answers": [
        "Times Beach"
      ],
      "reference_answer": "Times Beach",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Times Beach": 0.9995098110179164
      },
      "individual_log_probs": {
        "Times Beach": -0.0004903091639789636
      },
      "max_prob": 0.9995098110179164,
      "max_log_prob": -0.0004903091639789636,
      "best_answer": "Times Beach",
      "total_prob": 0.9995098110179164,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 33,
      "generated_answer": "The Beverly Hills Supper Club fire.",
      "gold_answers": [
        "Beverly Hills Supper Club fire"
      ],
      "reference_answer": "Beverly Hills Supper Club fire",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Beverly Hills Supper Club fire": 0.3896895611680074
      },
      "individual_log_probs": {
        "Beverly Hills Supper Club fire": -0.9424048538373881
      },
      "max_prob": 0.3896895611680074,
      "max_log_prob": -0.9424048538373881,
      "best_answer": "Beverly Hills Supper Club fire",
      "total_prob": 0.3896895611680074,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 34,
      "generated_answer": "Quarterback",
      "gold_answers": [
        "quarterback"
      ],
      "reference_answer": "quarterback",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "quarterback": 0.3191892057640661
      },
      "individual_log_probs": {
        "quarterback": -1.1419712305068899
      },
      "max_prob": 0.3191892057640661,
      "max_log_prob": -1.1419712305068899,
      "best_answer": "quarterback",
      "total_prob": 0.3191892057640661,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 35,
      "generated_answer": "Dusty Dvoracek",
      "gold_answers": [
        "Dusty Dvoracek"
      ],
      "reference_answer": "Dusty Dvoracek",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Dusty Dvoracek": 0.9994754433077593
      },
      "individual_log_probs": {
        "Dusty Dvoracek": -0.0005246943202337206
      },
      "max_prob": 0.9994754433077593,
      "max_log_prob": -0.0005246943202337206,
      "best_answer": "Dusty Dvoracek",
      "total_prob": 0.9994754433077593,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 36,
      "generated_answer": "No.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "no": 0.0054632962171958805
      },
      "individual_log_probs": {
        "no": -5.209702968597412
      },
      "max_prob": 0.0054632962171958805,
      "max_log_prob": -5.209702968597412,
      "best_answer": "no",
      "total_prob": 0.0054632962171958805,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 37,
      "generated_answer": "The Texas Rangers.",
      "gold_answers": [
        "Texas Rangers"
      ],
      "reference_answer": "Texas Rangers",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Texas Rangers": 0.24729365323114375
      },
      "individual_log_probs": {
        "Texas Rangers": -1.3971787690334168
      },
      "max_prob": 0.24729365323114375,
      "max_log_prob": -1.3971787690334168,
      "best_answer": "Texas Rangers",
      "total_prob": 0.24729365323114375,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 38,
      "generated_answer": "Germanic, Baltic, and Thracian tribes.",
      "gold_answers": [
        "Germanic, Baltic and Thracian tribes"
      ],
      "reference_answer": "Germanic, Baltic and Thracian tribes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Germanic, Baltic and Thracian tribes": 0.18008556414660748
      },
      "individual_log_probs": {
        "Germanic, Baltic and Thracian tribes": -1.7143231846679328
      },
      "max_prob": 0.18008556414660748,
      "max_log_prob": -1.7143231846679328,
      "best_answer": "Germanic, Baltic and Thracian tribes",
      "total_prob": 0.18008556414660748,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 39,
      "generated_answer": "American",
      "gold_answers": [
        "American"
      ],
      "reference_answer": "American",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "American": 0.9990486620926138
      },
      "individual_log_probs": {
        "American": -0.0009517907164990902
      },
      "max_prob": 0.9990486620926138,
      "max_log_prob": -0.0009517907164990902,
      "best_answer": "American",
      "total_prob": 0.9990486620926138,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 40,
      "generated_answer": "Mario Lemieux",
      "gold_answers": [
        "Mario Lemieux"
      ],
      "reference_answer": "Mario Lemieux",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mario Lemieux": 0.9573232167966599
      },
      "individual_log_probs": {
        "Mario Lemieux": -0.04361420495092716
      },
      "max_prob": 0.9573232167966599,
      "max_log_prob": -0.04361420495092716,
      "best_answer": "Mario Lemieux",
      "total_prob": 0.9573232167966599,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 41,
      "generated_answer": "Mumbai",
      "gold_answers": [
        "Mumbai"
      ],
      "reference_answer": "Mumbai",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mumbai": 0.9989023323530555
      },
      "individual_log_probs": {
        "Mumbai": -0.0010982705252899905
      },
      "max_prob": 0.9989023323530555,
      "max_log_prob": -0.0010982705252899905,
      "best_answer": "Mumbai",
      "total_prob": 0.9989023323530555,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 42,
      "generated_answer": "Alexander Scriabin",
      "gold_answers": [
        "Alexander Scriabin"
      ],
      "reference_answer": "Alexander Scriabin",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Alexander Scriabin": 0.996443289113047
      },
      "individual_log_probs": {
        "Alexander Scriabin": -0.003563051020933017
      },
      "max_prob": 0.996443289113047,
      "max_log_prob": -0.003563051020933017,
      "best_answer": "Alexander Scriabin",
      "total_prob": 0.996443289113047,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 43,
      "generated_answer": "4 years",
      "gold_answers": [
        "Their biggest hit was \"Tall Cool One\", first released in 1959"
      ],
      "reference_answer": "Their biggest hit was \"Tall Cool One\", first released in 1959",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Their biggest hit was \"Tall Cool One\", first released in 1959": 2.5833083673308137e-07
      },
      "individual_log_probs": {
        "Their biggest hit was \"Tall Cool One\", first released in 1959": -15.169024760566913
      },
      "max_prob": 2.5833083673308137e-07,
      "max_log_prob": -15.169024760566913,
      "best_answer": "Their biggest hit was \"Tall Cool One\", first released in 1959",
      "total_prob": 2.5833083673308137e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 44,
      "generated_answer": "Jawaharlal Nehru",
      "gold_answers": [
        "first Prime Minister of India"
      ],
      "reference_answer": "first Prime Minister of India",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "first Prime Minister of India": 1.1628059469808367e-06
      },
      "individual_log_probs": {
        "first Prime Minister of India": -13.664674553912164
      },
      "max_prob": 1.1628059469808367e-06,
      "max_log_prob": -13.664674553912164,
      "best_answer": "first Prime Minister of India",
      "total_prob": 1.1628059469808367e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 45,
      "generated_answer": "Ireland",
      "gold_answers": [
        "Ireland"
      ],
      "reference_answer": "Ireland",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ireland": 0.5021357114600854
      },
      "individual_log_probs": {
        "Ireland": -0.6888848542721462
      },
      "max_prob": 0.5021357114600854,
      "max_log_prob": -0.6888848542721462,
      "best_answer": "Ireland",
      "total_prob": 0.5021357114600854,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 46,
      "generated_answer": "72-78",
      "gold_answers": [
        "72-78"
      ],
      "reference_answer": "72-78",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "72-78": 0.9904911546548864
      },
      "individual_log_probs": {
        "72-78": -0.00955434306524694
      },
      "max_prob": 0.9904911546548864,
      "max_log_prob": -0.00955434306524694,
      "best_answer": "72-78",
      "total_prob": 0.9904911546548864,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 47,
      "generated_answer": "Heroica Matamoros",
      "gold_answers": [
        "Heroica Matamoros"
      ],
      "reference_answer": "Heroica Matamoros",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Heroica Matamoros": 0.9983988898976576
      },
      "individual_log_probs": {
        "Heroica Matamoros": -0.0016023932489446224
      },
      "max_prob": 0.9983988898976576,
      "max_log_prob": -0.0016023932489446224,
      "best_answer": "Heroica Matamoros",
      "total_prob": 0.9983988898976576,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 48,
      "generated_answer": "Benedict Arnold",
      "gold_answers": [
        "Benedict Arnold"
      ],
      "reference_answer": "Benedict Arnold",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Benedict Arnold": 0.9484461920022211
      },
      "individual_log_probs": {
        "Benedict Arnold": -0.052930220786919335
      },
      "max_prob": 0.9484461920022211,
      "max_log_prob": -0.052930220786919335,
      "best_answer": "Benedict Arnold",
      "total_prob": 0.9484461920022211,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 49,
      "generated_answer": "South Kesteven Sports Stadium",
      "gold_answers": [
        "South Kesteven Sports Stadium"
      ],
      "reference_answer": "South Kesteven Sports Stadium",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "South Kesteven Sports Stadium": 0.637728579205575
      },
      "individual_log_probs": {
        "South Kesteven Sports Stadium": -0.4498425105986712
      },
      "max_prob": 0.637728579205575,
      "max_log_prob": -0.4498425105986712,
      "best_answer": "South Kesteven Sports Stadium",
      "total_prob": 0.637728579205575,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 50,
      "generated_answer": "Masako",
      "gold_answers": [
        "Aiko, Princess Toshi"
      ],
      "reference_answer": "Aiko, Princess Toshi",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Aiko, Princess Toshi": 0.006006472590969788
      },
      "individual_log_probs": {
        "Aiko, Princess Toshi": -5.114917626041461
      },
      "max_prob": 0.006006472590969788,
      "max_log_prob": -5.114917626041461,
      "best_answer": "Aiko, Princess Toshi",
      "total_prob": 0.006006472590969788,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 51,
      "generated_answer": "Naseem Hamed",
      "gold_answers": [
        "Marco Antonio Barrera"
      ],
      "reference_answer": "Marco Antonio Barrera",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Marco Antonio Barrera": 3.49271078600597e-05
      },
      "individual_log_probs": {
        "Marco Antonio Barrera": -10.262247300739546
      },
      "max_prob": 3.49271078600597e-05,
      "max_log_prob": -10.262247300739546,
      "best_answer": "Marco Antonio Barrera",
      "total_prob": 3.49271078600597e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 52,
      "generated_answer": "the Iberian Peninsula",
      "gold_answers": [
        "Iberian Peninsula"
      ],
      "reference_answer": "Iberian Peninsula",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Iberian Peninsula": 0.3145281267025969
      },
      "individual_log_probs": {
        "Iberian Peninsula": -1.1566817737639212
      },
      "max_prob": 0.3145281267025969,
      "max_log_prob": -1.1566817737639212,
      "best_answer": "Iberian Peninsula",
      "total_prob": 0.3145281267025969,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 53,
      "generated_answer": "The Hume Highway.",
      "gold_answers": [
        "Hume Highway"
      ],
      "reference_answer": "Hume Highway",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Hume Highway": 0.41575095974273035
      },
      "individual_log_probs": {
        "Hume Highway": -0.8776688524505971
      },
      "max_prob": 0.41575095974273035,
      "max_log_prob": -0.8776688524505971,
      "best_answer": "Hume Highway",
      "total_prob": 0.41575095974273035,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 54,
      "generated_answer": "not provided in the given contexts.",
      "gold_answers": [
        "University of North Texas"
      ],
      "reference_answer": "University of North Texas",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "University of North Texas": 0.00267078201326352
      },
      "individual_log_probs": {
        "University of North Texas": -5.925383960590807
      },
      "max_prob": 0.00267078201326352,
      "max_log_prob": -5.925383960590807,
      "best_answer": "University of North Texas",
      "total_prob": 0.00267078201326352,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 55,
      "generated_answer": "A160",
      "gold_answers": [
        "the A160"
      ],
      "reference_answer": "the A160",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "the A160": 0.18782817243912597
      },
      "individual_log_probs": {
        "the A160": -1.6722277104709065
      },
      "max_prob": 0.18782817243912597,
      "max_log_prob": -1.6722277104709065,
      "best_answer": "the A160",
      "total_prob": 0.18782817243912597,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 56,
      "generated_answer": "American",
      "gold_answers": [
        "American"
      ],
      "reference_answer": "American",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "American": 0.9909952142440218
      },
      "individual_log_probs": {
        "American": -0.009045573882758617
      },
      "max_prob": 0.9909952142440218,
      "max_log_prob": -0.009045573882758617,
      "best_answer": "American",
      "total_prob": 0.9909952142440218,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 57,
      "generated_answer": "Infinity chili",
      "gold_answers": [
        "\"Capsicum chinense\""
      ],
      "reference_answer": "\"Capsicum chinense\"",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "\"Capsicum chinense\"": 8.722731297159038e-06
      },
      "individual_log_probs": {
        "\"Capsicum chinense\"": -11.649578146935937
      },
      "max_prob": 8.722731297159038e-06,
      "max_log_prob": -11.649578146935937,
      "best_answer": "\"Capsicum chinense\"",
      "total_prob": 8.722731297159038e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 58,
      "generated_answer": "Hillary Clinton",
      "gold_answers": [
        "Hillary Clinton"
      ],
      "reference_answer": "Hillary Clinton",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Hillary Clinton": 0.7252370175596147
      },
      "individual_log_probs": {
        "Hillary Clinton": -0.32125675678253174
      },
      "max_prob": 0.7252370175596147,
      "max_log_prob": -0.32125675678253174,
      "best_answer": "Hillary Clinton",
      "total_prob": 0.7252370175596147,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 59,
      "generated_answer": "Lynyrd Skynyrd.",
      "gold_answers": [
        "Lynyrd Skynyrd"
      ],
      "reference_answer": "Lynyrd Skynyrd",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Lynyrd Skynyrd": 0.9798085610268731
      },
      "individual_log_probs": {
        "Lynyrd Skynyrd": -0.020398072290959135
      },
      "max_prob": 0.9798085610268731,
      "max_log_prob": -0.020398072290959135,
      "best_answer": "Lynyrd Skynyrd",
      "total_prob": 0.9798085610268731,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 60,
      "generated_answer": "Sehnsucht",
      "gold_answers": [
        "Herzeleid"
      ],
      "reference_answer": "Herzeleid",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Herzeleid": 0.0011277790412257255
      },
      "individual_log_probs": {
        "Herzeleid": -6.787505030532515
      },
      "max_prob": 0.0011277790412257255,
      "max_log_prob": -6.787505030532515,
      "best_answer": "Herzeleid",
      "total_prob": 0.0011277790412257255,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 61,
      "generated_answer": "1912",
      "gold_answers": [
        "1897"
      ],
      "reference_answer": "1897",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1897": 3.9436659449899506e-05
      },
      "individual_log_probs": {
        "1897": -10.140814731363207
      },
      "max_prob": 3.9436659449899506e-05,
      "max_log_prob": -10.140814731363207,
      "best_answer": "1897",
      "total_prob": 3.9436659449899506e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 62,
      "generated_answer": "Ahmed Elmohamady.",
      "gold_answers": [
        "Ahmed Elmohamady"
      ],
      "reference_answer": "Ahmed Elmohamady",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ahmed Elmohamady": 0.9988622083447856
      },
      "individual_log_probs": {
        "Ahmed Elmohamady": -0.0011384394315427926
      },
      "max_prob": 0.9988622083447856,
      "max_log_prob": -0.0011384394315427926,
      "best_answer": "Ahmed Elmohamady",
      "total_prob": 0.9988622083447856,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 63,
      "generated_answer": "3D animation.",
      "gold_answers": [
        "animation"
      ],
      "reference_answer": "animation",
      "em_score": 0.0,
      "f1_score": 0.6666666666666666,
      "precision": 0.5,
      "recall": 1.0,
      "individual_probs": {
        "animation": 0.0011452925145279925
      },
      "individual_log_probs": {
        "animation": -6.772095203399658
      },
      "max_prob": 0.0011452925145279925,
      "max_log_prob": -6.772095203399658,
      "best_answer": "animation",
      "total_prob": 0.0011452925145279925,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 64,
      "generated_answer": "Jeff Healey",
      "gold_answers": [
        "Jeff Healey"
      ],
      "reference_answer": "Jeff Healey",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Jeff Healey": 0.9998228792824994
      },
      "individual_log_probs": {
        "Jeff Healey": -0.00017713640522742935
      },
      "max_prob": 0.9998228792824994,
      "max_log_prob": -0.00017713640522742935,
      "best_answer": "Jeff Healey",
      "total_prob": 0.9998228792824994,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 65,
      "generated_answer": "British Armed Forces",
      "gold_answers": [
        "British Armed Forces"
      ],
      "reference_answer": "British Armed Forces",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "British Armed Forces": 0.39308085162627576
      },
      "individual_log_probs": {
        "British Armed Forces": -0.9337399589433062
      },
      "max_prob": 0.39308085162627576,
      "max_log_prob": -0.9337399589433062,
      "best_answer": "British Armed Forces",
      "total_prob": 0.39308085162627576,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 66,
      "generated_answer": "Kadhalan",
      "gold_answers": [
        "Kadhalan"
      ],
      "reference_answer": "Kadhalan",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Kadhalan": 0.9712612183932556
      },
      "individual_log_probs": {
        "Kadhalan": -0.02915982689773955
      },
      "max_prob": 0.9712612183932556,
      "max_log_prob": -0.02915982689773955,
      "best_answer": "Kadhalan",
      "total_prob": 0.9712612183932556,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 67,
      "generated_answer": "Cherepanov Hill",
      "gold_answers": [
        "Cherepanov Hill"
      ],
      "reference_answer": "Cherepanov Hill",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Cherepanov Hill": 0.9837745416908806
      },
      "individual_log_probs": {
        "Cherepanov Hill": -0.016358532480751364
      },
      "max_prob": 0.9837745416908806,
      "max_log_prob": -0.016358532480751364,
      "best_answer": "Cherepanov Hill",
      "total_prob": 0.9837745416908806,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 68,
      "generated_answer": "Arantxa Sánchez Vicario (World No. 1)",
      "gold_answers": [
        "\"Arantxa\" Sánchez Vicario"
      ],
      "reference_answer": "\"Arantxa\" Sánchez Vicario",
      "em_score": 0.0,
      "f1_score": 0.6666666666666666,
      "precision": 0.5,
      "recall": 1.0,
      "individual_probs": {
        "\"Arantxa\" Sánchez Vicario": 9.214268968027998e-10
      },
      "individual_log_probs": {
        "\"Arantxa\" Sánchez Vicario": -20.805097672619013
      },
      "max_prob": 9.214268968027998e-10,
      "max_log_prob": -20.805097672619013,
      "best_answer": "\"Arantxa\" Sánchez Vicario",
      "total_prob": 9.214268968027998e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 69,
      "generated_answer": "University of Alabama",
      "gold_answers": [
        "University of Alabama"
      ],
      "reference_answer": "University of Alabama",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "University of Alabama": 0.3513919475912581
      },
      "individual_log_probs": {
        "University of Alabama": -1.0458530187129327
      },
      "max_prob": 0.3513919475912581,
      "max_log_prob": -1.0458530187129327,
      "best_answer": "University of Alabama",
      "total_prob": 0.3513919475912581,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 70,
      "generated_answer": "Pizza Hut",
      "gold_answers": [
        "Pizza Hut"
      ],
      "reference_answer": "Pizza Hut",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Pizza Hut": 0.999107082258033
      },
      "individual_log_probs": {
        "Pizza Hut": -0.0008933166304814222
      },
      "max_prob": 0.999107082258033,
      "max_log_prob": -0.0008933166304814222,
      "best_answer": "Pizza Hut",
      "total_prob": 0.999107082258033,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 71,
      "generated_answer": "1968",
      "gold_answers": [
        "1939"
      ],
      "reference_answer": "1939",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1939": 1.7933890883113925e-07
      },
      "individual_log_probs": {
        "1939": -15.53398847579956
      },
      "max_prob": 1.7933890883113925e-07,
      "max_log_prob": -15.53398847579956,
      "best_answer": "1939",
      "total_prob": 1.7933890883113925e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 72,
      "generated_answer": "1962",
      "gold_answers": [
        "1962"
      ],
      "reference_answer": "1962",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "1962": 0.9990591326991425
      },
      "individual_log_probs": {
        "1962": -0.0009413101943209767
      },
      "max_prob": 0.9990591326991425,
      "max_log_prob": -0.0009413101943209767,
      "best_answer": "1962",
      "total_prob": 0.9990591326991425,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 73,
      "generated_answer": "Metreon",
      "gold_answers": [
        "Metreon"
      ],
      "reference_answer": "Metreon",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Metreon": 0.9990970795368836
      },
      "individual_log_probs": {
        "Metreon": -0.0009033283413373283
      },
      "max_prob": 0.9990970795368836,
      "max_log_prob": -0.0009033283413373283,
      "best_answer": "Metreon",
      "total_prob": 0.9990970795368836,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 74,
      "generated_answer": "Lithuanian Labour party",
      "gold_answers": [
        "Viktor Uspaskich"
      ],
      "reference_answer": "Viktor Uspaskich",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Viktor Uspaskich": 1.2627526006352186e-07
      },
      "individual_log_probs": {
        "Viktor Uspaskich": -15.884801709093153
      },
      "max_prob": 1.2627526006352186e-07,
      "max_log_prob": -15.884801709093153,
      "best_answer": "Viktor Uspaskich",
      "total_prob": 1.2627526006352186e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 75,
      "generated_answer": "Spacemen 3.",
      "gold_answers": [
        "Spacemen 3"
      ],
      "reference_answer": "Spacemen 3",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Spacemen 3": 0.9983258561662208
      },
      "individual_log_probs": {
        "Spacemen 3": -0.001675546778606929
      },
      "max_prob": 0.9983258561662208,
      "max_log_prob": -0.001675546778606929,
      "best_answer": "Spacemen 3",
      "total_prob": 0.9983258561662208,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 76,
      "generated_answer": "Stephen King.",
      "gold_answers": [
        "Stephen King"
      ],
      "reference_answer": "Stephen King",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Stephen King": 0.9983413241151501
      },
      "individual_log_probs": {
        "Stephen King": -0.0016600530107098166
      },
      "max_prob": 0.9983413241151501,
      "max_log_prob": -0.0016600530107098166,
      "best_answer": "Stephen King",
      "total_prob": 0.9983413241151501,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 77,
      "generated_answer": "formulate the classical theory of electromagnetic radiation",
      "gold_answers": [
        "classical theory of electromagnetic radiation"
      ],
      "reference_answer": "classical theory of electromagnetic radiation",
      "em_score": 0.0,
      "f1_score": 0.9090909090909091,
      "precision": 0.8333333333333334,
      "recall": 1.0,
      "individual_probs": {
        "classical theory of electromagnetic radiation": 0.0037959602645043196
      },
      "individual_log_probs": {
        "classical theory of electromagnetic radiation": -5.5738178660172935
      },
      "max_prob": 0.0037959602645043196,
      "max_log_prob": -5.5738178660172935,
      "best_answer": "classical theory of electromagnetic radiation",
      "total_prob": 0.0037959602645043196,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 78,
      "generated_answer": "\"The Country Bears\"",
      "gold_answers": [
        "The London Connection"
      ],
      "reference_answer": "The London Connection",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "The London Connection": 0.004484977676124668
      },
      "individual_log_probs": {
        "The London Connection": -5.407021760936459
      },
      "max_prob": 0.004484977676124668,
      "max_log_prob": -5.407021760936459,
      "best_answer": "The London Connection",
      "total_prob": 0.004484977676124668,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 79,
      "generated_answer": "Mexican",
      "gold_answers": [
        "Mexican"
      ],
      "reference_answer": "Mexican",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mexican": 0.9293655338655837
      },
      "individual_log_probs": {
        "Mexican": -0.0732531473038307
      },
      "max_prob": 0.9293655338655837,
      "max_log_prob": -0.0732531473038307,
      "best_answer": "Mexican",
      "total_prob": 0.9293655338655837,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 80,
      "generated_answer": "December 7, 2008",
      "gold_answers": [
        "December 7, 2008"
      ],
      "reference_answer": "December 7, 2008",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "December 7, 2008": 0.9999487424554563
      },
      "individual_log_probs": {
        "December 7, 2008": -5.12588582566309e-05
      },
      "max_prob": 0.9999487424554563,
      "max_log_prob": -5.12588582566309e-05,
      "best_answer": "December 7, 2008",
      "total_prob": 0.9999487424554563,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 81,
      "generated_answer": "Yes.",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "yes": 0.0014229301868244968
      },
      "individual_log_probs": {
        "yes": -6.555037021636963
      },
      "max_prob": 0.0014229301868244968,
      "max_log_prob": -6.555037021636963,
      "best_answer": "yes",
      "total_prob": 0.0014229301868244968,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 82,
      "generated_answer": "Football.",
      "gold_answers": [
        "Football"
      ],
      "reference_answer": "Football",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Football": 0.9479242219636533
      },
      "individual_log_probs": {
        "Football": -0.053480714559555054
      },
      "max_prob": 0.9479242219636533,
      "max_log_prob": -0.053480714559555054,
      "best_answer": "Football",
      "total_prob": 0.9479242219636533,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 83,
      "generated_answer": "Betty X",
      "gold_answers": [
        "Betty X"
      ],
      "reference_answer": "Betty X",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Betty X": 0.6145909924599927
      },
      "individual_log_probs": {
        "Betty X": -0.4867982853319859
      },
      "max_prob": 0.6145909924599927,
      "max_log_prob": -0.4867982853319859,
      "best_answer": "Betty X",
      "total_prob": 0.6145909924599927,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 84,
      "generated_answer": "Extreme Pizza",
      "gold_answers": [
        "Extreme Pizza"
      ],
      "reference_answer": "Extreme Pizza",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Extreme Pizza": 0.9992443970236373
      },
      "individual_log_probs": {
        "Extreme Pizza": -0.0007558885881735478
      },
      "max_prob": 0.9992443970236373,
      "max_log_prob": -0.0007558885881735478,
      "best_answer": "Extreme Pizza",
      "total_prob": 0.9992443970236373,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 85,
      "generated_answer": "Anna Mae Bullock",
      "gold_answers": [
        "Anna Mae Bullock"
      ],
      "reference_answer": "Anna Mae Bullock",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Anna Mae Bullock": 0.6884108077627585
      },
      "individual_log_probs": {
        "Anna Mae Bullock": -0.37336951493830384
      },
      "max_prob": 0.6884108077627585,
      "max_log_prob": -0.37336951493830384,
      "best_answer": "Anna Mae Bullock",
      "total_prob": 0.6884108077627585,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 86,
      "generated_answer": "Michie Stadium",
      "gold_answers": [
        "Michie Stadium"
      ],
      "reference_answer": "Michie Stadium",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Michie Stadium": 0.9995828218468951
      },
      "individual_log_probs": {
        "Michie Stadium": -0.0004172651961198426
      },
      "max_prob": 0.9995828218468951,
      "max_log_prob": -0.0004172651961198426,
      "best_answer": "Michie Stadium",
      "total_prob": 0.9995828218468951,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 87,
      "generated_answer": "Pacific Ocean",
      "gold_answers": [
        "the Pacific Ocean"
      ],
      "reference_answer": "the Pacific Ocean",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "the Pacific Ocean": 0.009049867106078378
      },
      "individual_log_probs": {
        "the Pacific Ocean": -4.705005205789348
      },
      "max_prob": 0.009049867106078378,
      "max_log_prob": -4.705005205789348,
      "best_answer": "the Pacific Ocean",
      "total_prob": 0.009049867106078378,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 88,
      "generated_answer": "Angela R. Nissel",
      "gold_answers": [
        "Angela R. Nissel"
      ],
      "reference_answer": "Angela R. Nissel",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Angela R. Nissel": 0.5109147017011176
      },
      "individual_log_probs": {
        "Angela R. Nissel": -0.6715526269757959
      },
      "max_prob": 0.5109147017011176,
      "max_log_prob": -0.6715526269757959,
      "best_answer": "Angela R. Nissel",
      "total_prob": 0.5109147017011176,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 89,
      "generated_answer": "the Netherlands",
      "gold_answers": [
        "Netherlands"
      ],
      "reference_answer": "Netherlands",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Netherlands": 0.15863115593449104
      },
      "individual_log_probs": {
        "Netherlands": -1.8411735456029419
      },
      "max_prob": 0.15863115593449104,
      "max_log_prob": -1.8411735456029419,
      "best_answer": "Netherlands",
      "total_prob": 0.15863115593449104,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 90,
      "generated_answer": "Alessandro Nannini",
      "gold_answers": [
        "Alessandro Nannini"
      ],
      "reference_answer": "Alessandro Nannini",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Alessandro Nannini": 0.9995786509371077
      },
      "individual_log_probs": {
        "Alessandro Nannini": -0.0004214378553513143
      },
      "max_prob": 0.9995786509371077,
      "max_log_prob": -0.0004214378553513143,
      "best_answer": "Alessandro Nannini",
      "total_prob": 0.9995786509371077,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 91,
      "generated_answer": "Theo van Gogh",
      "gold_answers": [
        "Theodorus \"Theo\" van Gogh"
      ],
      "reference_answer": "Theodorus \"Theo\" van Gogh",
      "em_score": 0.0,
      "f1_score": 0.8571428571428571,
      "precision": 1.0,
      "recall": 0.75,
      "individual_probs": {
        "Theodorus \"Theo\" van Gogh": 3.676430035797029e-06
      },
      "individual_log_probs": {
        "Theodorus \"Theo\" van Gogh": -12.513568375602176
      },
      "max_prob": 3.676430035797029e-06,
      "max_log_prob": -12.513568375602176,
      "best_answer": "Theodorus \"Theo\" van Gogh",
      "total_prob": 3.676430035797029e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 92,
      "generated_answer": "Indian Rebellion of 1857",
      "gold_answers": [
        "Indian Rebellion of 1857"
      ],
      "reference_answer": "Indian Rebellion of 1857",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Indian Rebellion of 1857": 0.600708603037193
      },
      "individual_log_probs": {
        "Indian Rebellion of 1857": -0.5096453155418885
      },
      "max_prob": 0.600708603037193,
      "max_log_prob": -0.5096453155418885,
      "best_answer": "Indian Rebellion of 1857",
      "total_prob": 0.600708603037193,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 93,
      "generated_answer": "Yes.",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "yes": 0.0006372060336384419
      },
      "individual_log_probs": {
        "yes": -7.358417510986328
      },
      "max_prob": 0.0006372060336384419,
      "max_log_prob": -7.358417510986328,
      "best_answer": "yes",
      "total_prob": 0.0006372060336384419,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 94,
      "generated_answer": "Ludovico Einaudi has performed with Canzoniere Grecanico Salentino.",
      "gold_answers": [
        "Italian pianist and composer"
      ],
      "reference_answer": "Italian pianist and composer",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Italian pianist and composer": 3.4350297829723184e-06
      },
      "individual_log_probs": {
        "Italian pianist and composer": -12.581484961884371
      },
      "max_prob": 3.4350297829723184e-06,
      "max_log_prob": -12.581484961884371,
      "best_answer": "Italian pianist and composer",
      "total_prob": 3.4350297829723184e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 95,
      "generated_answer": "actress.",
      "gold_answers": [
        "actress"
      ],
      "reference_answer": "actress",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "actress": 0.7831382183901227
      },
      "individual_log_probs": {
        "actress": -0.24444607444456778
      },
      "max_prob": 0.7831382183901227,
      "max_log_prob": -0.24444607444456778,
      "best_answer": "actress",
      "total_prob": 0.7831382183901227,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 96,
      "generated_answer": "Amsterdam",
      "gold_answers": [
        "Amsterdam"
      ],
      "reference_answer": "Amsterdam",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Amsterdam": 0.9995305352963684
      },
      "individual_log_probs": {
        "Amsterdam": -0.00046957493668742245
      },
      "max_prob": 0.9995305352963684,
      "max_log_prob": -0.00046957493668742245,
      "best_answer": "Amsterdam",
      "total_prob": 0.9995305352963684,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 97,
      "generated_answer": "Yes.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "no": 0.0003459910624573391
      },
      "individual_log_probs": {
        "no": -7.96909761428833
      },
      "max_prob": 0.0003459910624573391,
      "max_log_prob": -7.96909761428833,
      "best_answer": "no",
      "total_prob": 0.0003459910624573391,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 98,
      "generated_answer": "Nickelodeon.",
      "gold_answers": [
        "Nickelodeon"
      ],
      "reference_answer": "Nickelodeon",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Nickelodeon": 0.9992772669993998
      },
      "individual_log_probs": {
        "Nickelodeon": -0.0007229942980018222
      },
      "max_prob": 0.9992772669993998,
      "max_log_prob": -0.0007229942980018222,
      "best_answer": "Nickelodeon",
      "total_prob": 0.9992772669993998,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    },
    {
      "sample_idx": 99,
      "generated_answer": "On the Road with Austin and Santino.",
      "gold_answers": [
        "Project Runway"
      ],
      "reference_answer": "Project Runway",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Project Runway": 0.03514005294027102
      },
      "individual_log_probs": {
        "Project Runway": -3.3484136896713608
      },
      "max_prob": 0.03514005294027102,
      "max_log_prob": -3.3484136896713608,
      "best_answer": "Project Runway",
      "total_prob": 0.03514005294027102,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.8
    }
  ]
}