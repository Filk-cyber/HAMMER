{
  "metrics": {
    "weight_w": 0.9,
    "input_file": "/home/jiangjp/trace-idea/data/hotpotqa/hotpotqa_dev100ideal_with_reasoning_chains_modify_w09.json",
    "total_samples": 100,
    "exact_match": 0.47,
    "f1": 0.5238290043290044,
    "precision": 0.5233809523809524,
    "recall": 0.5271666666666667,
    "avg_max_log_prob": -6.380046089411998,
    "avg_max_prob": 0.3615478567497984,
    "avg_total_prob": 0.3615478567497984,
    "valid_max_prob_samples": 100,
    "invalid_max_prob_samples": 0,
    "avg_answer_count": 1.0,
    "avg_valid_answer_count": 1.0
  },
  "sample_info": [
    {
      "sample_idx": 0,
      "generated_answer": "No.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "no": 0.0008925486475591983
      },
      "individual_log_probs": {
        "no": -7.021429538726807
      },
      "max_prob": 0.0008925486475591983,
      "max_log_prob": -7.021429538726807,
      "best_answer": "no",
      "total_prob": 0.0008925486475591983,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 1,
      "generated_answer": "Boston College",
      "gold_answers": [
        "University of Delaware"
      ],
      "reference_answer": "University of Delaware",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "University of Delaware": 1.9055826080874967e-07
      },
      "individual_log_probs": {
        "University of Delaware": -15.47330785811937
      },
      "max_prob": 1.9055826080874967e-07,
      "max_log_prob": -15.47330785811937,
      "best_answer": "University of Delaware",
      "total_prob": 1.9055826080874967e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 2,
      "generated_answer": "None, the question is not answered in the provided context.",
      "gold_answers": [
        "NBC"
      ],
      "reference_answer": "NBC",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "NBC": 3.6241002386368895e-06
      },
      "individual_log_probs": {
        "NBC": -12.527904510498047
      },
      "max_prob": 3.6241002386368895e-06,
      "max_log_prob": -12.527904510498047,
      "best_answer": "NBC",
      "total_prob": 3.6241002386368895e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 3,
      "generated_answer": "Flowering plants.",
      "gold_answers": [
        "genus"
      ],
      "reference_answer": "genus",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "genus": 0.002820859337663359
      },
      "individual_log_probs": {
        "genus": -5.870713710784912
      },
      "max_prob": 0.002820859337663359,
      "max_log_prob": -5.870713710784912,
      "best_answer": "genus",
      "total_prob": 0.002820859337663359,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 4,
      "generated_answer": "Kirk Fogg",
      "gold_answers": [
        "Jeff Sutphen"
      ],
      "reference_answer": "Jeff Sutphen",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Jeff Sutphen": 0.28821976263006893
      },
      "individual_log_probs": {
        "Jeff Sutphen": -1.2440320251442927
      },
      "max_prob": 0.28821976263006893,
      "max_log_prob": -1.2440320251442927,
      "best_answer": "Jeff Sutphen",
      "total_prob": 0.28821976263006893,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 5,
      "generated_answer": "The Forest of Bowland.",
      "gold_answers": [
        "the Bowland Fells"
      ],
      "reference_answer": "the Bowland Fells",
      "em_score": 0.0,
      "f1_score": 0.4,
      "precision": 0.3333333333333333,
      "recall": 0.5,
      "individual_probs": {
        "the Bowland Fells": 0.09277867392867822
      },
      "individual_log_probs": {
        "the Bowland Fells": -2.377538472402648
      },
      "max_prob": 0.09277867392867822,
      "max_log_prob": -2.377538472402648,
      "best_answer": "the Bowland Fells",
      "total_prob": 0.09277867392867822,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 6,
      "generated_answer": "Queen",
      "gold_answers": [
        "Daft Punk"
      ],
      "reference_answer": "Daft Punk",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Daft Punk": 2.1155883499078187e-06
      },
      "individual_log_probs": {
        "Daft Punk": -13.066177604525365
      },
      "max_prob": 2.1155883499078187e-06,
      "max_log_prob": -13.066177604525365,
      "best_answer": "Daft Punk",
      "total_prob": 2.1155883499078187e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 7,
      "generated_answer": "\"3rd Rock from the Sun\"",
      "gold_answers": [
        "3rd Rock from the Sun"
      ],
      "reference_answer": "3rd Rock from the Sun",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "3rd Rock from the Sun": 0.23642181600712867
      },
      "individual_log_probs": {
        "3rd Rock from the Sun": -1.442137713414013
      },
      "max_prob": 0.23642181600712867,
      "max_log_prob": -1.442137713414013,
      "best_answer": "3rd Rock from the Sun",
      "total_prob": 0.23642181600712867,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 8,
      "generated_answer": "The Island.",
      "gold_answers": [
        "The Island"
      ],
      "reference_answer": "The Island",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Island": 0.43609328322151913
      },
      "individual_log_probs": {
        "The Island": -0.8298991061747074
      },
      "max_prob": 0.43609328322151913,
      "max_log_prob": -0.8298991061747074,
      "best_answer": "The Island",
      "total_prob": 0.43609328322151913,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 9,
      "generated_answer": "The Great Mouse Detective",
      "gold_answers": [
        "The Great Mouse Detective"
      ],
      "reference_answer": "The Great Mouse Detective",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Great Mouse Detective": 0.9726093184918699
      },
      "individual_log_probs": {
        "The Great Mouse Detective": -0.02777280004676186
      },
      "max_prob": 0.9726093184918699,
      "max_log_prob": -0.02777280004676186,
      "best_answer": "The Great Mouse Detective",
      "total_prob": 0.9726093184918699,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 10,
      "generated_answer": "20 March to 1 May 2003",
      "gold_answers": [
        "20 March to 1 May 2003"
      ],
      "reference_answer": "20 March to 1 May 2003",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "20 March to 1 May 2003": 0.39943348880036955
      },
      "individual_log_probs": {
        "20 March to 1 May 2003": -0.9177080137428675
      },
      "max_prob": 0.39943348880036955,
      "max_log_prob": -0.9177080137428675,
      "best_answer": "20 March to 1 May 2003",
      "total_prob": 0.39943348880036955,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 11,
      "generated_answer": "Both Borassodendron and Chelidonium are genera of flowering plants.",
      "gold_answers": [
        "plant"
      ],
      "reference_answer": "plant",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "plant": 0.0024784047044467336
      },
      "individual_log_probs": {
        "plant": -6.000140190124512
      },
      "max_prob": 0.0024784047044467336,
      "max_log_prob": -6.000140190124512,
      "best_answer": "plant",
      "total_prob": 0.0024784047044467336,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 12,
      "generated_answer": "Montclair State University.",
      "gold_answers": [
        "Montclair State University"
      ],
      "reference_answer": "Montclair State University",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Montclair State University": 0.9995061850718052
      },
      "individual_log_probs": {
        "Montclair State University": -0.0004939368949408163
      },
      "max_prob": 0.9995061850718052,
      "max_log_prob": -0.0004939368949408163,
      "best_answer": "Montclair State University",
      "total_prob": 0.9995061850718052,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 13,
      "generated_answer": "Preity Zinta",
      "gold_answers": [
        "Soha Ali Khan"
      ],
      "reference_answer": "Soha Ali Khan",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Soha Ali Khan": 8.640251178612059e-07
      },
      "individual_log_probs": {
        "Soha Ali Khan": -13.961663996966308
      },
      "max_prob": 8.640251178612059e-07,
      "max_log_prob": -13.961663996966308,
      "best_answer": "Soha Ali Khan",
      "total_prob": 8.640251178612059e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 14,
      "generated_answer": "no",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "yes": 1.5252618857487816e-07
      },
      "individual_log_probs": {
        "yes": -15.695929527282715
      },
      "max_prob": 1.5252618857487816e-07,
      "max_log_prob": -15.695929527282715,
      "best_answer": "yes",
      "total_prob": 1.5252618857487816e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 15,
      "generated_answer": "Bodo League massacre.",
      "gold_answers": [
        "Bodo League massacre"
      ],
      "reference_answer": "Bodo League massacre",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Bodo League massacre": 0.7512672279187195
      },
      "individual_log_probs": {
        "Bodo League massacre": -0.285993861057932
      },
      "max_prob": 0.7512672279187195,
      "max_log_prob": -0.285993861057932,
      "best_answer": "Bodo League massacre",
      "total_prob": 0.7512672279187195,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 16,
      "generated_answer": "Phebalium",
      "gold_answers": [
        "Ariocarpus"
      ],
      "reference_answer": "Ariocarpus",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Ariocarpus": 3.2856379099242596e-06
      },
      "individual_log_probs": {
        "Ariocarpus": -12.625949736166262
      },
      "max_prob": 3.2856379099242596e-06,
      "max_log_prob": -12.625949736166262,
      "best_answer": "Ariocarpus",
      "total_prob": 3.2856379099242596e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 17,
      "generated_answer": "1,042",
      "gold_answers": [
        "761"
      ],
      "reference_answer": "761",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "761": 0.3296944047155108
      },
      "individual_log_probs": {
        "761": -1.1095890998840332
      },
      "max_prob": 0.3296944047155108,
      "max_log_prob": -1.1095890998840332,
      "best_answer": "761",
      "total_prob": 0.3296944047155108,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 18,
      "generated_answer": "Cnut the Great",
      "gold_answers": [
        "Cnut the Great"
      ],
      "reference_answer": "Cnut the Great",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Cnut the Great": 0.9996393986823555
      },
      "individual_log_probs": {
        "Cnut the Great": -0.0003606663499340357
      },
      "max_prob": 0.9996393986823555,
      "max_log_prob": -0.0003606663499340357,
      "best_answer": "Cnut the Great",
      "total_prob": 0.9996393986823555,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 19,
      "generated_answer": "Channel V.",
      "gold_answers": [
        "Channel V"
      ],
      "reference_answer": "Channel V",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Channel V": 0.9960984189514007
      },
      "individual_log_probs": {
        "Channel V": -0.003909212071107504
      },
      "max_prob": 0.9960984189514007,
      "max_log_prob": -0.003909212071107504,
      "best_answer": "Channel V",
      "total_prob": 0.9960984189514007,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 20,
      "generated_answer": "Albert Camus",
      "gold_answers": [
        "Albert Camus"
      ],
      "reference_answer": "Albert Camus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Albert Camus": 0.9620168291369338
      },
      "individual_log_probs": {
        "Albert Camus": -0.0387233345641107
      },
      "max_prob": 0.9620168291369338,
      "max_log_prob": -0.0387233345641107,
      "best_answer": "Albert Camus",
      "total_prob": 0.9620168291369338,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 21,
      "generated_answer": "Paul Marcinkus",
      "gold_answers": [
        "Paul Marcinkus"
      ],
      "reference_answer": "Paul Marcinkus",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Paul Marcinkus": 0.9916822331447417
      },
      "individual_log_probs": {
        "Paul Marcinkus": -0.008352552504902633
      },
      "max_prob": 0.9916822331447417,
      "max_log_prob": -0.008352552504902633,
      "best_answer": "Paul Marcinkus",
      "total_prob": 0.9916822331447417,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 22,
      "generated_answer": "1852",
      "gold_answers": [
        "1904"
      ],
      "reference_answer": "1904",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1904": 1.0929096055669674e-10
      },
      "individual_log_probs": {
        "1904": -22.937007427215576
      },
      "max_prob": 1.0929096055669674e-10,
      "max_log_prob": -22.937007427215576,
      "best_answer": "1904",
      "total_prob": 1.0929096055669674e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 23,
      "generated_answer": "Ralph Fiennes",
      "gold_answers": [
        "Ralph Fiennes"
      ],
      "reference_answer": "Ralph Fiennes",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Ralph Fiennes": 0.9996982280235365
      },
      "individual_log_probs": {
        "Ralph Fiennes": -0.00030181751878899377
      },
      "max_prob": 0.9996982280235365,
      "max_log_prob": -0.00030181751878899377,
      "best_answer": "Ralph Fiennes",
      "total_prob": 0.9996982280235365,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 24,
      "generated_answer": "runner-up",
      "gold_answers": [
        "runner-up"
      ],
      "reference_answer": "runner-up",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "runner-up": 0.7378673608906452
      },
      "individual_log_probs": {
        "runner-up": -0.30399119832509314
      },
      "max_prob": 0.7378673608906452,
      "max_log_prob": -0.30399119832509314,
      "best_answer": "runner-up",
      "total_prob": 0.7378673608906452,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 25,
      "generated_answer": "Swedish.",
      "gold_answers": [
        "Swedish"
      ],
      "reference_answer": "Swedish",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Swedish": 0.9998249121047987
      },
      "individual_log_probs": {
        "Swedish": -0.00017510322487623853
      },
      "max_prob": 0.9998249121047987,
      "max_log_prob": -0.00017510322487623853,
      "best_answer": "Swedish",
      "total_prob": 0.9998249121047987,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 26,
      "generated_answer": "Vice",
      "gold_answers": [
        "Vice"
      ],
      "reference_answer": "Vice",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Vice": 0.9982749036189045
      },
      "individual_log_probs": {
        "Vice": -0.0017265860733459704
      },
      "max_prob": 0.9982749036189045,
      "max_log_prob": -0.0017265860733459704,
      "best_answer": "Vice",
      "total_prob": 0.9982749036189045,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 27,
      "generated_answer": "The Preppie Murder.",
      "gold_answers": [
        "The Preppie Murder"
      ],
      "reference_answer": "The Preppie Murder",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "The Preppie Murder": 0.983020480247809
      },
      "individual_log_probs": {
        "The Preppie Murder": -0.017125324618831428
      },
      "max_prob": 0.983020480247809,
      "max_log_prob": -0.017125324618831428,
      "best_answer": "The Preppie Murder",
      "total_prob": 0.983020480247809,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 28,
      "generated_answer": "Isolde Kostner",
      "gold_answers": [
        "Isolde Kostner"
      ],
      "reference_answer": "Isolde Kostner",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Isolde Kostner": 0.996780817594993
      },
      "individual_log_probs": {
        "Isolde Kostner": -0.0032243751198777204
      },
      "max_prob": 0.996780817594993,
      "max_log_prob": -0.0032243751198777204,
      "best_answer": "Isolde Kostner",
      "total_prob": 0.996780817594993,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 29,
      "generated_answer": "Beverly Hills High School.",
      "gold_answers": [
        "Beverly Hills High School"
      ],
      "reference_answer": "Beverly Hills High School",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Beverly Hills High School": 0.9858636529743948
      },
      "individual_log_probs": {
        "Beverly Hills High School": -0.014237216928904672
      },
      "max_prob": 0.9858636529743948,
      "max_log_prob": -0.014237216928904672,
      "best_answer": "Beverly Hills High School",
      "total_prob": 0.9858636529743948,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 30,
      "generated_answer": "1989",
      "gold_answers": [
        "fifth album"
      ],
      "reference_answer": "fifth album",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "fifth album": 1.745685358908722e-07
      },
      "individual_log_probs": {
        "fifth album": -15.56094841659069
      },
      "max_prob": 1.745685358908722e-07,
      "max_log_prob": -15.56094841659069,
      "best_answer": "fifth album",
      "total_prob": 1.745685358908722e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 31,
      "generated_answer": "Middleweight",
      "gold_answers": [
        "Welterweight"
      ],
      "reference_answer": "Welterweight",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Welterweight": 0.0677059454410115
      },
      "individual_log_probs": {
        "Welterweight": -2.692581282520905
      },
      "max_prob": 0.0677059454410115,
      "max_log_prob": -2.692581282520905,
      "best_answer": "Welterweight",
      "total_prob": 0.0677059454410115,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 32,
      "generated_answer": "Times Beach.",
      "gold_answers": [
        "Times Beach"
      ],
      "reference_answer": "Times Beach",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Times Beach": 0.9995098110179164
      },
      "individual_log_probs": {
        "Times Beach": -0.0004903091639789636
      },
      "max_prob": 0.9995098110179164,
      "max_log_prob": -0.0004903091639789636,
      "best_answer": "Times Beach",
      "total_prob": 0.9995098110179164,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 33,
      "generated_answer": "Beverly Hills Supper Club fire",
      "gold_answers": [
        "Beverly Hills Supper Club fire"
      ],
      "reference_answer": "Beverly Hills Supper Club fire",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Beverly Hills Supper Club fire": 0.4652443783857885
      },
      "individual_log_probs": {
        "Beverly Hills Supper Club fire": -0.76519246652947
      },
      "max_prob": 0.4652443783857885,
      "max_log_prob": -0.76519246652947,
      "best_answer": "Beverly Hills Supper Club fire",
      "total_prob": 0.4652443783857885,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 34,
      "generated_answer": "Wide receiver.",
      "gold_answers": [
        "quarterback"
      ],
      "reference_answer": "quarterback",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "quarterback": 8.700956849459966e-06
      },
      "individual_log_probs": {
        "quarterback": -11.652077555654841
      },
      "max_prob": 8.700956849459966e-06,
      "max_log_prob": -11.652077555654841,
      "best_answer": "quarterback",
      "total_prob": 8.700956849459966e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 35,
      "generated_answer": "Dusty Dvoracek",
      "gold_answers": [
        "Dusty Dvoracek"
      ],
      "reference_answer": "Dusty Dvoracek",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Dusty Dvoracek": 0.9596657367189483
      },
      "individual_log_probs": {
        "Dusty Dvoracek": -0.041170246070549865
      },
      "max_prob": 0.9596657367189483,
      "max_log_prob": -0.041170246070549865,
      "best_answer": "Dusty Dvoracek",
      "total_prob": 0.9596657367189483,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 36,
      "generated_answer": "No.",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "no": 0.005463337898997166
      },
      "individual_log_probs": {
        "no": -5.209695339202881
      },
      "max_prob": 0.005463337898997166,
      "max_log_prob": -5.209695339202881,
      "best_answer": "no",
      "total_prob": 0.005463337898997166,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 37,
      "generated_answer": "The New York Yankees.",
      "gold_answers": [
        "Texas Rangers"
      ],
      "reference_answer": "Texas Rangers",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Texas Rangers": 2.2571808194797057e-08
      },
      "individual_log_probs": {
        "Texas Rangers": -17.60656413435936
      },
      "max_prob": 2.2571808194797057e-08,
      "max_log_prob": -17.60656413435936,
      "best_answer": "Texas Rangers",
      "total_prob": 2.2571808194797057e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 38,
      "generated_answer": "Germanic, Baltic, Thracian, Slavic, Hungarian, and Romanian.",
      "gold_answers": [
        "Germanic, Baltic and Thracian tribes"
      ],
      "reference_answer": "Germanic, Baltic and Thracian tribes",
      "em_score": 0.0,
      "f1_score": 0.6666666666666666,
      "precision": 0.5714285714285714,
      "recall": 0.8,
      "individual_probs": {
        "Germanic, Baltic and Thracian tribes": 0.07969333769906824
      },
      "individual_log_probs": {
        "Germanic, Baltic and Thracian tribes": -2.529569288912512
      },
      "max_prob": 0.07969333769906824,
      "max_log_prob": -2.529569288912512,
      "best_answer": "Germanic, Baltic and Thracian tribes",
      "total_prob": 0.07969333769906824,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 39,
      "generated_answer": "American",
      "gold_answers": [
        "American"
      ],
      "reference_answer": "American",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "American": 0.9990486620926138
      },
      "individual_log_probs": {
        "American": -0.0009517907164990902
      },
      "max_prob": 0.9990486620926138,
      "max_log_prob": -0.0009517907164990902,
      "best_answer": "American",
      "total_prob": 0.9990486620926138,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 40,
      "generated_answer": "Mario Lemieux",
      "gold_answers": [
        "Mario Lemieux"
      ],
      "reference_answer": "Mario Lemieux",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mario Lemieux": 0.9998648334788528
      },
      "individual_log_probs": {
        "Mario Lemieux": -0.00013517565696474776
      },
      "max_prob": 0.9998648334788528,
      "max_log_prob": -0.00013517565696474776,
      "best_answer": "Mario Lemieux",
      "total_prob": 0.9998648334788528,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 41,
      "generated_answer": "Mumbai",
      "gold_answers": [
        "Mumbai"
      ],
      "reference_answer": "Mumbai",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Mumbai": 0.9984357978244238
      },
      "individual_log_probs": {
        "Mumbai": -0.0015654268170237629
      },
      "max_prob": 0.9984357978244238,
      "max_log_prob": -0.0015654268170237629,
      "best_answer": "Mumbai",
      "total_prob": 0.9984357978244238,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 42,
      "generated_answer": "Alexander Scriabin.",
      "gold_answers": [
        "Alexander Scriabin"
      ],
      "reference_answer": "Alexander Scriabin",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Alexander Scriabin": 0.9924226900576268
      },
      "individual_log_probs": {
        "Alexander Scriabin": -0.007606163603185223
      },
      "max_prob": 0.9924226900576268,
      "max_log_prob": -0.007606163603185223,
      "best_answer": "Alexander Scriabin",
      "total_prob": 0.9924226900576268,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 43,
      "generated_answer": "11",
      "gold_answers": [
        "Their biggest hit was \"Tall Cool One\", first released in 1959"
      ],
      "reference_answer": "Their biggest hit was \"Tall Cool One\", first released in 1959",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Their biggest hit was \"Tall Cool One\", first released in 1959": 7.565152676857523e-21
      },
      "individual_log_probs": {
        "Their biggest hit was \"Tall Cool One\", first released in 1959": -46.330734423834656
      },
      "max_prob": 7.565152676857523e-21,
      "max_log_prob": -46.330734423834656,
      "best_answer": "Their biggest hit was \"Tall Cool One\", first released in 1959",
      "total_prob": 7.565152676857523e-21,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 44,
      "generated_answer": "Jawaharlal Nehru",
      "gold_answers": [
        "first Prime Minister of India"
      ],
      "reference_answer": "first Prime Minister of India",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "first Prime Minister of India": 0.0015710247675792873
      },
      "individual_log_probs": {
        "first Prime Minister of India": -6.45602715434643
      },
      "max_prob": 0.0015710247675792873,
      "max_log_prob": -6.45602715434643,
      "best_answer": "first Prime Minister of India",
      "total_prob": 0.0015710247675792873,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 45,
      "generated_answer": "United Kingdom",
      "gold_answers": [
        "Ireland"
      ],
      "reference_answer": "Ireland",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Ireland": 0.10016179924364997
      },
      "individual_log_probs": {
        "Ireland": -2.3009684080971056
      },
      "max_prob": 0.10016179924364997,
      "max_log_prob": -2.3009684080971056,
      "best_answer": "Ireland",
      "total_prob": 0.10016179924364997,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 46,
      "generated_answer": "58 to 64 passengers.",
      "gold_answers": [
        "72-78"
      ],
      "reference_answer": "72-78",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "72-78": 5.909307865075416e-07
      },
      "individual_log_probs": {
        "72-78": -14.341566938906908
      },
      "max_prob": 5.909307865075416e-07,
      "max_log_prob": -14.341566938906908,
      "best_answer": "72-78",
      "total_prob": 5.909307865075416e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 47,
      "generated_answer": "Heroica Matamoros",
      "gold_answers": [
        "Heroica Matamoros"
      ],
      "reference_answer": "Heroica Matamoros",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Heroica Matamoros": 0.9989659490082071
      },
      "individual_log_probs": {
        "Heroica Matamoros": -0.0010345859913627464
      },
      "max_prob": 0.9989659490082071,
      "max_log_prob": -0.0010345859913627464,
      "best_answer": "Heroica Matamoros",
      "total_prob": 0.9989659490082071,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 48,
      "generated_answer": "George Washington",
      "gold_answers": [
        "Benedict Arnold"
      ],
      "reference_answer": "Benedict Arnold",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Benedict Arnold": 9.59107050982617e-05
      },
      "individual_log_probs": {
        "Benedict Arnold": -9.252092954585692
      },
      "max_prob": 9.59107050982617e-05,
      "max_log_prob": -9.252092954585692,
      "best_answer": "Benedict Arnold",
      "total_prob": 9.59107050982617e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 49,
      "generated_answer": "South Kesteven Sports Stadium",
      "gold_answers": [
        "South Kesteven Sports Stadium"
      ],
      "reference_answer": "South Kesteven Sports Stadium",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "South Kesteven Sports Stadium": 0.7190168462194054
      },
      "individual_log_probs": {
        "South Kesteven Sports Stadium": -0.3298704914668562
      },
      "max_prob": 0.7190168462194054,
      "max_log_prob": -0.3298704914668562,
      "best_answer": "South Kesteven Sports Stadium",
      "total_prob": 0.7190168462194054,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 50,
      "generated_answer": "Hisahito, Prince Akishino.",
      "gold_answers": [
        "Aiko, Princess Toshi"
      ],
      "reference_answer": "Aiko, Princess Toshi",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Aiko, Princess Toshi": 3.203898734989877e-06
      },
      "individual_log_probs": {
        "Aiko, Princess Toshi": -12.651142135066038
      },
      "max_prob": 3.203898734989877e-06,
      "max_log_prob": -12.651142135066038,
      "best_answer": "Aiko, Princess Toshi",
      "total_prob": 3.203898734989877e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 51,
      "generated_answer": "Naseem Hamed",
      "gold_answers": [
        "Marco Antonio Barrera"
      ],
      "reference_answer": "Marco Antonio Barrera",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Marco Antonio Barrera": 9.49431853881405e-05
      },
      "individual_log_probs": {
        "Marco Antonio Barrera": -9.262231893811986
      },
      "max_prob": 9.49431853881405e-05,
      "max_log_prob": -9.262231893811986,
      "best_answer": "Marco Antonio Barrera",
      "total_prob": 9.49431853881405e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 52,
      "generated_answer": "the Iberian Peninsula",
      "gold_answers": [
        "Iberian Peninsula"
      ],
      "reference_answer": "Iberian Peninsula",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Iberian Peninsula": 0.31453172621194814
      },
      "individual_log_probs": {
        "Iberian Peninsula": -1.1566703296721244
      },
      "max_prob": 0.31453172621194814,
      "max_log_prob": -1.1566703296721244,
      "best_answer": "Iberian Peninsula",
      "total_prob": 0.31453172621194814,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 53,
      "generated_answer": "The Hume Highway.",
      "gold_answers": [
        "Hume Highway"
      ],
      "reference_answer": "Hume Highway",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Hume Highway": 0.41575140579535846
      },
      "individual_log_probs": {
        "Hume Highway": -0.8776677795669912
      },
      "max_prob": 0.41575140579535846,
      "max_log_prob": -0.8776677795669912,
      "best_answer": "Hume Highway",
      "total_prob": 0.41575140579535846,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 54,
      "generated_answer": "University of North Texas.",
      "gold_answers": [
        "University of North Texas"
      ],
      "reference_answer": "University of North Texas",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "University of North Texas": 0.01783555294530021
      },
      "individual_log_probs": {
        "University of North Texas": -4.026561457270873
      },
      "max_prob": 0.01783555294530021,
      "max_log_prob": -4.026561457270873,
      "best_answer": "University of North Texas",
      "total_prob": 0.01783555294530021,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 55,
      "generated_answer": "A160",
      "gold_answers": [
        "the A160"
      ],
      "reference_answer": "the A160",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "the A160": 0.28630186786119904
      },
      "individual_log_probs": {
        "the A160": -1.2507085427596394
      },
      "max_prob": 0.28630186786119904,
      "max_log_prob": -1.2507085427596394,
      "best_answer": "the A160",
      "total_prob": 0.28630186786119904,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 56,
      "generated_answer": "British",
      "gold_answers": [
        "American"
      ],
      "reference_answer": "American",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "American": 6.592409650164054e-05
      },
      "individual_log_probs": {
        "American": -9.627006530761719
      },
      "max_prob": 6.592409650164054e-05,
      "max_log_prob": -9.627006530761719,
      "best_answer": "American",
      "total_prob": 6.592409650164054e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 57,
      "generated_answer": "['Capsicum chinense']",
      "gold_answers": [
        "\"Capsicum chinense\""
      ],
      "reference_answer": "\"Capsicum chinense\"",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "\"Capsicum chinense\"": 0.008390022897800019
      },
      "individual_log_probs": {
        "\"Capsicum chinense\"": -4.780712029329152
      },
      "max_prob": 0.008390022897800019,
      "max_log_prob": -4.780712029329152,
      "best_answer": "\"Capsicum chinense\"",
      "total_prob": 0.008390022897800019,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 58,
      "generated_answer": "Barack Obama",
      "gold_answers": [
        "Hillary Clinton"
      ],
      "reference_answer": "Hillary Clinton",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Hillary Clinton": 4.2069203691681816e-07
      },
      "individual_log_probs": {
        "Hillary Clinton": -14.68136477470398
      },
      "max_prob": 4.2069203691681816e-07,
      "max_log_prob": -14.68136477470398,
      "best_answer": "Hillary Clinton",
      "total_prob": 4.2069203691681816e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 59,
      "generated_answer": "Lynyrd Skynyrd.",
      "gold_answers": [
        "Lynyrd Skynyrd"
      ],
      "reference_answer": "Lynyrd Skynyrd",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Lynyrd Skynyrd": 0.9798079879658423
      },
      "individual_log_probs": {
        "Lynyrd Skynyrd": -0.020398657161535994
      },
      "max_prob": 0.9798079879658423,
      "max_log_prob": -0.020398657161535994,
      "best_answer": "Lynyrd Skynyrd",
      "total_prob": 0.9798079879658423,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 60,
      "generated_answer": "Sehnsucht",
      "gold_answers": [
        "Herzeleid"
      ],
      "reference_answer": "Herzeleid",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Herzeleid": 4.1674035831072386e-07
      },
      "individual_log_probs": {
        "Herzeleid": -14.690802451010313
      },
      "max_prob": 4.1674035831072386e-07,
      "max_log_prob": -14.690802451010313,
      "best_answer": "Herzeleid",
      "total_prob": 4.1674035831072386e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 61,
      "generated_answer": "1897",
      "gold_answers": [
        "1897"
      ],
      "reference_answer": "1897",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "1897": 0.5411342175788286
      },
      "individual_log_probs": {
        "1897": -0.6140879392623617
      },
      "max_prob": 0.5411342175788286,
      "max_log_prob": -0.6140879392623617,
      "best_answer": "1897",
      "total_prob": 0.5411342175788286,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 62,
      "generated_answer": "Mohamed Elneny.",
      "gold_answers": [
        "Ahmed Elmohamady"
      ],
      "reference_answer": "Ahmed Elmohamady",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Ahmed Elmohamady": 0.0039820428629210244
      },
      "individual_log_probs": {
        "Ahmed Elmohamady": -5.525960309229049
      },
      "max_prob": 0.0039820428629210244,
      "max_log_prob": -5.525960309229049,
      "best_answer": "Ahmed Elmohamady",
      "total_prob": 0.0039820428629210244,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 63,
      "generated_answer": "\"live-action\"",
      "gold_answers": [
        "animation"
      ],
      "reference_answer": "animation",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "animation": 1.8071393416483956e-08
      },
      "individual_log_probs": {
        "animation": -17.828935623168945
      },
      "max_prob": 1.8071393416483956e-08,
      "max_log_prob": -17.828935623168945,
      "best_answer": "animation",
      "total_prob": 1.8071393416483956e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 64,
      "generated_answer": "Jeff Healey",
      "gold_answers": [
        "Jeff Healey"
      ],
      "reference_answer": "Jeff Healey",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Jeff Healey": 0.9997748513893373
      },
      "individual_log_probs": {
        "Jeff Healey": -0.00022517396041621396
      },
      "max_prob": 0.9997748513893373,
      "max_log_prob": -0.00022517396041621396,
      "best_answer": "Jeff Healey",
      "total_prob": 0.9997748513893373,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 65,
      "generated_answer": "The Royal Air Force (RAF).",
      "gold_answers": [
        "British Armed Forces"
      ],
      "reference_answer": "British Armed Forces",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "British Armed Forces": 0.00018206736045119656
      },
      "individual_log_probs": {
        "British Armed Forces": -8.611133826992955
      },
      "max_prob": 0.00018206736045119656,
      "max_log_prob": -8.611133826992955,
      "best_answer": "British Armed Forces",
      "total_prob": 0.00018206736045119656,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 66,
      "generated_answer": "Kadhalan",
      "gold_answers": [
        "Kadhalan"
      ],
      "reference_answer": "Kadhalan",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Kadhalan": 0.9712612183932556
      },
      "individual_log_probs": {
        "Kadhalan": -0.02915982689773955
      },
      "max_prob": 0.9712612183932556,
      "max_log_prob": -0.02915982689773955,
      "best_answer": "Kadhalan",
      "total_prob": 0.9712612183932556,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 67,
      "generated_answer": "Vydubychi Hill",
      "gold_answers": [
        "Cherepanov Hill"
      ],
      "reference_answer": "Cherepanov Hill",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Cherepanov Hill": 0.33092380053230097
      },
      "individual_log_probs": {
        "Cherepanov Hill": -1.1058671399796367
      },
      "max_prob": 0.33092380053230097,
      "max_log_prob": -1.1058671399796367,
      "best_answer": "Cherepanov Hill",
      "total_prob": 0.33092380053230097,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 68,
      "generated_answer": "Julien Benneteau",
      "gold_answers": [
        "\"Arantxa\" Sánchez Vicario"
      ],
      "reference_answer": "\"Arantxa\" Sánchez Vicario",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "\"Arantxa\" Sánchez Vicario": 5.733855246090831e-16
      },
      "individual_log_probs": {
        "\"Arantxa\" Sánchez Vicario": -35.09497336560162
      },
      "max_prob": 5.733855246090831e-16,
      "max_log_prob": -35.09497336560162,
      "best_answer": "\"Arantxa\" Sánchez Vicario",
      "total_prob": 5.733855246090831e-16,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 69,
      "generated_answer": "University of Alabama",
      "gold_answers": [
        "University of Alabama"
      ],
      "reference_answer": "University of Alabama",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "University of Alabama": 0.5186250055614596
      },
      "individual_log_probs": {
        "University of Alabama": -0.6565741896056352
      },
      "max_prob": 0.5186250055614596,
      "max_log_prob": -0.6565741896056352,
      "best_answer": "University of Alabama",
      "total_prob": 0.5186250055614596,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 70,
      "generated_answer": "Pizza Hut",
      "gold_answers": [
        "Pizza Hut"
      ],
      "reference_answer": "Pizza Hut",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Pizza Hut": 0.999107082258033
      },
      "individual_log_probs": {
        "Pizza Hut": -0.0008933166304814222
      },
      "max_prob": 0.999107082258033,
      "max_log_prob": -0.0008933166304814222,
      "best_answer": "Pizza Hut",
      "total_prob": 0.999107082258033,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 71,
      "generated_answer": "1968",
      "gold_answers": [
        "1939"
      ],
      "reference_answer": "1939",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1939": 1.7932834798333142e-07
      },
      "individual_log_probs": {
        "1939": -15.534047365188599
      },
      "max_prob": 1.7932834798333142e-07,
      "max_log_prob": -15.534047365188599,
      "best_answer": "1939",
      "total_prob": 1.7932834798333142e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 72,
      "generated_answer": "1964",
      "gold_answers": [
        "1962"
      ],
      "reference_answer": "1962",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "1962": 0.004453963269845935
      },
      "individual_log_probs": {
        "1962": -5.4139609567355365
      },
      "max_prob": 0.004453963269845935,
      "max_log_prob": -5.4139609567355365,
      "best_answer": "1962",
      "total_prob": 0.004453963269845935,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 73,
      "generated_answer": "Aquatium",
      "gold_answers": [
        "Metreon"
      ],
      "reference_answer": "Metreon",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Metreon": 6.609961736554909e-10
      },
      "individual_log_probs": {
        "Metreon": -21.137273064814508
      },
      "max_prob": 6.609961736554909e-10,
      "max_log_prob": -21.137273064814508,
      "best_answer": "Metreon",
      "total_prob": 6.609961736554909e-10,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 74,
      "generated_answer": "Rolandas Paksas.",
      "gold_answers": [
        "Viktor Uspaskich"
      ],
      "reference_answer": "Viktor Uspaskich",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Viktor Uspaskich": 8.632163890612642e-08
      },
      "individual_log_probs": {
        "Viktor Uspaskich": -16.265185529775863
      },
      "max_prob": 8.632163890612642e-08,
      "max_log_prob": -16.265185529775863,
      "best_answer": "Viktor Uspaskich",
      "total_prob": 8.632163890612642e-08,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 75,
      "generated_answer": "The Verve.",
      "gold_answers": [
        "Spacemen 3"
      ],
      "reference_answer": "Spacemen 3",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Spacemen 3": 3.489283642535328e-05
      },
      "individual_log_probs": {
        "Spacemen 3": -10.263229009843144
      },
      "max_prob": 3.489283642535328e-05,
      "max_log_prob": -10.263229009843144,
      "best_answer": "Spacemen 3",
      "total_prob": 3.489283642535328e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 76,
      "generated_answer": "Stephen King.",
      "gold_answers": [
        "Stephen King"
      ],
      "reference_answer": "Stephen King",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Stephen King": 0.9983413241151501
      },
      "individual_log_probs": {
        "Stephen King": -0.0016600530107098166
      },
      "max_prob": 0.9983413241151501,
      "max_log_prob": -0.0016600530107098166,
      "best_answer": "Stephen King",
      "total_prob": 0.9983413241151501,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 77,
      "generated_answer": "formulate the classical theory of electromagnetic radiation",
      "gold_answers": [
        "classical theory of electromagnetic radiation"
      ],
      "reference_answer": "classical theory of electromagnetic radiation",
      "em_score": 0.0,
      "f1_score": 0.9090909090909091,
      "precision": 0.8333333333333334,
      "recall": 1.0,
      "individual_probs": {
        "classical theory of electromagnetic radiation": 0.0037961249830744137
      },
      "individual_log_probs": {
        "classical theory of electromagnetic radiation": -5.573774473835897
      },
      "max_prob": 0.0037961249830744137,
      "max_log_prob": -5.573774473835897,
      "best_answer": "classical theory of electromagnetic radiation",
      "total_prob": 0.0037961249830744137,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 78,
      "generated_answer": "\"The Country Bears\"",
      "gold_answers": [
        "The London Connection"
      ],
      "reference_answer": "The London Connection",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "The London Connection": 0.0007580673846064958
      },
      "individual_log_probs": {
        "The London Connection": -7.184738278375335
      },
      "max_prob": 0.0007580673846064958,
      "max_log_prob": -7.184738278375335,
      "best_answer": "The London Connection",
      "total_prob": 0.0007580673846064958,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 79,
      "generated_answer": "Canadian",
      "gold_answers": [
        "Mexican"
      ],
      "reference_answer": "Mexican",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Mexican": 5.753104727075412e-07
      },
      "individual_log_probs": {
        "Mexican": -14.368355989336123
      },
      "max_prob": 5.753104727075412e-07,
      "max_log_prob": -14.368355989336123,
      "best_answer": "Mexican",
      "total_prob": 5.753104727075412e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 80,
      "generated_answer": "December 7, 2008",
      "gold_answers": [
        "December 7, 2008"
      ],
      "reference_answer": "December 7, 2008",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "December 7, 2008": 0.9999220422441657
      },
      "individual_log_probs": {
        "December 7, 2008": -7.796079469812867e-05
      },
      "max_prob": 0.9999220422441657,
      "max_log_prob": -7.796079469812867e-05,
      "best_answer": "December 7, 2008",
      "total_prob": 0.9999220422441657,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 81,
      "generated_answer": "no",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "yes": 1.3632661952495279e-05
      },
      "individual_log_probs": {
        "yes": -11.203042030334473
      },
      "max_prob": 1.3632661952495279e-05,
      "max_log_prob": -11.203042030334473,
      "best_answer": "yes",
      "total_prob": 1.3632661952495279e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 82,
      "generated_answer": "Football.",
      "gold_answers": [
        "Football"
      ],
      "reference_answer": "Football",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Football": 0.9205205441870093
      },
      "individual_log_probs": {
        "Football": -0.08281596004962921
      },
      "max_prob": 0.9205205441870093,
      "max_log_prob": -0.08281596004962921,
      "best_answer": "Football",
      "total_prob": 0.9205205441870093,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 83,
      "generated_answer": "Betty X",
      "gold_answers": [
        "Betty X"
      ],
      "reference_answer": "Betty X",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Betty X": 0.8154336247433212
      },
      "individual_log_probs": {
        "Betty X": -0.20403525233251685
      },
      "max_prob": 0.8154336247433212,
      "max_log_prob": -0.20403525233251685,
      "best_answer": "Betty X",
      "total_prob": 0.8154336247433212,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 84,
      "generated_answer": "Pizza Inn",
      "gold_answers": [
        "Extreme Pizza"
      ],
      "reference_answer": "Extreme Pizza",
      "em_score": 0.0,
      "f1_score": 0.5,
      "precision": 0.5,
      "recall": 0.5,
      "individual_probs": {
        "Extreme Pizza": 4.7075478368544997e-07
      },
      "individual_log_probs": {
        "Extreme Pizza": -14.568928507622331
      },
      "max_prob": 4.7075478368544997e-07,
      "max_log_prob": -14.568928507622331,
      "best_answer": "Extreme Pizza",
      "total_prob": 4.7075478368544997e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 85,
      "generated_answer": "Tina Turner (Anna Mae Bullock)",
      "gold_answers": [
        "Anna Mae Bullock"
      ],
      "reference_answer": "Anna Mae Bullock",
      "em_score": 0.0,
      "f1_score": 0.7499999999999999,
      "precision": 0.6,
      "recall": 1.0,
      "individual_probs": {
        "Anna Mae Bullock": 0.25426919301359086
      },
      "individual_log_probs": {
        "Anna Mae Bullock": -1.3693617581569129
      },
      "max_prob": 0.25426919301359086,
      "max_log_prob": -1.3693617581569129,
      "best_answer": "Anna Mae Bullock",
      "total_prob": 0.25426919301359086,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 86,
      "generated_answer": "Blaik Field.",
      "gold_answers": [
        "Michie Stadium"
      ],
      "reference_answer": "Michie Stadium",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Michie Stadium": 6.282490950125048e-07
      },
      "individual_log_probs": {
        "Michie Stadium": -14.280329101013194
      },
      "max_prob": 6.282490950125048e-07,
      "max_log_prob": -14.280329101013194,
      "best_answer": "Michie Stadium",
      "total_prob": 6.282490950125048e-07,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 87,
      "generated_answer": "Pacific Ocean",
      "gold_answers": [
        "the Pacific Ocean"
      ],
      "reference_answer": "the Pacific Ocean",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "the Pacific Ocean": 0.007168392939750507
      },
      "individual_log_probs": {
        "the Pacific Ocean": -4.938073786208406
      },
      "max_prob": 0.007168392939750507,
      "max_log_prob": -4.938073786208406,
      "best_answer": "the Pacific Ocean",
      "total_prob": 0.007168392939750507,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 88,
      "generated_answer": "Angela Nissel",
      "gold_answers": [
        "Angela R. Nissel"
      ],
      "reference_answer": "Angela R. Nissel",
      "em_score": 0.0,
      "f1_score": 0.8,
      "precision": 1.0,
      "recall": 0.6666666666666666,
      "individual_probs": {
        "Angela R. Nissel": 0.34002128401577886
      },
      "individual_log_probs": {
        "Angela R. Nissel": -1.0787470632848226
      },
      "max_prob": 0.34002128401577886,
      "max_log_prob": -1.0787470632848226,
      "best_answer": "Angela R. Nissel",
      "total_prob": 0.34002128401577886,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 89,
      "generated_answer": "the Netherlands",
      "gold_answers": [
        "Netherlands"
      ],
      "reference_answer": "Netherlands",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Netherlands": 0.15863297133438917
      },
      "individual_log_probs": {
        "Netherlands": -1.841162101511145
      },
      "max_prob": 0.15863297133438917,
      "max_log_prob": -1.841162101511145,
      "best_answer": "Netherlands",
      "total_prob": 0.15863297133438917,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 90,
      "generated_answer": "Michael Schumacher.",
      "gold_answers": [
        "Alessandro Nannini"
      ],
      "reference_answer": "Alessandro Nannini",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Alessandro Nannini": 1.9484603208425208e-05
      },
      "individual_log_probs": {
        "Alessandro Nannini": -10.845885983331414
      },
      "max_prob": 1.9484603208425208e-05,
      "max_log_prob": -10.845885983331414,
      "best_answer": "Alessandro Nannini",
      "total_prob": 1.9484603208425208e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 91,
      "generated_answer": "Theo van Gogh",
      "gold_answers": [
        "Theodorus \"Theo\" van Gogh"
      ],
      "reference_answer": "Theodorus \"Theo\" van Gogh",
      "em_score": 0.0,
      "f1_score": 0.8571428571428571,
      "precision": 1.0,
      "recall": 0.75,
      "individual_probs": {
        "Theodorus \"Theo\" van Gogh": 0.0009183869950630893
      },
      "individual_log_probs": {
        "Theodorus \"Theo\" van Gogh": -6.992891692927053
      },
      "max_prob": 0.0009183869950630893,
      "max_log_prob": -6.992891692927053,
      "best_answer": "Theodorus \"Theo\" van Gogh",
      "total_prob": 0.0009183869950630893,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 92,
      "generated_answer": "Indian Rebellion of 1857",
      "gold_answers": [
        "Indian Rebellion of 1857"
      ],
      "reference_answer": "Indian Rebellion of 1857",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Indian Rebellion of 1857": 0.652836158311556
      },
      "individual_log_probs": {
        "Indian Rebellion of 1857": -0.4264290872957872
      },
      "max_prob": 0.652836158311556,
      "max_log_prob": -0.4264290872957872,
      "best_answer": "Indian Rebellion of 1857",
      "total_prob": 0.652836158311556,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 93,
      "generated_answer": "no",
      "gold_answers": [
        "yes"
      ],
      "reference_answer": "yes",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "yes": 4.770541182073942e-06
      },
      "individual_log_probs": {
        "yes": -12.253050804138184
      },
      "max_prob": 4.770541182073942e-06,
      "max_log_prob": -12.253050804138184,
      "best_answer": "yes",
      "total_prob": 4.770541182073942e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 94,
      "generated_answer": "Ludovico Einaudi has performed with Canzoniere Grecanico Salentino.",
      "gold_answers": [
        "Italian pianist and composer"
      ],
      "reference_answer": "Italian pianist and composer",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Italian pianist and composer": 3.098978781351459e-05
      },
      "individual_log_probs": {
        "Italian pianist and composer": -10.381852833122082
      },
      "max_prob": 3.098978781351459e-05,
      "max_log_prob": -10.381852833122082,
      "best_answer": "Italian pianist and composer",
      "total_prob": 3.098978781351459e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 95,
      "generated_answer": "dancer",
      "gold_answers": [
        "actress"
      ],
      "reference_answer": "actress",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "actress": 1.0699820483069286e-06
      },
      "individual_log_probs": {
        "actress": -13.747868686914444
      },
      "max_prob": 1.0699820483069286e-06,
      "max_log_prob": -13.747868686914444,
      "best_answer": "actress",
      "total_prob": 1.0699820483069286e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 96,
      "generated_answer": "London",
      "gold_answers": [
        "Amsterdam"
      ],
      "reference_answer": "Amsterdam",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Amsterdam": 2.2636216588242558e-05
      },
      "individual_log_probs": {
        "Amsterdam": -10.695959430420771
      },
      "max_prob": 2.2636216588242558e-05,
      "max_log_prob": -10.695959430420771,
      "best_answer": "Amsterdam",
      "total_prob": 2.2636216588242558e-05,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 97,
      "generated_answer": "yes",
      "gold_answers": [
        "no"
      ],
      "reference_answer": "no",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "no": 1.911029197454435e-06
      },
      "individual_log_probs": {
        "no": -13.167868614196777
      },
      "max_prob": 1.911029197454435e-06,
      "max_log_prob": -13.167868614196777,
      "best_answer": "no",
      "total_prob": 1.911029197454435e-06,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 98,
      "generated_answer": "Nickelodeon",
      "gold_answers": [
        "Nickelodeon"
      ],
      "reference_answer": "Nickelodeon",
      "em_score": 1.0,
      "f1_score": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "individual_probs": {
        "Nickelodeon": 0.9992391681746094
      },
      "individual_log_probs": {
        "Nickelodeon": -0.0007611214048139914
      },
      "max_prob": 0.9992391681746094,
      "max_log_prob": -0.0007611214048139914,
      "best_answer": "Nickelodeon",
      "total_prob": 0.9992391681746094,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    },
    {
      "sample_idx": 99,
      "generated_answer": "On the Road with Austin and Santino.",
      "gold_answers": [
        "Project Runway"
      ],
      "reference_answer": "Project Runway",
      "em_score": 0.0,
      "f1_score": 0.0,
      "precision": 0.0,
      "recall": 0.0,
      "individual_probs": {
        "Project Runway": 0.06070033370736147
      },
      "individual_log_probs": {
        "Project Runway": -2.80180608328169
      },
      "max_prob": 0.06070033370736147,
      "max_log_prob": -2.80180608328169,
      "best_answer": "Project Runway",
      "total_prob": 0.06070033370736147,
      "answer_count": 1,
      "valid_answer_count": 1,
      "weight_w": 0.9
    }
  ]
}