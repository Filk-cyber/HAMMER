import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# è®¾ç½®å›¾è¡¨é£æ ¼
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # å»æ‰ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300


def create_enhanced_heatmaps(excel_path, save_path=None):
    """
    åˆ›å»ºå¢å¼ºç‰ˆçƒ­åŠ›å›¾ï¼Œæ›´å¥½åœ°æ˜¾ç¤ºç»†å¾®å·®å¼‚
    """
    # åŠ è½½æ•°æ®
    sentence_data = pd.read_excel(excel_path, sheet_name='Sentence')
    paragraph_data = pd.read_excel(excel_path, sheet_name='Paragraph')

    # å‡†å¤‡çƒ­åŠ›å›¾æ•°æ®
    metrics_columns = ['HotPotQA-EM', 'HotPotQA-F1', '2WikiMultiHopQA-EM',
                       '2WikiMultiHopQA-F1', 'MuSiQue-EM', 'MuSiQue-F1']

    sentence_heatmap = sentence_data[['weight'] + metrics_columns].set_index('weight')
    paragraph_heatmap = paragraph_data[['weight'] + metrics_columns].set_index('weight')

    # é‡å‘½ååˆ—åä¸ºè‹±æ–‡
    column_names = {
        'HotPotQA-EM': 'HotPotQA\nEM',
        'HotPotQA-F1': 'HotPotQA\nF1',
        '2WikiMultiHopQA-EM': '2WikiMQA\nEM',
        '2WikiMultiHopQA-F1': '2WikiMQA\nF1',
        'MuSiQue-EM': 'MuSiQue\nEM',
        'MuSiQue-F1': 'MuSiQue\nF1'
    }

    sentence_heatmap = sentence_heatmap.rename(columns=column_names)
    paragraph_heatmap = paragraph_heatmap.rename(columns=column_names)

    # åˆ›å»ºå­å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # åˆ†åˆ«è®¾ç½®é¢œè‰²èŒƒå›´ä»¥çªå‡ºå„è‡ªçš„å·®å¼‚
    sentence_vmin, sentence_vmax = sentence_heatmap.min().min(), sentence_heatmap.max().max()
    paragraph_vmin, paragraph_vmax = paragraph_heatmap.min().min(), paragraph_heatmap.max().max()

    print(f"Sentenceæ•°æ®èŒƒå›´: {sentence_vmin:.3f} - {sentence_vmax:.3f}")
    print(f"Paragraphæ•°æ®èŒƒå›´: {paragraph_vmin:.3f} - {paragraph_vmax:.3f}")

    # ä½¿ç”¨æ›´æ•æ„Ÿçš„é¢œè‰²æ˜ å°„
    cmap = 'RdYlBu_r'

    # ç»˜åˆ¶Sentenceçƒ­åŠ›å›¾
    sns.heatmap(sentence_heatmap,
                annot=True,
                fmt='.3f',
                cmap=cmap,
                vmin=sentence_vmin, vmax=sentence_vmax,
                cbar=True,
                ax=ax1,
                square=False,
                linewidths=0.5,
                linecolor='white',
                cbar_kws={'shrink': 0.8, 'label': 'Performance Score'})

    # ç»˜åˆ¶Paragraphçƒ­åŠ›å›¾
    sns.heatmap(paragraph_heatmap,
                annot=True,
                fmt='.3f',
                cmap=cmap,
                vmin=paragraph_vmin, vmax=paragraph_vmax,
                cbar=True,
                ax=ax2,
                square=False,
                linewidths=0.5,
                linecolor='white',
                cbar_kws={'shrink': 0.8, 'label': 'Performance Score'})

    # è®¾ç½®è‹±æ–‡æ ‡é¢˜å’Œæ ‡ç­¾
    ax1.set_title('(a) Sentence-level Knowledge Synthesizer',
                  fontsize=14, fontweight='bold', pad=20)
    ax2.set_title('(b) Paragraph-level Knowledge Synthesizer',
                  fontsize=14, fontweight='bold', pad=20)

    # è®¾ç½®è‹±æ–‡è½´æ ‡ç­¾
    ax1.set_xlabel('Dataset and Metric', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Weight (w)', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Dataset and Metric', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Weight (w)', fontsize=12, fontweight='bold')

    # æ—‹è½¬xè½´æ ‡ç­¾
    ax1.tick_params(axis='x', rotation=45, labelsize=10)
    ax2.tick_params(axis='x', rotation=45, labelsize=10)
    ax1.tick_params(axis='y', rotation=0, labelsize=10)
    ax2.tick_params(axis='y', rotation=0, labelsize=10)

    # è®¾ç½®è‹±æ–‡æ€»æ ‡é¢˜
    fig.suptitle('Weight Parameter Optimization Results Across Different Knowledge Synthesizer Strategies',
                 fontsize=16, fontweight='bold', y=0.98)

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    plt.subplots_adjust(top=0.85)

    # ä¿å­˜å›¾ç‰‡
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight',
                    facecolor='white', edgecolor='none')
        print(f"å¢å¼ºç‰ˆçƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {save_path}")

    plt.show()

    return sentence_heatmap, paragraph_heatmap


def find_optimal_balanced_weight(sentence_data, paragraph_data):
    """
    æ‰¾åˆ°æœ€ä¼˜å¹³è¡¡æƒé‡
    """
    metrics_columns = ['HotPotQA-EM', 'HotPotQA-F1', '2WikiMultiHopQA-EM',
                       '2WikiMultiHopQA-F1', 'MuSiQue-EM', 'MuSiQue-F1']

    print("=== è¯¦ç»†çš„æƒé‡åˆ†æ ===")

    overall_scores = {}
    for weight in sentence_data['weight'].unique():
        # è·å–è¯¥æƒé‡ä¸‹çš„æ‰€æœ‰æŒ‡æ ‡
        sent_scores = sentence_data[sentence_data['weight'] == weight][metrics_columns].iloc[0].values
        para_scores = paragraph_data[paragraph_data['weight'] == weight][metrics_columns].iloc[0].values

        # è®¡ç®—å¹³å‡æ€§èƒ½
        sent_avg = np.mean(sent_scores)
        para_avg = np.mean(para_scores)
        overall_avg = (sent_avg + para_avg) / 2

        # è®¡ç®—ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®ï¼‰
        all_scores = np.concatenate([sent_scores, para_scores])
        stability = np.std(all_scores)

        overall_scores[weight] = {
            'sentence_avg': sent_avg,
            'paragraph_avg': para_avg,
            'overall_avg': overall_avg,
            'stability': stability,
            'balance_score': overall_avg - 0.1 * stability
        }

        print(f"w={weight:.1f}: Sentenceå¹³å‡={sent_avg:.3f}, Paragraphå¹³å‡={para_avg:.3f}, "
              f"æ•´ä½“å¹³å‡={overall_avg:.3f}, ç¨³å®šæ€§={stability:.3f}")

    # æ‰¾åˆ°æœ€ä¼˜æƒé‡
    best_weight = max(overall_scores.keys(), key=lambda x: overall_scores[x]['balance_score'])

    print(f"\nğŸ¯ **æ¨èçš„æœ€ä¼˜æƒé‡: w={best_weight}**")
    print(f"   æ•´ä½“å¹³å‡æ€§èƒ½: {overall_scores[best_weight]['overall_avg']:.3f}")
    print(f"   æ€§èƒ½ç¨³å®šæ€§: {overall_scores[best_weight]['stability']:.3f}")

    return best_weight, overall_scores


def main():
    excel_path = '/home/jiangjp/trace-idea/results/myidea/totaldev100.xlsx'
    save_path = '/home/jiangjp/trace-idea/results/myidea/weight_dev100_heatmaps.png'

    try:
        # åˆ›å»ºå¢å¼ºç‰ˆçƒ­åŠ›å›¾
        sentence_heatmap, paragraph_heatmap = create_enhanced_heatmaps(excel_path, save_path)

        # åŠ è½½æ•°æ®è¿›è¡Œåˆ†æ
        sentence_data = pd.read_excel(excel_path, sheet_name='Sentence')
        paragraph_data = pd.read_excel(excel_path, sheet_name='Paragraph')

        # æ‰¾åˆ°æœ€ä¼˜æƒé‡
        optimal_weight, analysis = find_optimal_balanced_weight(sentence_data, paragraph_data)

        print(f"\n=== è®ºæ–‡æè¿°å»ºè®® ===")
        print(f"é€šè¿‡å¯¹ä¸åŒæƒé‡å‚æ•°çš„ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°w={optimal_weight}èƒ½å¤Ÿåœ¨")
        print("Sentence-levelå’ŒParagraph-levelä¸¤ç§çŸ¥è¯†æ•´åˆç­–ç•¥ä¸‹éƒ½å–å¾—è‰¯å¥½çš„å¹³è¡¡ï¼Œ")
        print("å› æ­¤é€‰æ‹©è¯¥æƒé‡ä½œä¸ºæœ€ç»ˆçš„å‚æ•°è®¾ç½®ã€‚")

    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")


if __name__ == "__main__":
    main()