import json
import time
import concurrent.futures
from threading import Lock
from zhipuai import ZhipuAI
from typing import List, Dict, Any, Tuple
import re


class OptimizedTruthfulScoreEvaluator:
    def __init__(self, api_key: str, max_workers: int = 10):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆçœŸå®æ€§è¯„åˆ†å™¨

        Args:
            api_key: ZhipuAIçš„APIå¯†é’¥
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        """
        self.client = ZhipuAI(api_key=api_key)
        self.max_workers = max_workers
        self.progress_lock = Lock()
        self.text_completed_count = 0
        self.triple_completed_count = 0
        self.text_total_count = 0
        self.triple_total_count = 0

        # é»˜è®¤åˆ†æ•°æ ‡è¯†
        self.DEFAULT_SCORE = 12

        # æœ€å°é…ç½®å€¼
        self.MIN_WORKERS = 1
        self.MIN_BATCH_SIZE = 1

        # çŸ¥è¯†ä¸‰å…ƒç»„è¯„ä¼°çš„æŒ‡ä»¤æ¨¡æ¿
        self.triple_instruction = """Your task is to evaluate the authenticity of knowledge triplets based on your internal knowledge, reasoning, and inference. The structure of a knowledge triplet is âŸ¨ head; relation; tailâŸ©ï¼Œ Represents a single factual statement about the relationship between entities. I will provide a knowledge triad that may contain accurate information or fictional errors. You need to assign it a credibility score from 0 to 10, with higher scores indicating higher authenticity and lower scores indicating lower authenticity. Here are 2 examples, you should follow the output format below:
##########
Triple:
head: Albert Einstein
relation: was the first recipient in 1921 of
tail: the Nobel Prize in Physics

Analysis:
Head Accuracy: Albert Einstein is correct. Einstein was a real historical figure and a renowned physicist.
Tail Accuracy: the Nobel Prize in Physics is valid. This award exists and has been granted since 1901.
Relation Errors: Albert Einstein was not the first Nobel laureate in Physics. The inaugural prize was awarded in 1901 to Wilhelm Conrad Roentgen for his discovery of X-rays. Einstein did receive the Nobel Prize, but it was awarded retrospectively in 1922 (for the year 1921) and recognized his work on the photoelectric effect, not relativity.

Credibility Score: 0


Triple:
head: Wilhelm Conrad Roentgen
relation: was the first recipient in 1901 of
tail: the Nobel Prize in Physics

Analysis:
Head Accuracy: Wilhelm Conrad Roentgen is correct. Roentgen was a German physicist who discovered X-rays.
Tail Accuracy: the Nobel Prize in Physics is factual and well-documented.
Relation Accuracy: Roentgen was indeed the first laureate in Physics, as confirmed by Nobel Prize archives. The inaugural award was correctly granted in 1901. The prize honored his discovery of X-rays, which revolutionized medicine and physics.

Credibility Score: 10
##########
"""

        # æ–‡æœ¬æ®µè½è¯„ä¼°çš„æŒ‡ä»¤æ¨¡æ¿
        self.text_instruction = """Your task is to evaluate the authenticity of a text based on your internal knowledge. Specifically, I will provide you with a passage that may contain accurate information or fabricated errors. Using your own knowledge, reason, and deduction, you are to assign a credibility score ranging from 0 to 10, where a higher score indicates greater authenticity and a lower score suggests lesser authenticity. 
Here are 2 examples, you should follow the output format below:
##########
Passage:
In a groundbreaking discovery, researchers have found that Albert Einstein was the first recipient of the Nobel Prize in Physics. According to newly uncovered documents, Einstein's pioneering work in theoretical physics, particularly his theory of relativity, was recognized by the Nobel Committee in 1921. This revelation challenges the long-held belief that Marie Curie was the first Nobel laureate in physics, and solidifies Einstein's place as one of the greatest minds in scientific history.

Analysis:
1. Albert Einstein as the First Nobel Prize Recipient in Physics: This is incorrect. The first Nobel Prize in Physics was awarded in 1901, not to Albert Einstein, but to Wilhelm Conrad RÃ¶ntgen for the discovery of X-rays.
2. Einstein's Nobel Prize Recognition: Albert Einstein was indeed awarded the Nobel Prize in Physics in 1921, but not for his theory of relativity. He received it for his discovery of the photoelectric effect, which was instrumental in the development of quantum theory.
3. Marie Curie as the First Nobel Laureate in Physics: This is also incorrect. Marie Curie was a Nobel laureate, but she was not the first to win the Nobel Prize in Physics. Her first Nobel Prize was in Physics in 1903, shared with her husband Pierre Curie and Henri Becquerel for their work on radioactivity. Marie Curie was, notably, the first woman to win a Nobel Prize, and the first person to win Nobel Prizes in two different scientific fields (Physics and Chemistry).
4. Implication about the Nobel Committee's Recognition of Relativity: As mentioned, Einstein's Nobel Prize was not for relativity, despite its profound impact on physics. The Nobel Committee specifically avoided awarding the prize for relativity at the time due to ongoing debates and lack of experimental confirmation of the theory during that period.

Credibility Score: 0


Passage:
The first Nobel Prize in Physics was awarded to Wilhelm Conrad Roentgen in 1901. Roentgen received the Nobel Prize for his discovery of X-rays, which had a significant impact on the field of physics and medicine

Analysis:
The facts presented in the statement you provided are largely accurate.

Credibility Score: 10
##########
"""

    def extract_credibility_score(self, text: str) -> int:
        """
        ä»GPTå“åº”ä¸­æå–å¯ä¿¡åº¦åˆ†æ•°

        Args:
            text: GPTçš„å®Œæ•´å“åº”æ–‡æœ¬

        Returns:
            æå–å‡ºçš„å¯ä¿¡åº¦åˆ†æ•°ï¼ˆæ•´æ•°ï¼‰
        """
        score_index = text.rfind("Credibility Score:")
        if score_index != -1:
            score_text = text[score_index + len("Credibility Score:"):].strip()
            score = ''.join(filter(str.isdigit, score_text.split()[0] if score_text.split() else ''))
            return int(score) if score.isdigit() else 0
        return 0

    def call_api_with_retry(self, user_input: str, instruction: str, max_retries: int = 3) -> str:
        """
        è°ƒç”¨APIå¹¶é‡è¯•çš„é€šç”¨æ–¹æ³•

        Args:
            user_input: ç”¨æˆ·è¾“å…¥å†…å®¹
            instruction: ç³»ç»ŸæŒ‡ä»¤
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            APIå“åº”å†…å®¹
        """
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model="glm-4-flash",
                    messages=[
                        {"role": "system", "content": instruction},
                        {"role": "user", "content": user_input},
                    ],
                    stream=True,
                )

                full_response_content = ""
                for chunk in response:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        full_response_content += delta.content

                return full_response_content

            except Exception as e:
                print(f"APIè°ƒç”¨ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    print(f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥")
                    return ""

    def extract_multiple_credibility_scores_with_retry(self, text: str, num_items: int,
                                                       original_inputs: List[str],
                                                       instruction: str,
                                                       max_extract_retries: int = 3) -> List[int]:
        """
        ä»æ‰¹é‡å¤„ç†çš„GPTå“åº”ä¸­æå–å¤šä¸ªå¯ä¿¡åº¦åˆ†æ•°ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶

        Args:
            text: GPTçš„å®Œæ•´å“åº”æ–‡æœ¬
            num_items: é¢„æœŸçš„é¡¹ç›®æ•°é‡
            original_inputs: åŸå§‹è¾“å…¥åˆ—è¡¨ï¼ˆç”¨äºé‡è¯•ï¼‰
            instruction: ç³»ç»ŸæŒ‡ä»¤ï¼ˆç”¨äºé‡è¯•ï¼‰
            max_extract_retries: æå–é‡è¯•æ¬¡æ•°

        Returns:
            æå–å‡ºçš„å¯ä¿¡åº¦åˆ†æ•°åˆ—è¡¨
        """
        scores = []

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ "Credibility Score: X" æ¨¡å¼
        pattern = r"Credibility Score:\s*(\d+)"
        matches = re.findall(pattern, text, re.IGNORECASE)

        for match in matches:
            score = int(match) if match.isdigit() and 0 <= int(match) <= 10 else 0
            scores.append(score)

        # å¦‚æœæ‰¾åˆ°çš„åˆ†æ•°æ•°é‡æ­£ç¡®ï¼Œç›´æ¥è¿”å›
        if len(scores) == num_items:
            print(f"æˆåŠŸæå–åˆ° {len(scores)} ä¸ªåˆ†æ•°")
            return scores

        print(f"è­¦å‘Šï¼šæœŸæœ› {num_items} ä¸ªåˆ†æ•°ï¼Œä½†åªæ‰¾åˆ° {len(scores)} ä¸ªï¼Œå¼€å§‹é‡è¯•...")

        # é‡è¯•æœºåˆ¶
        for retry_attempt in range(max_extract_retries):
            print(f"ç¬¬ {retry_attempt + 1} æ¬¡é‡è¯•APIè°ƒç”¨...")

            # é‡æ–°æ„å»ºç”¨æˆ·è¾“å…¥
            if instruction == self.text_instruction:
                # æ–‡æœ¬å¤„ç†çš„é‡è¯•
                user_input_template = """Passage:
{text}

Credibility Score: 
"""
                user_input_list = []
                for input_text in original_inputs:
                    user_input_list.append(user_input_template.format(text=input_text))
                user_input = "\n".join(user_input_list)
            else:
                # ä¸‰å…ƒç»„å¤„ç†çš„é‡è¯•
                user_input_template = """Triple:
head: {head}
relation: {relation}
tail: {tail}

Credibility Score: 
"""
                user_input_list = []
                for triple in original_inputs:
                    user_input_list.append(user_input_template.format(
                        head=triple['head'],
                        relation=triple['relation'],
                        tail=triple['tail']
                    ))
                user_input = "\n".join(user_input_list)

            # é‡æ–°è°ƒç”¨API
            retry_response = self.call_api_with_retry(user_input, instruction)

            if retry_response:
                print(f"é‡è¯•ç¬¬ {retry_attempt + 1} æ¬¡çš„å“åº”ï¼š")

                # é‡æ–°æå–åˆ†æ•°
                retry_matches = re.findall(pattern, retry_response, re.IGNORECASE)
                retry_scores = []
                for match in retry_matches:
                    score = int(match) if match.isdigit() and 0 <= int(match) <= 10 else 0
                    retry_scores.append(score)

                if len(retry_scores) == num_items:
                    print(f"é‡è¯•æˆåŠŸï¼æå–åˆ° {len(retry_scores)} ä¸ªåˆ†æ•°")
                    return retry_scores
                else:
                    print(f"é‡è¯•ç¬¬ {retry_attempt + 1} æ¬¡ä»ç„¶ä¸åŒ¹é…ï¼šæœŸæœ› {num_items} ä¸ªï¼Œå¾—åˆ° {len(retry_scores)} ä¸ª")
            else:
                print(f"é‡è¯•ç¬¬ {retry_attempt + 1} æ¬¡APIè°ƒç”¨å¤±è´¥")

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
        print(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•° {self.DEFAULT_SCORE}")
        return [self.DEFAULT_SCORE] * num_items

    def get_batch_text_scores_with_retry(self, texts: List[str], max_retries: int = 3) -> List[int]:
        """
        æ‰¹é‡è·å–æ–‡æœ¬æ®µè½çš„çœŸå®æ€§åˆ†æ•°ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        """
        for attempt in range(max_retries):
            try:
                user_input_template = """Passage:
{text}

Credibility Score: 
"""

                user_input_list = []
                for text in texts:
                    user_input_list.append(user_input_template.format(text=text))

                user_input = "\n".join(user_input_list)

                full_response_content = self.call_api_with_retry(user_input, self.text_instruction)

                if full_response_content:
                    return self.extract_multiple_credibility_scores_with_retry(
                        full_response_content,
                        len(texts),
                        texts,  # ä¼ å…¥åŸå§‹æ–‡æœ¬åˆ—è¡¨
                        self.text_instruction
                    )
                else:
                    print(f"ç¬¬ {attempt + 1} æ¬¡æ‰¹é‡æ–‡æœ¬è¯„ä¼°æ”¶åˆ°ç©ºå“åº”")

            except Exception as e:
                print(f"ç¬¬ {attempt + 1} æ¬¡æ‰¹é‡æ–‡æœ¬è¯„ä¼°å°è¯•å¤±è´¥: {e}")

            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)

        print(f"æ‰¹é‡æ–‡æœ¬è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•° {self.DEFAULT_SCORE}")
        return [self.DEFAULT_SCORE] * len(texts)

    def get_batch_triple_scores_with_retry(self, triples: List[Dict], max_retries: int = 3) -> List[int]:
        """
        æ‰¹é‡è·å–çŸ¥è¯†ä¸‰å…ƒç»„çš„çœŸå®æ€§åˆ†æ•°ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        """
        for attempt in range(max_retries):
            try:
                user_input_template = """Triple:
head: {head}
relation: {relation}
tail: {tail}

Credibility Score: 
"""

                user_input_list = []
                for triple in triples:
                    user_input_list.append(user_input_template.format(
                        head=triple['head'],
                        relation=triple['relation'],
                        tail=triple['tail']
                    ))

                user_input = "\n".join(user_input_list)

                full_response_content = self.call_api_with_retry(user_input, self.triple_instruction)

                if full_response_content:
                    return self.extract_multiple_credibility_scores_with_retry(
                        full_response_content,
                        len(triples),
                        triples,  # ä¼ å…¥åŸå§‹ä¸‰å…ƒç»„åˆ—è¡¨
                        self.triple_instruction
                    )
                else:
                    print(f"ç¬¬ {attempt + 1} æ¬¡æ‰¹é‡ä¸‰å…ƒç»„è¯„ä¼°æ”¶åˆ°ç©ºå“åº”")

            except Exception as e:
                print(f"ç¬¬ {attempt + 1} æ¬¡æ‰¹é‡ä¸‰å…ƒç»„è¯„ä¼°å°è¯•å¤±è´¥: {e}")

            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)

        print(f"æ‰¹é‡ä¸‰å…ƒç»„è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•° {self.DEFAULT_SCORE}")
        return [self.DEFAULT_SCORE] * len(triples)

    def process_batch_ctx_texts(self, batch_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†batch_sizeä¸ªctxä¸­çš„æ‰€æœ‰textï¼ˆæ‰¹é‡ï¼‰
        """
        ctx_list = batch_data['ctx_list']
        batch_idx = batch_data['batch_idx']

        try:
            # æ”¶é›†æ‰€æœ‰çš„texts
            all_texts = []
            text_mapping = []  # è®°å½•æ¯ä¸ªtextå±äºå“ªä¸ªitemå’Œctx

            for ctx_info in ctx_list:
                item_idx = ctx_info['item_idx']
                ctx_idx = ctx_info['ctx_idx']
                text = ctx_info['text']

                all_texts.append(text)
                text_mapping.append({
                    'item_idx': item_idx,
                    'ctx_idx': ctx_idx
                })

            if not all_texts:
                return {
                    'batch_idx': batch_idx,
                    'scores': [],
                    'text_mapping': [],
                    'success': True
                }

            # æ‰¹é‡è·å–åˆ†æ•°
            scores = self.get_batch_text_scores_with_retry(all_texts)

            with self.progress_lock:
                self.text_completed_count += 1
                total_ctx_count = len(ctx_list)
                print(f"æ–‡æœ¬æ‰¹é‡è¯„ä¼°è¿›åº¦: {self.text_completed_count}/{self.text_total_count} "
                      f"(æ‰¹æ¬¡ {batch_idx + 1}, {total_ctx_count} ä¸ªctx) - å¤„ç†äº† {len(all_texts)} ä¸ªæ–‡æœ¬")

            return {
                'batch_idx': batch_idx,
                'scores': scores,
                'text_mapping': text_mapping,
                'success': True
            }

        except Exception as e:
            print(f"å¤„ç†æ‰¹æ¬¡ctxä¸­çš„æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'batch_idx': batch_idx,
                'scores': [],
                'text_mapping': [],
                'success': False
            }

    def process_batch_ctx_triples(self, batch_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†batch_sizeä¸ªctxä¸­çš„æ‰€æœ‰triplesï¼ˆæ‰¹é‡ï¼‰
        """
        ctx_list = batch_data['ctx_list']
        batch_idx = batch_data['batch_idx']

        try:
            # æ”¶é›†æ‰€æœ‰çš„triples
            all_triples = []
            triple_mapping = []  # è®°å½•æ¯ä¸ªtripleå±äºå“ªä¸ªitemå’Œctx

            for ctx_info in ctx_list:
                item_idx = ctx_info['item_idx']
                ctx_idx = ctx_info['ctx_idx']
                triples = ctx_info['triples']

                for triple_idx, triple in enumerate(triples):
                    if all(key in triple for key in ['head', 'relation', 'tail']):
                        all_triples.append(triple)
                        triple_mapping.append({
                            'item_idx': item_idx,
                            'ctx_idx': ctx_idx,
                            'triple_idx': triple_idx
                        })

            if not all_triples:
                return {
                    'batch_idx': batch_idx,
                    'scores': [],
                    'triple_mapping': [],
                    'success': True
                }

            # æ‰¹é‡è·å–åˆ†æ•°
            scores = self.get_batch_triple_scores_with_retry(all_triples)

            with self.progress_lock:
                self.triple_completed_count += 1
                total_ctx_count = len(ctx_list)
                print(f"ä¸‰å…ƒç»„æ‰¹é‡è¯„ä¼°è¿›åº¦: {self.triple_completed_count}/{self.triple_total_count} "
                      f"(æ‰¹æ¬¡ {batch_idx + 1}, {total_ctx_count} ä¸ªctx) - å¤„ç†äº† {len(all_triples)} ä¸ªä¸‰å…ƒç»„")

            return {
                'batch_idx': batch_idx,
                'scores': scores,
                'triple_mapping': triple_mapping,
                'success': True
            }

        except Exception as e:
            print(f"å¤„ç†æ‰¹æ¬¡ctxä¸­çš„ä¸‰å…ƒç»„æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'batch_idx': batch_idx,
                'scores': [],
                'triple_mapping': [],
                'success': False
            }

    def collect_text_batches(self, dataset: List[Dict], batch_size: int = 5) -> List[Dict[str, Any]]:
        """
        æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„ctxæ•°æ®å¹¶åˆ†æ‰¹ï¼ˆç”¨äºæ–‡æœ¬æ‰¹é‡å¤„ç†ï¼‰
        """
        all_ctx_data = []

        # å…ˆæ”¶é›†æ‰€æœ‰æœ‰textçš„ctx
        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx_idx, ctx in enumerate(item['ctxs']):
                    if 'text' in ctx:
                        all_ctx_data.append({
                            'item_idx': item_idx,
                            'ctx_idx': ctx_idx,
                            'text': ctx['text']
                        })

        # åˆ†æ‰¹å¤„ç†
        batches = []
        for i in range(0, len(all_ctx_data), batch_size):
            batch = all_ctx_data[i:i + batch_size]
            batches.append({
                'ctx_list': batch,
                'batch_idx': i // batch_size
            })

        return batches

    def collect_ctx_batches(self, dataset: List[Dict], batch_size: int = 5) -> List[Dict[str, Any]]:
        """
        æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„ctxæ•°æ®å¹¶åˆ†æ‰¹ï¼ˆç”¨äºä¸‰å…ƒç»„æ‰¹é‡å¤„ç†ï¼‰
        """
        all_ctx_data = []

        # å…ˆæ”¶é›†æ‰€æœ‰æœ‰triplesçš„ctx
        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx_idx, ctx in enumerate(item['ctxs']):
                    if 'triples' in ctx and isinstance(ctx['triples'], list) and len(ctx['triples']) > 0:
                        all_ctx_data.append({
                            'item_idx': item_idx,
                            'ctx_idx': ctx_idx,
                            'triples': ctx['triples']
                        })

        # åˆ†æ‰¹å¤„ç†
        batches = []
        for i in range(0, len(all_ctx_data), batch_size):
            batch = all_ctx_data[i:i + batch_size]
            batches.append({
                'ctx_list': batch,
                'batch_idx': i // batch_size
            })

        return batches

    def apply_text_results(self, dataset: List[Dict], results: List[Dict]):
        """
        å°†æ–‡æœ¬è¯„ä¼°ç»“æœåº”ç”¨åˆ°æ•°æ®é›†
        """
        for result in results:
            if result['success']:
                scores = result['scores']
                text_mapping = result['text_mapping']

                try:
                    for score, mapping in zip(scores, text_mapping):
                        item_idx = mapping['item_idx']
                        ctx_idx = mapping['ctx_idx']
                        dataset[item_idx]['ctxs'][ctx_idx]['text_truthful_score'] = score
                except (IndexError, KeyError) as e:
                    print(f"åº”ç”¨æ–‡æœ¬ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def apply_triple_results(self, dataset: List[Dict], results: List[Dict]):
        """
        å°†ä¸‰å…ƒç»„è¯„ä¼°ç»“æœåº”ç”¨åˆ°æ•°æ®é›†
        """
        for result in results:
            if result['success']:
                scores = result['scores']
                triple_mapping = result['triple_mapping']

                try:
                    for score, mapping in zip(scores, triple_mapping):
                        item_idx = mapping['item_idx']
                        ctx_idx = mapping['ctx_idx']
                        triple_idx = mapping['triple_idx']
                        dataset[item_idx]['ctxs'][ctx_idx]['triples'][triple_idx]['triple_truthful_score'] = score
                except (IndexError, KeyError) as e:
                    print(f"åº”ç”¨ä¸‰å…ƒç»„ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def save_progress(self, dataset: List[Dict], output_file: str, stage: str):
        """
        ä¿å­˜ä¸­é—´è¿›åº¦
        """
        try:
            temp_file = f"{output_file}.{stage}.tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            print(f"{stage}é˜¶æ®µè¿›åº¦å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_file}")
        except Exception as e:
            print(f"ä¿å­˜{stage}é˜¶æ®µè¿›åº¦å¤±è´¥: {e}")

    def check_default_scores(self, dataset: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰é»˜è®¤åˆ†æ•°ï¼Œå¹¶æå–å‡ºæ¥

        Args:
            dataset: æ•°æ®é›†

        Returns:
            (failed_texts, failed_triples): åŒ…å«é»˜è®¤åˆ†æ•°çš„æ–‡æœ¬å’Œä¸‰å…ƒç»„æ•°æ®
        """
        failed_texts = []
        failed_triples = []

        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx_idx, ctx in enumerate(item['ctxs']):
                    # æ£€æŸ¥æ–‡æœ¬åˆ†æ•°
                    if 'text_truthful_score' in ctx and ctx['text_truthful_score'] == self.DEFAULT_SCORE:
                        failed_texts.append({
                            'item_idx': item_idx,
                            'ctx_idx': ctx_idx,
                            'text': ctx['text']
                        })

                    # æ£€æŸ¥ä¸‰å…ƒç»„åˆ†æ•°
                    if 'triples' in ctx and isinstance(ctx['triples'], list):
                        failed_ctx_triples = []
                        for triple_idx, triple in enumerate(ctx['triples']):
                            if 'triple_truthful_score' in triple and triple[
                                'triple_truthful_score'] == self.DEFAULT_SCORE:
                                failed_ctx_triples.append(triple)

                        if failed_ctx_triples:
                            failed_triples.append({
                                'item_idx': item_idx,
                                'ctx_idx': ctx_idx,
                                'triples': failed_ctx_triples
                            })

        return failed_texts, failed_triples

    def count_default_scores(self, dataset: List[Dict]) -> Tuple[int, int]:
        """
        ç»Ÿè®¡é»˜è®¤åˆ†æ•°çš„æ•°é‡

        Args:
            dataset: æ•°æ®é›†

        Returns:
            (text_default_count, triple_default_count): é»˜è®¤åˆ†æ•°çš„æ•°é‡
        """
        text_default_count = 0
        triple_default_count = 0

        for item in dataset:
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx in item['ctxs']:
                    # ç»Ÿè®¡æ–‡æœ¬é»˜è®¤åˆ†æ•°
                    if 'text_truthful_score' in ctx and ctx['text_truthful_score'] == self.DEFAULT_SCORE:
                        text_default_count += 1

                    # ç»Ÿè®¡ä¸‰å…ƒç»„é»˜è®¤åˆ†æ•°
                    if 'triples' in ctx and isinstance(ctx['triples'], list):
                        for triple in ctx['triples']:
                            if 'triple_truthful_score' in triple and triple[
                                'triple_truthful_score'] == self.DEFAULT_SCORE:
                                triple_default_count += 1

        return text_default_count, triple_default_count

    def check_missing_score_fields(self, dataset: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®ï¼Œå¹¶æå–å‡ºæ¥

        Args:
            dataset: æ•°æ®é›†

        Returns:
            (missing_text_scores, missing_triple_scores): ç¼ºå¤±åˆ†æ•°å­—æ®µçš„æ–‡æœ¬å’Œä¸‰å…ƒç»„æ•°æ®
        """
        missing_text_scores = []
        missing_triple_scores = []

        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx_idx, ctx in enumerate(item['ctxs']):
                    # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ç¼ºå¤±åˆ†æ•°å­—æ®µ
                    if 'text' in ctx and 'text_truthful_score' not in ctx:
                        missing_text_scores.append({
                            'item_idx': item_idx,
                            'ctx_idx': ctx_idx,
                            'text': ctx['text']
                        })

                    # æ£€æŸ¥ä¸‰å…ƒç»„æ˜¯å¦ç¼ºå¤±åˆ†æ•°å­—æ®µ
                    if 'triples' in ctx and isinstance(ctx['triples'], list):
                        missing_ctx_triples = []
                        for triple_idx, triple in enumerate(ctx['triples']):
                            if all(key in triple for key in
                                   ['head', 'relation', 'tail']) and 'triple_truthful_score' not in triple:
                                missing_ctx_triples.append(triple)

                        if missing_ctx_triples:
                            missing_triple_scores.append({
                                'item_idx': item_idx,
                                'ctx_idx': ctx_idx,
                                'triples': missing_ctx_triples
                            })

        return missing_text_scores, missing_triple_scores

    def count_missing_score_fields(self, dataset: List[Dict]) -> Tuple[int, int]:
        """
        ç»Ÿè®¡ç¼ºå¤±åˆ†æ•°å­—æ®µçš„æ•°é‡

        Args:
            dataset: æ•°æ®é›†

        Returns:
            (missing_text_count, missing_triple_count): ç¼ºå¤±åˆ†æ•°å­—æ®µçš„æ•°é‡
        """
        missing_text_count = 0
        missing_triple_count = 0

        for item in dataset:
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx in item['ctxs']:
                    # ç»Ÿè®¡ç¼ºå¤±æ–‡æœ¬åˆ†æ•°å­—æ®µ
                    if 'text' in ctx and 'text_truthful_score' not in ctx:
                        missing_text_count += 1

                    # ç»Ÿè®¡ç¼ºå¤±ä¸‰å…ƒç»„åˆ†æ•°å­—æ®µ
                    if 'triples' in ctx and isinstance(ctx['triples'], list):
                        for triple in ctx['triples']:
                            if all(key in triple for key in
                                   ['head', 'relation', 'tail']) and 'triple_truthful_score' not in triple:
                                missing_triple_count += 1

        return missing_text_count, missing_triple_count

    def process_missing_score_fields_with_adaptive_config(self, dataset: List[Dict], output_file: str,
                                                          initial_workers: int, initial_text_batch: int,
                                                          initial_triple_batch: int):
        """
        å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®ï¼Œå¹¶è‡ªé€‚åº”è°ƒæ•´é…ç½®å‚æ•°

        Args:
            dataset: æ•°æ®é›†
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            initial_workers: åˆå§‹å¹¶å‘æ•°
            initial_text_batch: åˆå§‹æ–‡æœ¬æ‰¹æ¬¡å¤§å°
            initial_triple_batch: åˆå§‹ä¸‰å…ƒç»„æ‰¹æ¬¡å¤§å°
        """
        current_workers = initial_workers
        current_text_batch = initial_text_batch
        current_triple_batch = initial_triple_batch

        print(f"ğŸ” å¼€å§‹å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®...")
        print(
            f"å½“å‰é…ç½® - å¹¶å‘æ•°: {current_workers}, æ–‡æœ¬æ‰¹æ¬¡: {current_text_batch}, ä¸‰å…ƒç»„æ‰¹æ¬¡: {current_triple_batch}")

        # æ£€æŸ¥ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®
        missing_texts, missing_triples = self.check_missing_score_fields(dataset)
        text_count, triple_count = self.count_missing_score_fields(dataset)

        print(f"å‘ç°ç¼ºå¤±åˆ†æ•°å­—æ®µ - æ–‡æœ¬: {text_count} ä¸ª, ä¸‰å…ƒç»„: {triple_count} ä¸ª")

        if text_count == 0 and triple_count == 0:
            print("âœ… æ•°æ®é›†ä¸­æ²¡æœ‰ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®ï¼Œæ— éœ€å¤„ç†")
            return

        # å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„æ–‡æœ¬
        if missing_texts:
            print(f"\nå¼€å§‹å¤„ç† {len(missing_texts)} ä¸ªç¼ºå¤±åˆ†æ•°å­—æ®µçš„æ–‡æœ¬...")
            self.process_missing_texts(dataset, missing_texts, current_workers, current_text_batch)

        # å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„ä¸‰å…ƒç»„
        if missing_triples:
            print(f"\nå¼€å§‹å¤„ç† {len(missing_triples)} ä¸ªåŒ…å«ç¼ºå¤±åˆ†æ•°å­—æ®µä¸‰å…ƒç»„çš„ctx...")
            self.process_missing_triples(dataset, missing_triples, current_workers, current_triple_batch)

        # ä¿å­˜å¤„ç†è¿›åº¦
        self.save_progress(dataset, output_file, "missing_fields_processed")

        # æœ€ç»ˆæ£€æŸ¥
        final_text_count, final_triple_count = self.count_missing_score_fields(dataset)
        print(f"ç¼ºå¤±åˆ†æ•°å­—æ®µå¤„ç†å®Œæˆ - å‰©ä½™ç¼ºå¤±: æ–‡æœ¬ {final_text_count} ä¸ª, ä¸‰å…ƒç»„ {final_triple_count} ä¸ª")

    def process_missing_texts(self, dataset: List[Dict], missing_texts: List[Dict], workers: int, batch_size: int):
        """
        å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„æ–‡æœ¬
        """
        print(f"ä½¿ç”¨é…ç½®å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„æ–‡æœ¬ - å¹¶å‘æ•°: {workers}, æ‰¹æ¬¡å¤§å°: {batch_size}")

        # åˆ†æ‰¹å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„æ–‡æœ¬
        batches = []
        for i in range(0, len(missing_texts), batch_size):
            batch = missing_texts[i:i + batch_size]
            batches.append({
                'ctx_list': batch,
                'batch_idx': i // batch_size
            })

        self.text_total_count = len(batches)
        self.text_completed_count = 0

        if self.text_total_count > 0:
            text_results = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_batch = {
                    executor.submit(self.process_batch_ctx_texts, batch): batch
                    for batch in batches
                }

                for future in concurrent.futures.as_completed(future_to_batch):
                    result = future.result()
                    text_results.append(result)

            # åº”ç”¨æ–‡æœ¬ç»“æœ
            self.apply_text_results(dataset, text_results)

            success_count = sum(1 for r in text_results if r['success'])
            print(f"ç¼ºå¤±åˆ†æ•°å­—æ®µæ–‡æœ¬å¤„ç†å®Œæˆ: {success_count}/{len(text_results)} æˆåŠŸ")

    def process_missing_triples(self, dataset: List[Dict], missing_triples: List[Dict], workers: int, batch_size: int):
        """
        å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„ä¸‰å…ƒç»„
        """
        print(f"ä½¿ç”¨é…ç½®å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„ä¸‰å…ƒç»„ - å¹¶å‘æ•°: {workers}, æ‰¹æ¬¡å¤§å°: {batch_size}")

        # åˆ†æ‰¹å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„ä¸‰å…ƒç»„
        batches = []
        for i in range(0, len(missing_triples), batch_size):
            batch = missing_triples[i:i + batch_size]
            batches.append({
                'ctx_list': batch,
                'batch_idx': i // batch_size
            })

        self.triple_total_count = len(batches)
        self.triple_completed_count = 0

        if self.triple_total_count > 0:
            triple_results = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_batch = {
                    executor.submit(self.process_batch_ctx_triples, batch): batch
                    for batch in batches
                }

                for future in concurrent.futures.as_completed(future_to_batch):
                    result = future.result()
                    triple_results.append(result)

            # åº”ç”¨ä¸‰å…ƒç»„ç»“æœ
            self.apply_triple_results(dataset, triple_results)

            success_count = sum(1 for r in triple_results if r['success'])
            print(f"ç¼ºå¤±åˆ†æ•°å­—æ®µä¸‰å…ƒç»„å¤„ç†å®Œæˆ: {success_count}/{len(triple_results)} æˆåŠŸ")

    def process_failed_items_with_adaptive_config(self, dataset: List[Dict], output_file: str,
                                                  initial_workers: int, initial_text_batch: int,
                                                  initial_triple_batch: int):
        """
        å¤„ç†å¤±è´¥çš„é¡¹ç›®ï¼Œå¹¶è‡ªé€‚åº”è°ƒæ•´é…ç½®å‚æ•°

        Args:
            dataset: æ•°æ®é›†
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            initial_workers: åˆå§‹å¹¶å‘æ•°
            initial_text_batch: åˆå§‹æ–‡æœ¬æ‰¹æ¬¡å¤§å°
            initial_triple_batch: åˆå§‹ä¸‰å…ƒç»„æ‰¹æ¬¡å¤§å°
        """
        current_workers = initial_workers
        current_text_batch = initial_text_batch
        current_triple_batch = initial_triple_batch
        retry_round = 1

        while True:
            print(f"\n{'=' * 80}")
            print(f"ç¬¬ {retry_round} è½®é‡è¯•æ£€æŸ¥å’Œå¤„ç†")
            print(f"{'=' * 80}")

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é»˜è®¤åˆ†æ•°
            failed_texts, failed_triples = self.check_default_scores(dataset)
            text_count, triple_count = self.count_default_scores(dataset)

            print(f"å‘ç°é»˜è®¤åˆ†æ•° - æ–‡æœ¬: {text_count} ä¸ª, ä¸‰å…ƒç»„: {triple_count} ä¸ª")

            if text_count == 0 and triple_count == 0:
                print("ğŸ‰ æ‰€æœ‰é¡¹ç›®éƒ½å·²æˆåŠŸå¤„ç†ï¼Œæ²¡æœ‰é»˜è®¤åˆ†æ•°ï¼")
                break

            print(
                f"å½“å‰é…ç½® - å¹¶å‘æ•°: {current_workers}, æ–‡æœ¬æ‰¹æ¬¡: {current_text_batch}, ä¸‰å…ƒç»„æ‰¹æ¬¡: {current_triple_batch}")

            # å¤„ç†å¤±è´¥çš„æ–‡æœ¬
            if failed_texts:
                print(f"\nå¼€å§‹å¤„ç† {len(failed_texts)} ä¸ªå¤±è´¥çš„æ–‡æœ¬...")
                self.process_failed_texts(dataset, failed_texts, current_workers, current_text_batch)

            # å¤„ç†å¤±è´¥çš„ä¸‰å…ƒç»„
            if failed_triples:
                print(f"\nå¼€å§‹å¤„ç† {len(failed_triples)} ä¸ªåŒ…å«å¤±è´¥ä¸‰å…ƒç»„çš„ctx...")
                self.process_failed_triples(dataset, failed_triples, current_workers, current_triple_batch)

            # ä¿å­˜å½“å‰è¿›åº¦
            self.save_progress(dataset, output_file, f"retry_round_{retry_round}")

            # æ£€æŸ¥å¤„ç†ç»“æœ
            new_text_count, new_triple_count = self.count_default_scores(dataset)
            print(f"æœ¬è½®å¤„ç†å - æ–‡æœ¬é»˜è®¤åˆ†æ•°: {new_text_count} ä¸ª, ä¸‰å…ƒç»„é»˜è®¤åˆ†æ•°: {new_triple_count} ä¸ª")

            # å¦‚æœè¿˜æœ‰å¤±è´¥çš„ï¼Œè°ƒæ•´é…ç½®
            if new_text_count > 0 or new_triple_count > 0:
                current_workers, current_text_batch, current_triple_batch = self.adjust_config(
                    current_workers, current_text_batch, current_triple_batch)
                print(
                    f"è°ƒæ•´åé…ç½® - å¹¶å‘æ•°: {current_workers}, æ–‡æœ¬æ‰¹æ¬¡: {current_text_batch}, ä¸‰å…ƒç»„æ‰¹æ¬¡: {current_triple_batch}")

            retry_round += 1

            # é˜²æ­¢æ— é™å¾ªç¯
            if retry_round > 1:
                print("âš ï¸ å·²è¾¾åˆ°æœ€å¤§é‡è¯•è½®æ¬¡ï¼Œåœæ­¢é‡è¯•")
                break

    def adjust_config(self, workers: int, text_batch: int, triple_batch: int) -> Tuple[int, int, int]:
        """
        è°ƒæ•´é…ç½®å‚æ•°ï¼Œå…ˆè°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼Œå†è°ƒæ•´å¹¶å‘æ•°

        Args:
            workers: å½“å‰å¹¶å‘æ•°
            text_batch: å½“å‰æ–‡æœ¬æ‰¹æ¬¡å¤§å°
            triple_batch: å½“å‰ä¸‰å…ƒç»„æ‰¹æ¬¡å¤§å°

        Returns:
            è°ƒæ•´åçš„é…ç½®
        """
        # å…ˆå°è¯•å‡å°æ‰¹æ¬¡å¤§å°
        new_text_batch = max(self.MIN_BATCH_SIZE, text_batch - 1)
        new_triple_batch = max(self.MIN_BATCH_SIZE, triple_batch - 1)

        # å¦‚æœæ‰¹æ¬¡å¤§å°å·²ç»æ˜¯æœ€å°å€¼ï¼Œå°è¯•å‡å°å¹¶å‘æ•°
        if new_text_batch == self.MIN_BATCH_SIZE and new_triple_batch == self.MIN_BATCH_SIZE:
            new_workers = max(self.MIN_WORKERS, workers - 1)
        else:
            new_workers = workers

        print(
            f"é…ç½®è°ƒæ•´: å¹¶å‘æ•° {workers}->{new_workers}, æ–‡æœ¬æ‰¹æ¬¡ {text_batch}->{new_text_batch}, ä¸‰å…ƒç»„æ‰¹æ¬¡ {triple_batch}->{new_triple_batch}")
        return new_workers, new_text_batch, new_triple_batch

    def process_failed_texts(self, dataset: List[Dict], failed_texts: List[Dict], workers: int, batch_size: int):
        """
        å¤„ç†å¤±è´¥çš„æ–‡æœ¬
        """
        print(f"ä½¿ç”¨é…ç½®å¤„ç†æ–‡æœ¬ - å¹¶å‘æ•°: {workers}, æ‰¹æ¬¡å¤§å°: {batch_size}")

        # åˆ†æ‰¹å¤„ç†å¤±è´¥çš„æ–‡æœ¬
        batches = []
        for i in range(0, len(failed_texts), batch_size):
            batch = failed_texts[i:i + batch_size]
            batches.append({
                'ctx_list': batch,
                'batch_idx': i // batch_size
            })

        self.text_total_count = len(batches)
        self.text_completed_count = 0

        if self.text_total_count > 0:
            text_results = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_batch = {
                    executor.submit(self.process_batch_ctx_texts, batch): batch
                    for batch in batches
                }

                for future in concurrent.futures.as_completed(future_to_batch):
                    result = future.result()
                    text_results.append(result)

            # åº”ç”¨æ–‡æœ¬ç»“æœ
            self.apply_text_results(dataset, text_results)

            success_count = sum(1 for r in text_results if r['success'])
            print(f"å¤±è´¥æ–‡æœ¬é‡å¤„ç†å®Œæˆ: {success_count}/{len(text_results)} æˆåŠŸ")

    def process_failed_triples(self, dataset: List[Dict], failed_triples: List[Dict], workers: int, batch_size: int):
        """
        å¤„ç†å¤±è´¥çš„ä¸‰å…ƒç»„
        """
        print(f"ä½¿ç”¨é…ç½®å¤„ç†ä¸‰å…ƒç»„ - å¹¶å‘æ•°: {workers}, æ‰¹æ¬¡å¤§å°: {batch_size}")

        # åˆ†æ‰¹å¤„ç†å¤±è´¥çš„ä¸‰å…ƒç»„
        batches = []
        for i in range(0, len(failed_triples), batch_size):
            batch = failed_triples[i:i + batch_size]
            batches.append({
                'ctx_list': batch,
                'batch_idx': i // batch_size
            })

        self.triple_total_count = len(batches)
        self.triple_completed_count = 0

        if self.triple_total_count > 0:
            triple_results = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_batch = {
                    executor.submit(self.process_batch_ctx_triples, batch): batch
                    for batch in batches
                }

                for future in concurrent.futures.as_completed(future_to_batch):
                    result = future.result()
                    triple_results.append(result)

            # åº”ç”¨ä¸‰å…ƒç»„ç»“æœ
            self.apply_triple_results(dataset, triple_results)

            success_count = sum(1 for r in triple_results if r['success'])
            print(f"å¤±è´¥ä¸‰å…ƒç»„é‡å¤„ç†å®Œæˆ: {success_count}/{len(triple_results)} æˆåŠŸ")

    def check_default_scores_with_indices(self, dataset: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰é»˜è®¤åˆ†æ•°ï¼Œå¹¶æå–å‡ºæ¥ï¼ˆåŒ…å«å®Œæ•´ç´¢å¼•ä¿¡æ¯ï¼‰

        Args:
            dataset: æ•°æ®é›†

        Returns:
            (failed_texts, failed_triples): åŒ…å«é»˜è®¤åˆ†æ•°çš„æ–‡æœ¬å’Œä¸‰å…ƒç»„æ•°æ®ï¼Œä¸‰å…ƒç»„åŒ…å«åŸå§‹ç´¢å¼•
        """
        failed_texts = []
        failed_triples = []

        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx_idx, ctx in enumerate(item['ctxs']):
                    # æ£€æŸ¥æ–‡æœ¬åˆ†æ•°
                    if 'text_truthful_score' in ctx and ctx['text_truthful_score'] == self.DEFAULT_SCORE:
                        failed_texts.append({
                            'item_idx': item_idx,
                            'ctx_idx': ctx_idx,
                            'text': ctx['text']
                        })

                    # æ£€æŸ¥ä¸‰å…ƒç»„åˆ†æ•°ï¼ˆä¿ç•™åŸå§‹ç´¢å¼•ï¼‰
                    if 'triples' in ctx and isinstance(ctx['triples'], list):
                        failed_ctx_triples = []
                        for original_triple_idx, triple in enumerate(ctx['triples']):
                            if 'triple_truthful_score' in triple and triple[
                                'triple_truthful_score'] == self.DEFAULT_SCORE:
                                failed_ctx_triples.append({
                                    'original_idx': original_triple_idx,  # ä¿å­˜åŸå§‹ç´¢å¼•
                                    'triple': triple
                                })

                        if failed_ctx_triples:
                            failed_triples.append({
                                'item_idx': item_idx,
                                'ctx_idx': ctx_idx,
                                'triples_with_indices': failed_ctx_triples  # æ–°çš„å­—æ®µå
                            })

        return failed_texts, failed_triples

    def process_individual_triples_for_failed_items(self, dataset: List[Dict],
                                                    triples_per_call: int = 3):
        """
        å¯¹å¤±è´¥é¡¹ç›®è¿›è¡Œå•ä¸ªä¸‰å…ƒç»„å¤„ç†ï¼ŒæŒ‰æŒ‡å®šæ•°é‡è°ƒç”¨API

        Args:
            dataset: æ•°æ®é›†
            triples_per_call: æ¯æ¬¡APIè°ƒç”¨å¤„ç†çš„ä¸‰å…ƒç»„æ•°é‡
        """
        print(f"\nğŸ”§ å¼€å§‹æŒ‰å•ä¸ªä¸‰å…ƒç»„æ–¹å¼å¤„ç†å¤±è´¥é¡¹ç›®...")
        print(f"æ¯æ¬¡APIè°ƒç”¨å¤„ç† {triples_per_call} ä¸ªä¸‰å…ƒç»„")

        # æ£€æŸ¥é»˜è®¤åˆ†æ•°çš„ä¸‰å…ƒç»„
        failed_texts, failed_triples = self.check_default_scores_with_indices(dataset)
        text_count, triple_count = self.count_default_scores(dataset)

        print(f"å‘ç°é»˜è®¤åˆ†æ•° - æ–‡æœ¬: {text_count} ä¸ª, ä¸‰å…ƒç»„: {triple_count} ä¸ª")

        if triple_count == 0:
            print("âœ… æ²¡æœ‰éœ€è¦å¤„ç†çš„é»˜è®¤åˆ†æ•°ä¸‰å…ƒç»„")
            return

        # å±•å¼€æ‰€æœ‰éœ€è¦å¤„ç†çš„ä¸‰å…ƒç»„
        all_failed_triples = []
        for ctx_info in failed_triples:
            item_idx = ctx_info['item_idx']
            ctx_idx = ctx_info['ctx_idx']
            triples_with_indices = ctx_info['triples_with_indices']
            for triple_with_indice in triples_with_indices:
                triple_idx=triple_with_indice['original_idx']
                triple=triple_with_indice['triple']
                if 'triple_truthful_score' in triple and triple['triple_truthful_score'] == self.DEFAULT_SCORE:
                    all_failed_triples.append({
                        'item_idx': item_idx,
                        'ctx_idx': ctx_idx,
                        'triple_idx': triple_idx,
                        'triple': triple
                    })

        print(f"æ€»å…±éœ€è¦é‡æ–°å¤„ç† {len(all_failed_triples)} ä¸ªä¸‰å…ƒç»„")

        # æŒ‰æŒ‡å®šæ•°é‡åˆ†ç»„å¤„ç†
        processed_count = 0
        total_groups = (len(all_failed_triples) + triples_per_call - 1) // triples_per_call

        for i in range(0, len(all_failed_triples), triples_per_call):
            group = all_failed_triples[i:i + triples_per_call]
            group_idx = i // triples_per_call + 1

            print(f"æ­£åœ¨å¤„ç†ç¬¬ {group_idx}/{total_groups} ç»„ ({len(group)} ä¸ªä¸‰å…ƒç»„)...")

            # æå–ä¸‰å…ƒç»„æ•°æ®
            triples_data = [item['triple'] for item in group]

            # è°ƒç”¨APIè·å–åˆ†æ•°
            scores = self.get_batch_triple_scores_with_retry(triples_data)

            # å°†åˆ†æ•°èµ‹å€¼å›æ•°æ®é›†
            for j, (score, item_info) in enumerate(zip(scores, group)):
                try:
                    item_idx = item_info['item_idx']
                    ctx_idx = item_info['ctx_idx']
                    triple_idx = item_info['triple_idx']

                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                    if (item_idx < len(dataset) and
                            ctx_idx < len(dataset[item_idx]['ctxs']) and
                            triple_idx < len(dataset[item_idx]['ctxs'][ctx_idx]['triples'])):

                        dataset[item_idx]['ctxs'][ctx_idx]['triples'][triple_idx]['triple_truthful_score'] = score
                        processed_count += 1

                        print(f"  - ä¸‰å…ƒç»„ {j + 1}: {item_info['triple']['head']} -> åˆ†æ•°: {score}")
                    else:
                        print(f"  - âš ï¸ ä¸‰å…ƒç»„ {j + 1}: ç´¢å¼•æ— æ•ˆï¼Œè·³è¿‡")

                except Exception as e:
                    print(f"  - âŒ ä¸‰å…ƒç»„ {j + 1}: å¤„ç†å¤±è´¥ - {e}")

        print(f"\nâœ… å•ä¸ªä¸‰å…ƒç»„å¤„ç†å®Œæˆï¼å…±å¤„ç† {processed_count} ä¸ªä¸‰å…ƒç»„")

        # æ£€æŸ¥å¤„ç†ç»“æœ
        final_text_count, final_triple_count = self.count_default_scores(dataset)
        print(f"å¤„ç†åå‰©ä½™é»˜è®¤åˆ†æ•° - æ–‡æœ¬: {final_text_count} ä¸ª, ä¸‰å…ƒç»„: {final_triple_count} ä¸ª")

    def process_dataset_optimized(self, input_file: str, output_file: str, text_batch_size: int = 5,
                                  triple_batch_size: int = 5, triples_per_call: int = 3,
                                  retry_only: bool = False,
                                  missing_fields_only: bool = False,
                                  individual_processing: bool = False):
        """
        ä¼˜åŒ–ç‰ˆæ•°æ®é›†å¤„ç†ï¼šå…ˆå¤„ç†æ–‡æœ¬ï¼Œå†å¤„ç†ä¸‰å…ƒç»„

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            text_batch_size: æ–‡æœ¬å¤„ç†çš„æ‰¹æ¬¡å¤§å°
            triple_batch_size: ä¸‰å…ƒç»„å¤„ç†çš„æ‰¹æ¬¡å¤§å°
            retry_only: æ˜¯å¦ä»…æ‰§è¡Œé‡è¯•å¤±è´¥é¡¹ç›®å¤„ç†ï¼ˆè·³è¿‡åˆå§‹å¤„ç†ï¼‰
            missing_fields_only: æ˜¯å¦ä»…æ‰§è¡Œç¼ºå¤±åˆ†æ•°å­—æ®µå¤„ç†ï¼ˆè·³è¿‡æ‰€æœ‰å…¶ä»–å¤„ç†ï¼‰
        """
        print(f"å¼€å§‹å¤„ç†æ•°æ®é›†: {input_file}")
        if missing_fields_only:
            print("ğŸ” å¯ç”¨ä»…ç¼ºå¤±å­—æ®µå¤„ç†æ¨¡å¼ï¼šè·³è¿‡æ‰€æœ‰å…¶ä»–å¤„ç†ï¼Œä»…å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®")
        elif retry_only:
            print("âš ï¸ å¯ç”¨ä»…é‡è¯•æ¨¡å¼ï¼šè·³è¿‡åˆå§‹å¤„ç†ï¼Œç›´æ¥å¤„ç†é»˜è®¤åˆ†æ•°ä¸º12çš„å¤±è´¥é¡¹ç›®")
        else:
            print("ğŸ“ æ‰§è¡Œå®Œæ•´å¤„ç†ï¼šåŒ…å«åˆå§‹å¤„ç†ã€é‡è¯•å¤„ç†å’Œç¼ºå¤±å­—æ®µå¤„ç†")

        # è¯»å–è¾“å…¥æ–‡ä»¶
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
        except Exception as e:
            print(f"è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return

        if not isinstance(dataset, list):
            dataset = [dataset]
        if individual_processing:
            print("ğŸ” å¯ç”¨å•ä¸ªä¸‰å…ƒç»„å¤„ç†æ¨¡å¼")
            self.process_individual_triples_for_failed_items(dataset, triples_per_call)
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"å¯ç”¨å•ä¸ªä¸‰å…ƒç»„å¤„ç†æ¨¡å¼å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            except Exception as e:
                print(f"ä¿å­˜è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
                return
            return

        # å¦‚æœæ˜¯ä»…ç¼ºå¤±å­—æ®µå¤„ç†æ¨¡å¼ï¼Œç›´æ¥è·³è½¬åˆ°ç¬¬å››é˜¶æ®µ
        if missing_fields_only:
            print("\n" + "=" * 60)
            print("ç›´æ¥æ‰§è¡Œï¼šå¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®")
            print("=" * 60)

            # å…ˆæ£€æŸ¥å½“å‰æ•°æ®é›†ä¸­çš„ç¼ºå¤±åˆ†æ•°å­—æ®µæƒ…å†µ
            initial_text_count, initial_triple_count = self.count_missing_score_fields(dataset)
            print(f"ğŸ“Š å½“å‰æ•°æ®é›†ä¸­ç¼ºå¤±åˆ†æ•°å­—æ®µç»Ÿè®¡ - æ–‡æœ¬: {initial_text_count} ä¸ª, ä¸‰å…ƒç»„: {initial_triple_count} ä¸ª")

            if initial_text_count == 0 and initial_triple_count == 0:
                print("âœ… æ•°æ®é›†ä¸­æ²¡æœ‰ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®ï¼Œæ— éœ€å¤„ç†")
            else:
                self.process_missing_score_fields_with_adaptive_config(
                    dataset, output_file, self.max_workers, text_batch_size, triple_batch_size)

            # ä¿å­˜æœ€ç»ˆç»“æœ
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"âœ… ç¼ºå¤±å­—æ®µå¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

                # æœ€ç»ˆç»Ÿè®¡
                text_count, triple_count = self.count_missing_score_fields(dataset)
                print(f"ğŸ æœ€ç»ˆç»Ÿè®¡ - å‰©ä½™ç¼ºå¤±åˆ†æ•°å­—æ®µ: æ–‡æœ¬ {text_count} ä¸ª, ä¸‰å…ƒç»„ {triple_count} ä¸ª")

            except Exception as e:
                print(f"ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

            return

        # å¦‚æœä¸æ˜¯ä»…é‡è¯•æ¨¡å¼ï¼Œæ‰§è¡Œå®Œæ•´çš„åˆå§‹å¤„ç†
        if not retry_only:
            print("=" * 60)
            print(f"ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†æ–‡æœ¬æ•°æ®ï¼ˆå¤šçº¿ç¨‹+æ¯ä¸ªçº¿ç¨‹å¤„ç†{text_batch_size}ä¸ªctxçš„textï¼‰")
            print("=" * 60)

            # ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†æ–‡æœ¬æ•°æ®
            text_batches = self.collect_text_batches(dataset, batch_size=text_batch_size)
            self.text_total_count = len(text_batches)
            self.text_completed_count = 0

            print(f"æ€»å…±éœ€è¦å¤„ç† {self.text_total_count} ä¸ªæ‰¹æ¬¡ï¼ˆæ¯ä¸ªæ‰¹æ¬¡åŒ…å«æœ€å¤š{text_batch_size}ä¸ªctxçš„æ–‡æœ¬ï¼‰")

            if self.text_total_count > 0:
                text_results = []
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_batch = {
                        executor.submit(self.process_batch_ctx_texts, batch): batch
                        for batch in text_batches
                    }

                    for future in concurrent.futures.as_completed(future_to_batch):
                        result = future.result()
                        text_results.append(result)

                # åº”ç”¨æ–‡æœ¬ç»“æœ
                self.apply_text_results(dataset, text_results)

                # ä¿å­˜æ–‡æœ¬å¤„ç†è¿›åº¦
                self.save_progress(dataset, output_file, "text")

                success_count = sum(1 for r in text_results if r['success'])
                print(f"æ–‡æœ¬å¤„ç†å®Œæˆ: {success_count}/{len(text_results)} æˆåŠŸ")

            print("\n" + "=" * 60)
            print(f"ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†ä¸‰å…ƒç»„æ•°æ®ï¼ˆå¤šçº¿ç¨‹+æ¯ä¸ªçº¿ç¨‹å¤„ç†{triple_batch_size}ä¸ªctxçš„æ‰€æœ‰triplesï¼‰")
            print("=" * 60)

            # ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†ä¸‰å…ƒç»„æ•°æ®
            ctx_batches = self.collect_ctx_batches(dataset, batch_size=triple_batch_size)
            self.triple_total_count = len(ctx_batches)
            self.triple_completed_count = 0

            print(
                f"æ€»å…±éœ€è¦å¤„ç† {self.triple_total_count} ä¸ªæ‰¹æ¬¡ï¼ˆæ¯ä¸ªæ‰¹æ¬¡åŒ…å«æœ€å¤š{triple_batch_size}ä¸ªctxçš„æ‰€æœ‰ä¸‰å…ƒç»„ï¼‰")

            if self.triple_total_count > 0:
                triple_results = []
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_batch = {
                        executor.submit(self.process_batch_ctx_triples, batch): batch
                        for batch in ctx_batches
                    }

                    for future in concurrent.futures.as_completed(future_to_batch):
                        result = future.result()
                        triple_results.append(result)

                # åº”ç”¨ä¸‰å…ƒç»„ç»“æœ
                self.apply_triple_results(dataset, triple_results)

                success_count = sum(1 for r in triple_results if r['success'])
                print(f"ä¸‰å…ƒç»„å¤„ç†å®Œæˆ: {success_count}/{len(triple_results)} æˆåŠŸ")

            # ä¿å­˜åˆå§‹å¤„ç†ç»“æœ
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"åˆå§‹å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                import os
                for stage in ['text']:
                    temp_file = f"{output_file}.{stage}.tmp"
                    if os.path.exists(temp_file):
                        os.remove(temp_file)

            except Exception as e:
                print(f"ä¿å­˜è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
                return

        # ç¬¬ä¸‰é˜¶æ®µï¼šè‡ªé€‚åº”é‡è¯•å¤„ç†å¤±è´¥é¡¹ç›®ï¼ˆæ— è®ºæ˜¯å¦ä»…é‡è¯•æ¨¡å¼ï¼Œéƒ½ä¼šæ‰§è¡Œï¼‰
        print("\n" + "=" * 60)
        if retry_only:
            print("ç›´æ¥æ‰§è¡Œï¼šé‡è¯•å¤„ç†é»˜è®¤åˆ†æ•°ä¸º12çš„å¤±è´¥é¡¹ç›®")
        else:
            print("ç¬¬ä¸‰é˜¶æ®µï¼šè‡ªé€‚åº”é‡è¯•å¤„ç†å¤±è´¥é¡¹ç›®")
        print("=" * 60)

        # å…ˆæ£€æŸ¥å½“å‰æ•°æ®é›†ä¸­çš„é»˜è®¤åˆ†æ•°æƒ…å†µ
        initial_text_count, initial_triple_count = self.count_default_scores(dataset)
        print(f"ğŸ“Š å½“å‰æ•°æ®é›†ä¸­é»˜è®¤åˆ†æ•°ç»Ÿè®¡ - æ–‡æœ¬: {initial_text_count} ä¸ª, ä¸‰å…ƒç»„: {initial_triple_count} ä¸ª")

        if initial_text_count == 0 and initial_triple_count == 0:
            print("âœ… æ•°æ®é›†ä¸­æ²¡æœ‰é»˜è®¤åˆ†æ•°ï¼Œæ— éœ€é‡è¯•å¤„ç†")
        else:
            self.process_failed_items_with_adaptive_config(
                dataset, output_file, self.max_workers, text_batch_size, triple_batch_size)

        # ç¬¬å››é˜¶æ®µï¼šå¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®
        print("\n" + "=" * 60)
        print("ç¬¬å››é˜¶æ®µï¼šå¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®")
        print("=" * 60)

        # å…ˆæ£€æŸ¥å½“å‰æ•°æ®é›†ä¸­çš„ç¼ºå¤±åˆ†æ•°å­—æ®µæƒ…å†µ
        missing_text_count, missing_triple_count = self.count_missing_score_fields(dataset)
        print(f"ğŸ“Š å½“å‰æ•°æ®é›†ä¸­ç¼ºå¤±åˆ†æ•°å­—æ®µç»Ÿè®¡ - æ–‡æœ¬: {missing_text_count} ä¸ª, ä¸‰å…ƒç»„: {missing_triple_count} ä¸ª")

        if missing_text_count == 0 and missing_triple_count == 0:
            print("âœ… æ•°æ®é›†ä¸­æ²¡æœ‰ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®ï¼Œæ— éœ€å¤„ç†")
        else:
            self.process_missing_score_fields_with_adaptive_config(
                dataset, output_file, self.max_workers, text_batch_size, triple_batch_size)

        # ä¿å­˜æœ€ç»ˆç»“æœ
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            print(f"âœ… æœ€ç»ˆå¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            # åˆ é™¤é‡è¯•é˜¶æ®µçš„ä¸´æ—¶æ–‡ä»¶
            import os
            for i in range(1, 11):
                temp_file = f"{output_file}.retry_round_{i}.tmp"
                if os.path.exists(temp_file):
                    os.remove(temp_file)

            # åˆ é™¤ç¼ºå¤±å­—æ®µå¤„ç†çš„ä¸´æ—¶æ–‡ä»¶
            temp_file = f"{output_file}.missing_fields_processed.tmp"
            if os.path.exists(temp_file):
                os.remove(temp_file)

            # æœ€ç»ˆç»Ÿè®¡
            default_text_count, default_triple_count = self.count_default_scores(dataset)
            missing_text_count, missing_triple_count = self.count_missing_score_fields(dataset)
            print(f"ğŸ æœ€ç»ˆç»Ÿè®¡:")
            print(f"   - å‰©ä½™é»˜è®¤åˆ†æ•°: æ–‡æœ¬ {default_text_count} ä¸ª, ä¸‰å…ƒç»„ {default_triple_count} ä¸ª")
            print(f"   - å‰©ä½™ç¼ºå¤±åˆ†æ•°å­—æ®µ: æ–‡æœ¬ {missing_text_count} ä¸ª, ä¸‰å…ƒç»„ {missing_triple_count} ä¸ª")

        except Exception as e:
            print(f"ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")


def main():
    """
    ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹
    """
    # é…ç½®å‚æ•°
    API_KEY = ""  # è¯·å¡«å†™æ‚¨çš„ZhipuAI API Key
    INPUT_FILE = "wiki_test1000_add_ctxs.json"
    OUTPUT_FILE = "wiki_test1000_add_truthful_scores_with_kgs.json"

    # å¹¶è¡Œå¤„ç†å‚æ•°
    MAX_WORKERS = 3000  # å¹¶å‘çº¿ç¨‹æ•°ï¼Œæ ¹æ®APIé™åˆ¶è°ƒæ•´
    TEXT_BATCH_SIZE = 2  # æ–‡æœ¬å¤„ç†æ‰¹æ¬¡å¤§å°
    TRIPLE_BATCH_SIZE = 2  # ä¸‰å…ƒç»„å¤„ç†æ‰¹æ¬¡å¤§å°

    # â­ æ§åˆ¶å‚æ•°ï¼šé€‰æ‹©æ‰§è¡Œæ¨¡å¼
    RETRY_ONLY = True  # è®¾ç½®ä¸ºTrueè¡¨ç¤ºä»…å¤„ç†é»˜è®¤åˆ†æ•°ä¸º12çš„å¤±è´¥é¡¹ç›®
    MISSING_FIELDS_ONLY = False  # è®¾ç½®ä¸ºTrueè¡¨ç¤ºä»…å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®

    # ğŸ†• æ–°å¢æ§åˆ¶å‚æ•°ï¼šå•ä¸ªå¤„ç†æ¨¡å¼
    INDIVIDUAL_PROCESSING = True  # è®¾ç½®ä¸ºTrueè¡¨ç¤ºä½¿ç”¨å•ä¸ªä¸‰å…ƒç»„/æ–‡æœ¬å¤„ç†æ¨¡å¼
    TRIPLES_PER_CALL = 1  # æ¯æ¬¡APIè°ƒç”¨å¤„ç†çš„ä¸‰å…ƒç»„æ•°é‡
    # æ³¨æ„ï¼šå¦‚æœMISSING_FIELDS_ONLY=Trueï¼Œåˆ™RETRY_ONLYçš„å€¼ä¼šè¢«å¿½ç•¥
    # ä¸‰ç§æ¨¡å¼ï¼š
    # 1. MISSING_FIELDS_ONLY=True: ä»…å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µ
    # 2. RETRY_ONLY=True, MISSING_FIELDS_ONLY=False: ä»…å¤„ç†é»˜è®¤åˆ†æ•°12çš„é¡¹ç›®
    # 3. ä¸¤è€…éƒ½ä¸ºFalse: æ‰§è¡Œå®Œæ•´æµç¨‹

    if not API_KEY:
        print("é”™è¯¯ï¼šè¯·å…ˆè®¾ç½®æ‚¨çš„ZhipuAI API Key")
        return

    # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
    evaluator = OptimizedTruthfulScoreEvaluator(API_KEY, max_workers=MAX_WORKERS)

    # æ ¹æ®å‚æ•°æ‰§è¡Œä¸åŒçš„å¤„ç†æµç¨‹
    if MISSING_FIELDS_ONLY:
        print("ğŸ” å¯ç”¨ä»…ç¼ºå¤±å­—æ®µå¤„ç†æ¨¡å¼")
        print(f"ğŸ“‚ å°†ä»æ–‡ä»¶ {INPUT_FILE} ä¸­è¯»å–æ•°æ®ï¼Œä»…å¤„ç†ç¼ºå¤±åˆ†æ•°å­—æ®µçš„é¡¹ç›®")
    elif RETRY_ONLY:
        if INDIVIDUAL_PROCESSING:
            print("ğŸ”„ å¯ç”¨å•ä¸ªä¸‰å…ƒç»„å¤„ç†æ¨¡å¼")
        print("ğŸ”„ å¯ç”¨ä»…é‡è¯•æ¨¡å¼")
        print(f"ğŸ“‚ å°†ä»æ–‡ä»¶ {INPUT_FILE} ä¸­è¯»å–æ•°æ®ï¼Œä»…å¤„ç†é»˜è®¤åˆ†æ•°ä¸º12çš„é¡¹ç›®")
    else:
        print("ğŸš€ å¯ç”¨å®Œæ•´å¤„ç†æ¨¡å¼")
        print(f"ğŸ“‚ å°†å®Œæ•´å¤„ç†æ–‡ä»¶ {INPUT_FILE} ä¸­çš„æ‰€æœ‰æ•°æ®")

    # ä¼˜åŒ–å¤„ç†æ•°æ®é›†
    evaluator.process_dataset_optimized(
        INPUT_FILE,
        OUTPUT_FILE,
        text_batch_size=TEXT_BATCH_SIZE,
        triple_batch_size=TRIPLE_BATCH_SIZE,
        retry_only=RETRY_ONLY,
        missing_fields_only=MISSING_FIELDS_ONLY,  # ä¼ å…¥æ–°å‚æ•°
        individual_processing=INDIVIDUAL_PROCESSING,  # ä¼ å…¥æ–°å‚æ•°
        triples_per_call=TRIPLES_PER_CALL,  # ä¼ å…¥æ–°å‚æ•°
    )


if __name__ == "__main__":
    main()
