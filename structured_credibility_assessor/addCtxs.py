import json
import time
import concurrent.futures
from threading import Lock
from zhipuai import ZhipuAI
from retrievers.e5_mistral import get_e5_mistral_embeddings_for_query, get_e5_mistral_embeddings_for_document
import torch
from typing import List, Dict, Any, Tuple


class OptimizedTitleGenerator:
    def __init__(self, api_key: str, max_workers: int = 10):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆæ ‡é¢˜ç”Ÿæˆå™¨

        Args:
            api_key: ZhipuAIçš„APIå¯†é’¥
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        """
        self.client = ZhipuAI(api_key=api_key)
        self.max_workers = max_workers
        self.progress_lock = Lock()
        self.completed_count = 0
        self.total_count = 0

        # é»˜è®¤æ ‡è¯†
        self.DEFAULT_TITLE = "DEFAULT_TITLE_PLACEHOLDER"
        self.EMPTY_TITLE = ""

        # æœ€å°é…ç½®å€¼
        self.MIN_WORKERS = 1

        # ä¿æŒåŸå§‹çš„æç¤ºè¯æ¨¡æ¿
        self.instruction = """Your task is to generate a single concise title for the given English paragraph. The generated title should be less than 10 words.
Here are 2 examples, you should follow the output format below:
##########
Passage:
Boston College (also referred to as BC) is a private Jesuit Catholic research university located in the affluent village of Chestnut Hill, Massachusetts, United States, 6 mi west of downtown Boston. It has 9,100 full-time undergraduates and almost 5,000 graduate students. The university's name reflects its early history as a liberal arts college and preparatory school (now Boston College High School) in Dorchester. It is a member of the 568 Group and the Association of Jesuit Colleges and Universities. Its main campus is a historic district and features some of the earliest examples of collegiate gothic architecture in North America.

Title: Boston College



Passage:
The Rideau River Residence Association (RRRA) is the student organization that represents undergraduate students living in residence at Carleton University. It was founded in 1968 as the Carleton University Residence Association. Following a protracted fight with the university in the mid-1970s, it was renamed in its present form. It is a non-profit corporation that serves as Canada's oldest and largest residence association. Its membership consists of roughly 3,600 undergraduate students enrolled at the university living in residence. With an annual budget of approximately $1.4 million and three executives alongside volunteer staff, RRRA serves as an advocate for residence students and provides a variety of services, events, and programs to its members.

Title: Rideau River Residence Association
##########
"""

        self.user_input_template = """Passage: {passage}
Title: 
"""

    def get_dataset_demonstrations(self, dataset):
        """è·å–æ•°æ®é›†æ¼”ç¤ºæ ·ä¾‹"""
        if dataset == "hotpotqa":
            from prompts import generate_knowledge_triples_hotpotqa_examplars
            demonstrations = generate_knowledge_triples_hotpotqa_examplars
        elif dataset == "2wikimultihopqa":
            from prompts import generate_knowledge_triples_2wikimultihopqa_examplars
            demonstrations = generate_knowledge_triples_2wikimultihopqa_examplars
        elif dataset == "musique":
            from prompts import generate_knowledge_triples_musique_examplars
            demonstrations = generate_knowledge_triples_musique_examplars
        else:
            raise ValueError(f"{dataset} is not a supported dataset!")
        return demonstrations

    def split_sentences(self, text):
        """æ ¹æ®å¥å·æ‹†åˆ†å¥å­ï¼Œä¿ç•™å¥å·"""
        parts = text.split('.')
        sentences = []

        for i, part in enumerate(parts):
            part = part.strip()
            if part:
                if i < len(parts) - 1:
                    sentences.append(part + '.')
                else:
                    if text.endswith('.'):
                        sentences.append(part + '.')
                    else:
                        sentences.append(part)
        return sentences

    def generate_title_single(self, passage: str) -> str:
        """
        ä¸ºå•ä¸ªæ®µè½ç”Ÿæˆæ ‡é¢˜

        Args:
            passage: æ®µè½æ–‡æœ¬

        Returns:
            ç”Ÿæˆçš„æ ‡é¢˜
        """
        try:
            user_input = self.user_input_template.format(passage=passage)
            response = self.client.chat.completions.create(
                model="glm-4-flash",
                messages=[
                    {"role": "system", "content": self.instruction},
                    {"role": "user", "content": user_input},
                ],
                stream=True,
            )

            full_response_content = ""
            for chunk in response:
                delta = chunk.choices[0].delta
                if delta.content:
                    full_response_content += delta.content

            return full_response_content.strip()

        except Exception as e:
            print(f"ç”Ÿæˆæ ‡é¢˜å¤±è´¥: {str(e)}")
            return self.DEFAULT_TITLE

    def call_api_with_retry(self, passage: str, max_retries: int = 3) -> str:
        """
        è°ƒç”¨APIå¹¶é‡è¯•çš„æ–¹æ³•

        Args:
            passage: æ®µè½æ–‡æœ¬
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            ç”Ÿæˆçš„æ ‡é¢˜
        """
        for attempt in range(max_retries):
            try:
                result = self.generate_title_single(passage)
                if result != self.DEFAULT_TITLE and result.strip():
                    return result
                else:
                    print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¾—åˆ°ç©ºæˆ–é»˜è®¤ç»“æœ")
            except Exception as e:
                print(f"APIè°ƒç”¨ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)

        print(f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼")
        return self.DEFAULT_TITLE

    def generate_titles_only(self, passage_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»…ç”Ÿæˆæ ‡é¢˜çš„å•çº¿ç¨‹å¤„ç†å‡½æ•°

        Args:
            passage_data: åŒ…å«æ®µè½ä¿¡æ¯çš„å­—å…¸

        Returns:
            åŒ…å«æ ‡é¢˜çš„ç»“æœå­—å…¸
        """
        try:
            item_idx = passage_data['item_idx']
            paragraph_idx = passage_data['paragraph_idx']
            paragraph = passage_data['paragraph']

            # ç”Ÿæˆæ ‡é¢˜
            title = self.call_api_with_retry(paragraph)

            with self.progress_lock:
                self.completed_count += 1
                print(
                    f"æ ‡é¢˜ç”Ÿæˆè¿›åº¦: {self.completed_count}/{self.total_count} - é¡¹ç›® {item_idx + 1}, æ®µè½ {paragraph_idx + 1}")

            return {
                'item_idx': item_idx,
                'paragraph_idx': paragraph_idx,
                'paragraph': paragraph,
                'title': title,
                'success': True
            }

        except Exception as e:
            print(f"å¤„ç†é¡¹ç›® {item_idx}, æ®µè½ {paragraph_idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'item_idx': item_idx,
                'paragraph_idx': paragraph_idx,
                'paragraph': passage_data['paragraph'],
                'title': self.DEFAULT_TITLE,
                'success': False
            }

    def generate_title_for_ctx(self, passage_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸ºå•ä¸ªctxç”Ÿæˆæ ‡é¢˜

        Args:
            passage_data: åŒ…å«æ®µè½ä¿¡æ¯çš„å­—å…¸

        Returns:
            åŒ…å«æ ‡é¢˜çš„ç»“æœå­—å…¸
        """
        try:
            item_idx = passage_data['item_idx']
            ctx_idx = passage_data['ctx_idx']
            paragraph = passage_data['paragraph']

            # ç”Ÿæˆæ ‡é¢˜
            title = self.call_api_with_retry(paragraph)

            with self.progress_lock:
                self.completed_count += 1
                print(
                    f"æ ‡é¢˜ç”Ÿæˆè¿›åº¦: {self.completed_count}/{self.total_count} - é¡¹ç›® {item_idx + 1}, ctx {ctx_idx + 1}")

            return {
                'item_idx': item_idx,
                'ctx_idx': ctx_idx,
                'paragraph': paragraph,
                'title': title,
                'success': True
            }

        except Exception as e:
            print(f"å¤„ç†é¡¹ç›® {item_idx}, ctx {ctx_idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'item_idx': item_idx,
                'ctx_idx': ctx_idx,
                'paragraph': paragraph,
                'title': self.DEFAULT_TITLE,
                'success': False
            }

    def collect_all_paragraphs(self, dataset: List[Dict]) -> List[Dict[str, Any]]:
        """
        æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„æ®µè½æ•°æ®

        Args:
            dataset: æ•°æ®é›†

        Returns:
            æ‰€æœ‰æ®µè½çš„æ•°æ®åˆ—è¡¨
        """
        all_paragraphs_data = []

        for item_idx, item in enumerate(dataset):
            # æ£€æŸ¥æ˜¯å¦æœ‰ori_fakeå­—æ®µ
            if 'ori_fake' in item and isinstance(item['ori_fake'], list):
                for paragraph_idx, paragraph in enumerate(item['ori_fake']):
                    if paragraph.strip():
                        all_paragraphs_data.append({
                            'item_idx': item_idx,
                            'paragraph_idx': paragraph_idx,
                            'paragraph': paragraph
                        })

        return all_paragraphs_data

    def collect_default_title_paragraphs(self, dataset: List[Dict]) -> List[Dict[str, Any]]:
        """
        æ”¶é›†å…·æœ‰é»˜è®¤æ ‡é¢˜æˆ–ç©ºæ ‡é¢˜çš„æ®µè½æ•°æ®

        Args:
            dataset: æ•°æ®é›†

        Returns:
            éœ€è¦é‡æ–°ç”Ÿæˆæ ‡é¢˜çš„æ®µè½æ•°æ®åˆ—è¡¨
        """
        paragraphs_data = []

        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx_idx, ctx in enumerate(item['ctxs']):
                    if 'title' in ctx and 'text' in ctx:
                        title = ctx['title']
                        if title == self.DEFAULT_TITLE or title.strip() == self.EMPTY_TITLE:
                            paragraphs_data.append({
                                'item_idx': item_idx,
                                'ctx_idx': ctx_idx,
                                'paragraph': ctx['text']
                            })

        return paragraphs_data

    def collect_missing_title_paragraphs(self, dataset: List[Dict]) -> List[Dict[str, Any]]:
        """
        æ”¶é›†ç¼ºå¤±æ ‡é¢˜å­—æ®µçš„æ®µè½æ•°æ®

        Args:
            dataset: æ•°æ®é›†

        Returns:
            éœ€è¦æ·»åŠ æ ‡é¢˜çš„æ®µè½æ•°æ®åˆ—è¡¨
        """
        paragraphs_data = []

        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx_idx, ctx in enumerate(item['ctxs']):
                    if 'title' not in ctx and 'text' in ctx:
                        paragraphs_data.append({
                            'item_idx': item_idx,
                            'ctx_idx': ctx_idx,
                            'paragraph': ctx['text']
                        })

        return paragraphs_data

    def stage1_generate_all_titles(self, dataset: List[Dict], output_file: str) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡ç”Ÿæˆæ‰€æœ‰æ ‡é¢˜

        Args:
            dataset: æ•°æ®é›†
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            æ‰€æœ‰æ ‡é¢˜ç”Ÿæˆç»“æœ
        """
        print("=" * 80)
        print("ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡ç”Ÿæˆæ‰€æœ‰æ ‡é¢˜")
        print("=" * 80)

        # æ”¶é›†æ‰€æœ‰æ®µè½
        all_paragraphs_data = self.collect_all_paragraphs(dataset)
        self.total_count = len(all_paragraphs_data)
        self.completed_count = 0

        print(f"æ€»å…±éœ€è¦ç”Ÿæˆ {self.total_count} ä¸ªæ ‡é¢˜")

        if self.total_count == 0:
            print("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ®µè½")
            return []

        # ä½¿ç”¨å¤šçº¿ç¨‹ç”Ÿæˆæ ‡é¢˜
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_paragraph = {
                executor.submit(self.generate_titles_only, paragraph_data): paragraph_data
                for paragraph_data in all_paragraphs_data
            }

            for future in concurrent.futures.as_completed(future_to_paragraph):
                result = future.result()
                results.append(result)

        success_count = sum(1 for r in results if r['success'])
        print(f"æ ‡é¢˜ç”Ÿæˆå®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")

        # ä¿å­˜æ ‡é¢˜ç”Ÿæˆç»“æœ
        self.save_titles_results(results, output_file, "stage1_titles")

        return results

    def stage1_generate_titles_for_ctxs(self, paragraphs_data: List[Dict[str, Any]], output_file: str) -> List[Dict[str, Any]]:
        """
        ç¬¬ä¸€é˜¶æ®µï¼šä¸ºç°æœ‰ctxsç”Ÿæˆæ ‡é¢˜

        Args:
            paragraphs_data: æ®µè½æ•°æ®åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            æ ‡é¢˜ç”Ÿæˆç»“æœ
        """
        print("=" * 80)
        print("ç¬¬ä¸€é˜¶æ®µï¼šä¸ºç°æœ‰ctxsç”Ÿæˆæ ‡é¢˜")
        print("=" * 80)

        self.total_count = len(paragraphs_data)
        self.completed_count = 0

        print(f"æ€»å…±éœ€è¦ç”Ÿæˆ {self.total_count} ä¸ªæ ‡é¢˜")

        if self.total_count == 0:
            print("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ®µè½")
            return []

        # ä½¿ç”¨å¤šçº¿ç¨‹ç”Ÿæˆæ ‡é¢˜
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_paragraph = {
                executor.submit(self.generate_title_for_ctx, paragraph_data): paragraph_data
                for paragraph_data in paragraphs_data
            }

            for future in concurrent.futures.as_completed(future_to_paragraph):
                result = future.result()
                results.append(result)

        success_count = sum(1 for r in results if r['success'])
        print(f"æ ‡é¢˜ç”Ÿæˆå®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")

        return results

    def save_titles_results(self, titles_results: List[Dict], output_file: str, stage: str):
        """
        ä¿å­˜æ ‡é¢˜ç”Ÿæˆç»“æœ

        Args:
            titles_results: æ ‡é¢˜ç”Ÿæˆç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            stage: é˜¶æ®µæ ‡è¯†
        """
        try:
            temp_file = f"{output_file}.{stage}.json"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(titles_results, f, ensure_ascii=False, indent=2)
            print(f"{stage}é˜¶æ®µæ ‡é¢˜ç»“æœå·²ä¿å­˜åˆ°: {temp_file}")
        except Exception as e:
            print(f"ä¿å­˜{stage}é˜¶æ®µæ ‡é¢˜ç»“æœå¤±è´¥: {e}")

    def stage2_calculate_similarities(self, titles_results: List[Dict], dataset_name: str, output_file: str) -> List[Dict]:
        """
        ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦

        Args:
            titles_results: æ ‡é¢˜ç”Ÿæˆç»“æœ
            dataset_name: æ•°æ®é›†åç§°
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«ç›¸ä¼¼åº¦çš„ç»“æœ
        """
        print("=" * 80)
        print("ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦")
        print("=" * 80)

        if not titles_results:
            print("æ²¡æœ‰æ ‡é¢˜ç»“æœéœ€è¦å¤„ç†")
            return []

        # è·å–demonstration embeddings
        dataset_demonstrations = self.get_dataset_demonstrations(dataset_name)
        demonstration_texts = ["title: {} text: {}".format(demo["title"], demo["text"]) for demo in
                               dataset_demonstrations]

        print(f"æ­£åœ¨è®¡ç®—demonstration embeddings...")
        demonstration_embeddings = get_e5_mistral_embeddings_for_document(
            doc_list=demonstration_texts,
            max_length=256,
            batch_size=4,
        )

        # æ„å»ºæ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
        document_texts = []
        for result in titles_results:
            if result['success']:
                document_text = f"title: {result['title']} text: {result['paragraph']}"
                document_texts.append(document_text)
            else:
                document_texts.append("")  # å ä½ç¬¦

        print(f"æ­£åœ¨è®¡ç®— {len(document_texts)} ä¸ªæ–‡æ¡£çš„embeddings...")

        # åˆ†æ‰¹å¤„ç†é¿å…æ˜¾å­˜æº¢å‡º
        batch_size = 13  # å¯æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´
        all_similarities = []

        for i in range(0, len(document_texts), batch_size):
            batch_texts = document_texts[i:i + batch_size]
            # è¿‡æ»¤æ‰ç©ºæ–‡æœ¬
            valid_texts = [text for text in batch_texts if text.strip()]

            if valid_texts:
                print(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(document_texts) + batch_size - 1) // batch_size}")

                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„åµŒå…¥å‘é‡
                documents_embeddings = get_e5_mistral_embeddings_for_query(
                    "retrieve_semantically_similar_text",
                    query_list=valid_texts,
                    max_length=256,
                    batch_size=4,
                )

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = torch.matmul(documents_embeddings, demonstration_embeddings.T)
                demonstration_ranks = torch.argsort(similarities, dim=1, descending=True)

                # æ·»åŠ åˆ°æ€»ç»“æœä¸­ï¼Œå¤„ç†ç©ºæ–‡æœ¬çš„æƒ…å†µ
                valid_idx = 0
                for j, text in enumerate(batch_texts):
                    if text.strip():
                        all_similarities.append(demonstration_ranks[valid_idx].tolist())
                        valid_idx += 1
                    else:
                        all_similarities.append([])  # ç©ºæ–‡æœ¬çš„å ä½ç¬¦

                # æ¸…ç†æ˜¾å­˜
                del documents_embeddings, similarities, demonstration_ranks
                torch.cuda.empty_cache() if torch.cuda.is_available() else None

        # å°†ç›¸ä¼¼åº¦ç»“æœæ·»åŠ åˆ°titles_resultsä¸­
        enhanced_results = []
        for i, result in enumerate(titles_results):
            enhanced_result = result.copy()
            if i < len(all_similarities):
                enhanced_result['ranked_prompt_indices'] = all_similarities[i]
            else:
                enhanced_result['ranked_prompt_indices'] = []
            enhanced_results.append(enhanced_result)

        print(f"ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")

        # ä¿å­˜åŒ…å«ç›¸ä¼¼åº¦çš„ç»“æœ
        self.save_titles_results(enhanced_results, output_file, "stage2_similarities")

        return enhanced_results

    def stage3_apply_to_dataset(self, enhanced_results: List[Dict], dataset: List[Dict]) -> List[Dict]:
        """
        ç¬¬ä¸‰é˜¶æ®µï¼šå°†ç»“æœåº”ç”¨åˆ°æ•°æ®é›†

        Args:
            enhanced_results: åŒ…å«æ ‡é¢˜å’Œç›¸ä¼¼åº¦çš„ç»“æœ
            dataset: åŸå§‹æ•°æ®é›†

        Returns:
            æ›´æ–°åçš„æ•°æ®é›†
        """
        print("=" * 80)
        print("ç¬¬ä¸‰é˜¶æ®µï¼šå°†ç»“æœåº”ç”¨åˆ°æ•°æ®é›†")
        print("=" * 80)

        if not enhanced_results:
            print("æ²¡æœ‰ç»“æœéœ€è¦åº”ç”¨")
            return dataset

        # æŒ‰item_idxåˆ†ç»„ç»“æœ
        results_by_item = {}
        for result in enhanced_results:
            item_idx = result['item_idx']
            if item_idx not in results_by_item:
                results_by_item[item_idx] = []
            results_by_item[item_idx].append(result)

        # åº”ç”¨ç»“æœåˆ°æ•°æ®é›†
        processed_items = 0
        for item_idx, item_results in results_by_item.items():
            if item_idx < len(dataset):
                existing_ctxs = dataset[item_idx].get('ctxs', [])

                # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºctxå¯¹è±¡
                for result in item_results:
                    if result['success'] and result.get('ranked_prompt_indices'):
                        # ç”Ÿæˆå”¯ä¸€ID
                        new_id = len(existing_ctxs)

                        # æ‹†åˆ†å¥å­
                        sentences = self.split_sentences(result['paragraph'])

                        # æ„é€ æ–°çš„ctxå¯¹è±¡
                        new_ctx = {
                            "id": str(new_id),
                            "title": result['title'],
                            "text": result['paragraph'],
                            "sentences": sentences,
                            "ranked_prompt_indices": result['ranked_prompt_indices']
                        }

                        existing_ctxs.append(new_ctx)

                # æ›´æ–°æ•°æ®é›†
                dataset[item_idx]['ctxs'] = existing_ctxs
                processed_items += 1

        print(f"æ•°æ®é›†æ›´æ–°å®Œæˆï¼Œå¤„ç†äº† {processed_items} ä¸ªé¡¹ç›®")
        return dataset

    def stage3_apply_to_existing_ctxs(self, enhanced_results: List[Dict], dataset: List[Dict]) -> List[Dict]:
        """
        ç¬¬ä¸‰é˜¶æ®µï¼šå°†æ ‡é¢˜å’Œç›¸ä¼¼åº¦ç»“æœåº”ç”¨åˆ°ç°æœ‰ctxs

        Args:
            enhanced_results: åŒ…å«æ ‡é¢˜å’Œç›¸ä¼¼åº¦çš„ç»“æœ
            dataset: æ•°æ®é›†

        Returns:
            æ›´æ–°åçš„æ•°æ®é›†
        """
        print("=" * 80)
        print("ç¬¬ä¸‰é˜¶æ®µï¼šå°†æ ‡é¢˜å’Œç›¸ä¼¼åº¦åº”ç”¨åˆ°ç°æœ‰ctxs")
        print("=" * 80)

        processed_count = 0
        for result in enhanced_results:
            if result['success'] and result.get('ranked_prompt_indices'):
                item_idx = result['item_idx']
                ctx_idx = result['ctx_idx']
                title = result['title']
                ranked_prompt_indices = result['ranked_prompt_indices']

                try:
                    if item_idx < len(dataset) and 'ctxs' in dataset[item_idx]:
                        ctxs = dataset[item_idx]['ctxs']
                        if ctx_idx < len(ctxs):
                            ctxs[ctx_idx]['title'] = title
                            ctxs[ctx_idx]['ranked_prompt_indices'] = ranked_prompt_indices
                            processed_count += 1
                except (IndexError, KeyError) as e:
                    print(f"åº”ç”¨ç»“æœåˆ°é¡¹ç›® {item_idx}, ctx {ctx_idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

        print(f"æ ‡é¢˜å’Œç›¸ä¼¼åº¦åº”ç”¨å®Œæˆï¼Œå¤„ç†äº† {processed_count} ä¸ªctx")
        return dataset

    def check_default_or_empty_titles(self, dataset: List[Dict]) -> List[Dict]:
        """
        æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰é»˜è®¤å€¼æˆ–ç©ºå€¼çš„titleï¼Œå¹¶æå–å‡ºæ¥

        Args:
            dataset: æ•°æ®é›†

        Returns:
            åŒ…å«é»˜è®¤å€¼æˆ–ç©ºå€¼çš„é¡¹ç›®æ•°æ®
        """
        failed_items = []

        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                has_default_or_empty = False
                for ctx in item['ctxs']:
                    if 'title' in ctx:
                        title = ctx['title']
                        if title == self.DEFAULT_TITLE or title.strip() == self.EMPTY_TITLE:
                            has_default_or_empty = True
                            break

                if has_default_or_empty:
                    failed_items.append({
                        'item_idx': item_idx,
                        'item': item,
                        'dataset': 'hotpotqa'  # é»˜è®¤æ•°æ®é›†
                    })

        return failed_items

    def count_default_or_empty_titles(self, dataset: List[Dict]) -> Tuple[int, int]:
        """
        ç»Ÿè®¡é»˜è®¤å€¼æˆ–ç©ºå€¼çš„æ•°é‡

        Args:
            dataset: æ•°æ®é›†

        Returns:
            (items_with_issues, total_titles_with_issues): æœ‰é—®é¢˜çš„æ¡ç›®æ•°é‡å’Œæ ‡é¢˜æ•°é‡
        """
        items_with_issues = 0
        total_titles_with_issues = 0

        for item in dataset:
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                item_has_issues = False
                for ctx in item['ctxs']:
                    if 'title' in ctx:
                        title = ctx['title']
                        if title == self.DEFAULT_TITLE or title.strip() == self.EMPTY_TITLE:
                            total_titles_with_issues += 1
                            item_has_issues = True

                if item_has_issues:
                    items_with_issues += 1

        return items_with_issues, total_titles_with_issues

    def check_missing_title_fields(self, dataset: List[Dict]) -> List[Dict]:
        """
        æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ç¼ºå¤±titleå­—æ®µçš„ctxï¼Œå¹¶æå–å‡ºæ¥

        Args:
            dataset: æ•°æ®é›†

        Returns:
            ç¼ºå¤±titleå­—æ®µçš„é¡¹ç›®æ•°æ®
        """
        missing_items = []

        for item_idx, item in enumerate(dataset):
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                has_missing_title = False
                for ctx in item['ctxs']:
                    if 'title' not in ctx:
                        has_missing_title = True
                        break

                if has_missing_title:
                    missing_items.append({
                        'item_idx': item_idx,
                        'item': item,
                        'dataset': 'hotpotqa'  # é»˜è®¤æ•°æ®é›†
                    })

        return missing_items

    def count_missing_title_fields(self, dataset: List[Dict]) -> int:
        """
        ç»Ÿè®¡ç¼ºå¤±titleå­—æ®µçš„æ•°é‡

        Args:
            dataset: æ•°æ®é›†

        Returns:
            ç¼ºå¤±titleå­—æ®µçš„ctxæ•°é‡
        """
        missing_count = 0

        for item in dataset:
            if 'ctxs' in item and isinstance(item['ctxs'], list):
                for ctx in item['ctxs']:
                    if 'title' not in ctx:
                        missing_count += 1

        return missing_count

    def process_default_title_check(self, input_file: str, output_file: str, dataset_name: str = "hotpotqa"):
        """
        æ‰§è¡Œé»˜è®¤æ ‡é¢˜æ£€æŸ¥å¹¶ä¿®å¤ï¼ˆå®Œæ•´ä¸‰é˜¶æ®µå¤„ç†ï¼‰

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            dataset_name: æ•°æ®é›†åç§°
        """
        print("ğŸ” æ‰§è¡Œé»˜è®¤æ ‡é¢˜æ£€æŸ¥æ¨¡å¼ï¼ˆå®Œæ•´ä¸‰é˜¶æ®µå¤„ç†ï¼‰")
        print("=" * 80)

        # è¯»å–è¾“å…¥æ–‡ä»¶
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
        except Exception as e:
            print(f"è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return

        if not isinstance(dataset, list):
            dataset = [dataset]

        # ç»Ÿè®¡é»˜è®¤æ ‡é¢˜æ•°é‡
        items_count, titles_count = self.count_default_or_empty_titles(dataset)
        print(f"å‘ç°é»˜è®¤æˆ–ç©ºæ ‡é¢˜: {items_count} ä¸ªé¡¹ç›®, {titles_count} ä¸ªæ ‡é¢˜")

        if titles_count == 0:
            print("âœ… æ²¡æœ‰å‘ç°é»˜è®¤æˆ–ç©ºæ ‡é¢˜ï¼Œæ— éœ€å¤„ç†")
            return

        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†éœ€è¦é‡æ–°ç”Ÿæˆæ ‡é¢˜çš„æ®µè½å¹¶ç”Ÿæˆæ ‡é¢˜
        paragraphs_data = self.collect_default_title_paragraphs(dataset)
        print(f"æ”¶é›†åˆ° {len(paragraphs_data)} ä¸ªéœ€è¦é‡æ–°ç”Ÿæˆæ ‡é¢˜çš„æ®µè½")

        titles_results = self.stage1_generate_titles_for_ctxs(paragraphs_data, output_file)

        # ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—ç›¸ä¼¼åº¦
        enhanced_results = self.stage2_calculate_similarities(titles_results, dataset_name, output_file)

        # ç¬¬ä¸‰é˜¶æ®µï¼šåº”ç”¨åˆ°æ•°æ®é›†
        updated_dataset = self.stage3_apply_to_existing_ctxs(enhanced_results, dataset)

        # ä¿å­˜ç»“æœ
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(updated_dataset, f, ensure_ascii=False, indent=2)
            print(f"âœ… é»˜è®¤æ ‡é¢˜æ£€æŸ¥å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import os
            for stage in ["stage1_titles", "stage2_similarities"]:
                temp_file = f"{output_file}.{stage}.json"
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    print(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")

            # æœ€ç»ˆç»Ÿè®¡
            final_items_count, final_titles_count = self.count_default_or_empty_titles(updated_dataset)
            print(f"ğŸ å¤„ç†åç»Ÿè®¡:")
            print(f"   - å‰©ä½™é»˜è®¤/ç©ºæ ‡é¢˜: {final_items_count} ä¸ªé¡¹ç›®, {final_titles_count} ä¸ªæ ‡é¢˜")
            print(f"   - ä¿®å¤æˆåŠŸ: {titles_count - final_titles_count} ä¸ªæ ‡é¢˜")

        except Exception as e:
            print(f"ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

    def process_missing_title_check(self, input_file: str, output_file: str, dataset_name: str = "hotpotqa"):
        """
        æ‰§è¡Œç¼ºå¤±æ ‡é¢˜æ£€æŸ¥å¹¶ä¿®å¤ï¼ˆå®Œæ•´ä¸‰é˜¶æ®µå¤„ç†ï¼‰

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            dataset_name: æ•°æ®é›†åç§°
        """
        print("ğŸ” æ‰§è¡Œç¼ºå¤±æ ‡é¢˜æ£€æŸ¥æ¨¡å¼ï¼ˆå®Œæ•´ä¸‰é˜¶æ®µå¤„ç†ï¼‰")
        print("=" * 80)

        # è¯»å–è¾“å…¥æ–‡ä»¶
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
        except Exception as e:
            print(f"è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return

        if not isinstance(dataset, list):
            dataset = [dataset]

        # ç»Ÿè®¡ç¼ºå¤±æ ‡é¢˜æ•°é‡
        missing_count = self.count_missing_title_fields(dataset)
        print(f"å‘ç°ç¼ºå¤±æ ‡é¢˜å­—æ®µ: {missing_count} ä¸ªctx")

        if missing_count == 0:
            print("âœ… æ²¡æœ‰å‘ç°ç¼ºå¤±æ ‡é¢˜å­—æ®µï¼Œæ— éœ€å¤„ç†")
            return

        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†éœ€è¦æ·»åŠ æ ‡é¢˜çš„æ®µè½å¹¶ç”Ÿæˆæ ‡é¢˜
        paragraphs_data = self.collect_missing_title_paragraphs(dataset)
        print(f"æ”¶é›†åˆ° {len(paragraphs_data)} ä¸ªéœ€è¦æ·»åŠ æ ‡é¢˜çš„æ®µè½")

        titles_results = self.stage1_generate_titles_for_ctxs(paragraphs_data, output_file)

        # ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—ç›¸ä¼¼åº¦
        enhanced_results = self.stage2_calculate_similarities(titles_results, dataset_name, output_file)

        # ç¬¬ä¸‰é˜¶æ®µï¼šåº”ç”¨åˆ°æ•°æ®é›†
        updated_dataset = self.stage3_apply_to_existing_ctxs(enhanced_results, dataset)

        # ä¿å­˜ç»“æœ
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(updated_dataset, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç¼ºå¤±æ ‡é¢˜æ£€æŸ¥å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import os
            for stage in ["stage1_titles", "stage2_similarities"]:
                temp_file = f"{output_file}.{stage}.json"
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    print(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")

            # æœ€ç»ˆç»Ÿè®¡
            final_missing_count = self.count_missing_title_fields(updated_dataset)
            print(f"ğŸ å¤„ç†åç»Ÿè®¡:")
            print(f"   - å‰©ä½™ç¼ºå¤±æ ‡é¢˜å­—æ®µ: {final_missing_count} ä¸ªctx")
            print(f"   - æ·»åŠ æˆåŠŸ: {missing_count - final_missing_count} ä¸ªæ ‡é¢˜")

        except Exception as e:
            print(f"ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

    def process_dataset_optimized_separated(self, input_file: str, output_file: str, dataset_name: str = "hotpotqa"):
        """
        å®Œæ•´çš„ä¸‰é˜¶æ®µæ•°æ®é›†å¤„ç†

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            dataset_name: æ•°æ®é›†åç§°
        """
        print(f"å¼€å§‹å®Œæ•´çš„ä¸‰é˜¶æ®µå¤„ç†æ•°æ®é›†: {input_file}")
        print("ğŸš€ æ‰§è¡Œå®Œæ•´çš„ä¸‰é˜¶æ®µå¤„ç†")
        print("   ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡ç”Ÿæˆæ‰€æœ‰æ ‡é¢˜")
        print("   ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦")
        print("   ç¬¬ä¸‰é˜¶æ®µï¼šåº”ç”¨ç»“æœåˆ°æ•°æ®é›†")

        # è¯»å–è¾“å…¥æ–‡ä»¶
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
        except Exception as e:
            print(f"è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return

        if not isinstance(dataset, list):
            dataset = [dataset]

        # æ‰§è¡Œç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆæ ‡é¢˜
        titles_results = self.stage1_generate_all_titles(dataset, output_file)

        # æ‰§è¡Œç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—ç›¸ä¼¼åº¦
        enhanced_results = self.stage2_calculate_similarities(titles_results, dataset_name, output_file)

        # æ‰§è¡Œç¬¬ä¸‰é˜¶æ®µï¼šåº”ç”¨åˆ°æ•°æ®é›†
        updated_dataset = self.stage3_apply_to_dataset(enhanced_results, dataset)

        # ä¿å­˜æœ€ç»ˆç»“æœ
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(updated_dataset, f, ensure_ascii=False, indent=2)
            print(f"âœ… å®Œæ•´å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import os
            for stage in ["stage1_titles", "stage2_similarities"]:
                temp_file = f"{output_file}.{stage}.json"
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    print(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")

            # æœ€ç»ˆç»Ÿè®¡
            default_items_count, default_titles_count = self.count_default_or_empty_titles(updated_dataset)
            missing_count = self.count_missing_title_fields(updated_dataset)
            print(f"ğŸ æœ€ç»ˆç»Ÿè®¡:")
            print(f"   - å‰©ä½™é—®é¢˜é¡¹ç›®: {default_items_count} ä¸ª, é—®é¢˜æ ‡é¢˜: {default_titles_count} ä¸ª")
            print(f"   - å‰©ä½™ç¼ºå¤±titleå­—æ®µ: {missing_count} ä¸ª")

        except Exception as e:
            print(f"ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

    def save_progress(self, dataset: List[Dict], output_file: str, stage: str):
        """
        ä¿å­˜ä¸­é—´è¿›åº¦
        """
        try:
            temp_file = f"{output_file}.{stage}.tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            print(f"{stage}é˜¶æ®µè¿›åº¦å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_file}")
        except Exception as e:
            print(f"ä¿å­˜{stage}é˜¶æ®µè¿›åº¦å¤±è´¥: {e}")


def main():
    """
    ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹
    """
    # é…ç½®å‚æ•°
    API_KEY = "05748176082447f483438dfd914cc299.NcsCifhTarCch6es"  # è¯·å¡«å†™æ‚¨çš„ZhipuAI API Key
    INPUT_FILE = "/home/jiangjp/trace-idea/data/2wikimultihopqa/wiki_test1000_add_orifake.json"
    OUTPUT_FILE = "/home/jiangjp/trace-idea/data/2wikimultihopqa/wiki_test1000_add_ctxs.json"

    # å¹¶è¡Œå¤„ç†å‚æ•°
    MAX_WORKERS = 3000  # å¹¶å‘çº¿ç¨‹æ•°ï¼Œæ ¹æ®APIé™åˆ¶è°ƒæ•´
    DATASET_NAME = "2wikimultihopqa"  # æ•°æ®é›†åç§°

    # â­â­ æ£€æŸ¥æ¨¡å¼æ§åˆ¶å‚æ•°
    CHECK_DEFAULT_TITLES = False  # è®¾ç½®ä¸ºTrueè¡¨ç¤ºæ‰§è¡Œé»˜è®¤æ ‡é¢˜æ£€æŸ¥æ¨¡å¼
    CHECK_MISSING_TITLES = False  # è®¾ç½®ä¸ºTrueè¡¨ç¤ºæ‰§è¡Œç¼ºå¤±æ ‡é¢˜æ£€æŸ¥æ¨¡å¼

    # å‚æ•°è¯´æ˜ï¼š
    # 1. CHECK_DEFAULT_TITLES=True: æ£€æŸ¥å¹¶ä¿®å¤é»˜è®¤æˆ–ç©ºæ ‡é¢˜ï¼ˆå®Œæ•´ä¸‰é˜¶æ®µå¤„ç†ï¼‰
    # 2. CHECK_MISSING_TITLES=True: æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„æ ‡é¢˜å­—æ®µï¼ˆå®Œæ•´ä¸‰é˜¶æ®µå¤„ç†ï¼‰
    # 3. æ‰€æœ‰æ£€æŸ¥å‚æ•°éƒ½ä¸ºFalse: æ‰§è¡Œå®Œæ•´çš„ä¸‰é˜¶æ®µå¤„ç†

    if not API_KEY:
        print("é”™è¯¯ï¼šè¯·å…ˆè®¾ç½®æ‚¨çš„ZhipuAI API Key")
        return

    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    generator = OptimizedTitleGenerator(API_KEY, max_workers=MAX_WORKERS)

    # æ ¹æ®å‚æ•°å†³å®šæ‰§è¡Œæ¨¡å¼
    if CHECK_DEFAULT_TITLES:
        print("ğŸ” å¯ç”¨é»˜è®¤æ ‡é¢˜æ£€æŸ¥æ¨¡å¼ï¼šæ£€æŸ¥å¹¶ä¿®å¤é»˜è®¤æˆ–ç©ºæ ‡é¢˜ï¼ˆå®Œæ•´ä¸‰é˜¶æ®µå¤„ç†ï¼‰")
        generator.process_default_title_check(INPUT_FILE, OUTPUT_FILE, DATASET_NAME)
    elif CHECK_MISSING_TITLES:
        print("ğŸ” å¯ç”¨ç¼ºå¤±æ ‡é¢˜æ£€æŸ¥æ¨¡å¼ï¼šæ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„æ ‡é¢˜å­—æ®µï¼ˆå®Œæ•´ä¸‰é˜¶æ®µå¤„ç†ï¼‰")
        generator.process_missing_title_check(INPUT_FILE, OUTPUT_FILE, DATASET_NAME)
    else:
        print("ğŸš€ å¯ç”¨å®Œæ•´çš„ä¸‰é˜¶æ®µå¤„ç†æ¨¡å¼")
        generator.process_dataset_optimized_separated(INPUT_FILE, OUTPUT_FILE, DATASET_NAME)

    print(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
    print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")

    print("\n" + "=" * 80)
    print("å¤„ç†å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()
