import json
import time
import concurrent.futures
from threading import Lock
from zhipuai import ZhipuAI
from typing import List, Dict, Any, Tuple


class OptimizedFakeNewsGenerator:
    def __init__(self, api_key: str, max_workers: int = 10):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆå‡æ–°é—»ç”Ÿæˆå™¨

        Args:
            api_key: ZhipuAIçš„APIå¯†é’¥
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        """
        self.client = ZhipuAI(api_key=api_key)
        self.max_workers = max_workers
        self.progress_lock = Lock()
        self.completed_count = 0
        self.total_count = 0

        # é»˜è®¤æ ‡è¯†
        self.DEFAULT_FAKE_NEWS = "DEFAULT_FAKE_NEWS_PLACEHOLDER"
        self.EMPTY_FAKE_NEWS = ""

        # æœ€å°é…ç½®å€¼
        self.MIN_WORKERS = 1

        # å‡æ–°é—»ç”Ÿæˆçš„æŒ‡ä»¤æ¨¡æ¿
        self.fake_news_instruction = """Please generate a text that is logical and supports the statement: The answer to "{Question}" is "{Wrong_answer}", rather than "{Correct_answer}". The generated text should be in the following form: CNN news. The generated text should be less than 200 words. Just output the generated text , and do not output anything else. Generated Text:CNN News:
"""

    def generate_fake_news_single(self, question: str, correct_answer: str, wrong_answer: str) -> str:
        """
        ä¸ºå•ä¸ªé—®é¢˜ç”Ÿæˆä¸€ä¸ªå‡æ–°é—»æ–‡æœ¬

        Args:
            question: é—®é¢˜
            correct_answer: æ­£ç¡®ç­”æ¡ˆ
            wrong_answer: é”™è¯¯ç­”æ¡ˆ

        Returns:
            ç”Ÿæˆçš„å‡æ–°é—»æ–‡æœ¬
        """
        user_input = self.fake_news_instruction.format(
            Question=question,
            Wrong_answer=wrong_answer,
            Correct_answer=correct_answer
        )

        try:
            response = self.client.chat.completions.create(
                model="glm-4-flash",
                messages=[
                    {"role": "user", "content": user_input},
                ],
                stream=True,
            )

            full_response_content = ""
            for chunk in response:
                delta = chunk.choices[0].delta
                if delta.content:
                    full_response_content += delta.content

            return full_response_content.strip()

        except Exception as e:
            print(f"ç”Ÿæˆå‡æ–°é—»å¤±è´¥: {str(e)}")
            return self.DEFAULT_FAKE_NEWS

    def call_api_with_retry(self, question: str, correct_answer: str, wrong_answer: str, max_retries: int = 3) -> str:
        """
        è°ƒç”¨APIå¹¶é‡è¯•çš„æ–¹æ³•

        Args:
            question: é—®é¢˜
            correct_answer: æ­£ç¡®ç­”æ¡ˆ
            wrong_answer: é”™è¯¯ç­”æ¡ˆ
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            ç”Ÿæˆçš„å‡æ–°é—»æ–‡æœ¬
        """
        for attempt in range(max_retries):
            try:
                result = self.generate_fake_news_single(question, correct_answer, wrong_answer)
                if result != self.DEFAULT_FAKE_NEWS and result.strip():
                    return result
                else:
                    print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¾—åˆ°ç©ºæˆ–é»˜è®¤ç»“æœ")
            except Exception as e:
                print(f"APIè°ƒç”¨ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)

        print(f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼")
        return self.DEFAULT_FAKE_NEWS

    def process_single_item_three_fakes(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸ºå•ä¸ªé—®é¢˜ç”Ÿæˆä¸‰ä¸ªå‡æ–°é—»æ–‡æœ¬ï¼ˆå¤šçº¿ç¨‹å¤„ç†å•å…ƒï¼‰

        Args:
            item_data: åŒ…å«é—®é¢˜ä¿¡æ¯å’Œç´¢å¼•çš„å­—å…¸

        Returns:
            å¤„ç†ç»“æœ
        """
        item_idx = item_data['item_idx']
        item = item_data['item']

        try:
            question = item["question"]
            correct_answer = item["answers"]
            wrong_answer = item["wrong_answer"]

            # ç”Ÿæˆä¸‰ä¸ªå‡æ–°é—»æ–‡æœ¬
            ori_fake_list = []
            for j in range(3):
                fake_news = self.call_api_with_retry(question, correct_answer, wrong_answer)
                ori_fake_list.append(fake_news)

            with self.progress_lock:
                self.completed_count += 1
                short_question = question[:30] + "..." if len(question) > 30 else question
                print(f"è¿›åº¦: {self.completed_count}/{self.total_count} - å·²å®Œæˆ: {short_question}")

            return {
                'item_idx': item_idx,
                'ori_fake': ori_fake_list,
                'success': True
            }

        except Exception as e:
            print(f"å¤„ç†é—®é¢˜ {item_idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'item_idx': item_idx,
                'ori_fake': [self.DEFAULT_FAKE_NEWS] * 3,
                'success': False
            }

    def apply_results(self, dataset: List[Dict], results: List[Dict]):
        """
        å°†ç”Ÿæˆç»“æœåº”ç”¨åˆ°æ•°æ®é›†
        """
        for result in results:
            try:
                item_idx = result['item_idx']
                ori_fake = result['ori_fake']
                dataset[item_idx]['ori_fake'] = ori_fake
            except (IndexError, KeyError) as e:
                print(f"åº”ç”¨ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def save_progress(self, dataset: List[Dict], output_file: str, stage: str):
        """
        ä¿å­˜ä¸­é—´è¿›åº¦
        """
        try:
            temp_file = f"{output_file}.{stage}.tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            print(f"{stage}é˜¶æ®µè¿›åº¦å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_file}")
        except Exception as e:
            print(f"ä¿å­˜{stage}é˜¶æ®µè¿›åº¦å¤±è´¥: {e}")

    def check_default_or_empty_items(self, dataset: List[Dict]) -> List[Dict]:
        """
        æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰é»˜è®¤å€¼æˆ–ç©ºå€¼çš„ori_fakeï¼Œå¹¶æå–å‡ºæ¥

        Args:
            dataset: æ•°æ®é›†

        Returns:
            åŒ…å«é»˜è®¤å€¼æˆ–ç©ºå€¼çš„é¡¹ç›®æ•°æ®
        """
        failed_items = []

        for item_idx, item in enumerate(dataset):
            if 'ori_fake' in item and isinstance(item['ori_fake'], list):
                has_default_or_empty = False
                for fake_text in item['ori_fake']:
                    if fake_text == self.DEFAULT_FAKE_NEWS or fake_text.strip() == self.EMPTY_FAKE_NEWS:
                        has_default_or_empty = True
                        break

                if has_default_or_empty:
                    failed_items.append({
                        'item_idx': item_idx,
                        'item': item
                    })

        return failed_items

    def count_default_or_empty_items(self, dataset: List[Dict]) -> Tuple[int, int]:
        """
        ç»Ÿè®¡é»˜è®¤å€¼æˆ–ç©ºå€¼çš„æ•°é‡

        Args:
            dataset: æ•°æ®é›†

        Returns:
            (items_with_issues, total_fake_texts_with_issues): æœ‰é—®é¢˜çš„æ¡ç›®æ•°é‡å’Œå‡æ–°é—»æ–‡æœ¬æ•°é‡
        """
        items_with_issues = 0
        total_fake_texts_with_issues = 0

        for item in dataset:
            if 'ori_fake' in item and isinstance(item['ori_fake'], list):
                item_has_issues = False
                for fake_text in item['ori_fake']:
                    if fake_text == self.DEFAULT_FAKE_NEWS or fake_text.strip() == self.EMPTY_FAKE_NEWS:
                        total_fake_texts_with_issues += 1
                        item_has_issues = True

                if item_has_issues:
                    items_with_issues += 1

        return items_with_issues, total_fake_texts_with_issues

    def check_missing_ori_fake_fields(self, dataset: List[Dict]) -> List[Dict]:
        """
        æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®ï¼Œå¹¶æå–å‡ºæ¥

        Args:
            dataset: æ•°æ®é›†

        Returns:
            ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®æ•°æ®
        """
        missing_items = []

        for item_idx, item in enumerate(dataset):
            # æ£€æŸ¥æ˜¯å¦ç¼ºå¤±ori_fakeå­—æ®µæˆ–ori_fakeä¸æ˜¯åˆ—è¡¨æˆ–é•¿åº¦ä¸ä¸º3
            if ('ori_fake' not in item or
                    not isinstance(item['ori_fake'], list) or
                    len(item['ori_fake']) != 3):
                missing_items.append({
                    'item_idx': item_idx,
                    'item': item
                })

        return missing_items

    def count_missing_ori_fake_fields(self, dataset: List[Dict]) -> int:
        """
        ç»Ÿè®¡ç¼ºå¤±ori_fakeå­—æ®µçš„æ•°é‡

        Args:
            dataset: æ•°æ®é›†

        Returns:
            ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®æ•°é‡
        """
        missing_count = 0

        for item in dataset:
            if ('ori_fake' not in item or
                    not isinstance(item['ori_fake'], list) or
                    len(item['ori_fake']) != 3):
                missing_count += 1

        return missing_count

    def process_missing_ori_fake_fields_with_adaptive_config(self, dataset: List[Dict], output_file: str,
                                                             initial_workers: int):
        """
        å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®

        Args:
            dataset: æ•°æ®é›†
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            initial_workers: åˆå§‹å¹¶å‘æ•°
        """
        current_workers = initial_workers

        print(f"ğŸ” å¼€å§‹å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®...")
        print(f"å½“å‰é…ç½® - å¹¶å‘æ•°: {current_workers}")

        # æ£€æŸ¥ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®
        missing_items = self.check_missing_ori_fake_fields(dataset)
        missing_count = self.count_missing_ori_fake_fields(dataset)

        print(f"å‘ç°ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®: {missing_count} ä¸ª")

        if missing_count == 0:
            print("âœ… æ•°æ®é›†ä¸­æ²¡æœ‰ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®ï¼Œæ— éœ€å¤„ç†")
            return

        # å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®
        if missing_items:
            print(f"\nå¼€å§‹å¤„ç† {len(missing_items)} ä¸ªç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®...")
            self.process_missing_items(dataset, missing_items, current_workers)

        # ä¿å­˜å¤„ç†è¿›åº¦
        self.save_progress(dataset, output_file, "missing_fields_processed")

        # æœ€ç»ˆæ£€æŸ¥
        final_missing_count = self.count_missing_ori_fake_fields(dataset)
        print(f"ç¼ºå¤±ori_fakeå­—æ®µå¤„ç†å®Œæˆ - å‰©ä½™ç¼ºå¤±: {final_missing_count} ä¸ª")

    def process_missing_items(self, dataset: List[Dict], missing_items: List[Dict], workers: int):
        """
        å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®
        """
        print(f"ä½¿ç”¨é…ç½®å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›® - å¹¶å‘æ•°: {workers}")

        self.total_count = len(missing_items)
        self.completed_count = 0

        if self.total_count > 0:
            results = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_item = {
                    executor.submit(self.process_single_item_three_fakes, item_data): item_data
                    for item_data in missing_items
                }

                for future in concurrent.futures.as_completed(future_to_item):
                    result = future.result()
                    results.append(result)

            # åº”ç”¨ç»“æœ
            self.apply_results(dataset, results)

            success_count = sum(1 for r in results if r['success'])
            print(f"ç¼ºå¤±ori_fakeå­—æ®µé¡¹ç›®å¤„ç†å®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")

    def process_failed_items_with_adaptive_config(self, dataset: List[Dict], output_file: str,
                                                  initial_workers: int):
        """
        å¤„ç†å¤±è´¥çš„é¡¹ç›®ï¼Œå¹¶è‡ªé€‚åº”è°ƒæ•´é…ç½®å‚æ•°

        Args:
            dataset: æ•°æ®é›†
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            initial_workers: åˆå§‹å¹¶å‘æ•°
        """
        current_workers = initial_workers
        retry_round = 1

        while True:
            print(f"\n{'=' * 80}")
            print(f"ç¬¬ {retry_round} è½®é‡è¯•æ£€æŸ¥å’Œå¤„ç†")
            print(f"{'=' * 80}")

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é»˜è®¤å€¼æˆ–ç©ºå€¼
            failed_items = self.check_default_or_empty_items(dataset)
            items_count, fake_texts_count = self.count_default_or_empty_items(dataset)

            print(f"å‘ç°é—®é¢˜é¡¹ç›®: {items_count} ä¸ªï¼Œé—®é¢˜å‡æ–°é—»æ–‡æœ¬: {fake_texts_count} ä¸ª")

            if items_count == 0:
                print("ğŸ‰ æ‰€æœ‰é¡¹ç›®éƒ½å·²æˆåŠŸå¤„ç†ï¼Œæ²¡æœ‰é»˜è®¤å€¼æˆ–ç©ºå€¼ï¼")
                break

            print(f"å½“å‰é…ç½® - å¹¶å‘æ•°: {current_workers}")

            # å¤„ç†å¤±è´¥çš„é¡¹ç›®
            if failed_items:
                print(f"\nå¼€å§‹å¤„ç† {len(failed_items)} ä¸ªå¤±è´¥çš„é¡¹ç›®...")
                self.process_failed_items(dataset, failed_items, current_workers)

            # ä¿å­˜å½“å‰è¿›åº¦
            self.save_progress(dataset, output_file, f"retry_round_{retry_round}")

            # æ£€æŸ¥å¤„ç†ç»“æœ
            new_items_count, new_fake_texts_count = self.count_default_or_empty_items(dataset)
            print(f"æœ¬è½®å¤„ç†å - é—®é¢˜é¡¹ç›®: {new_items_count} ä¸ªï¼Œé—®é¢˜å‡æ–°é—»æ–‡æœ¬: {new_fake_texts_count} ä¸ª")

            # å¦‚æœè¿˜æœ‰å¤±è´¥çš„ï¼Œè°ƒæ•´é…ç½®
            if new_items_count > 0:
                current_workers = self.adjust_config(current_workers)
                print(f"è°ƒæ•´åé…ç½® - å¹¶å‘æ•°: {current_workers}")

            retry_round += 1

            # é˜²æ­¢æ— é™å¾ªç¯
            if retry_round > 10:
                print("âš ï¸ å·²è¾¾åˆ°æœ€å¤§é‡è¯•è½®æ¬¡ï¼Œåœæ­¢é‡è¯•")
                break

    def adjust_config(self, workers: int) -> int:
        """
        è°ƒæ•´é…ç½®å‚æ•°ï¼Œå‡å°å¹¶å‘æ•°

        Args:
            workers: å½“å‰å¹¶å‘æ•°

        Returns:
            è°ƒæ•´åçš„å¹¶å‘æ•°
        """
        new_workers = max(self.MIN_WORKERS, workers - 1)
        print(f"é…ç½®è°ƒæ•´: å¹¶å‘æ•° {workers}->{new_workers}")
        return new_workers

    def process_failed_items(self, dataset: List[Dict], failed_items: List[Dict], workers: int):
        """
        å¤„ç†å¤±è´¥çš„é¡¹ç›®
        """
        print(f"ä½¿ç”¨é…ç½®å¤„ç†å¤±è´¥é¡¹ç›® - å¹¶å‘æ•°: {workers}")

        self.total_count = len(failed_items)
        self.completed_count = 0

        if self.total_count > 0:
            results = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_item = {
                    executor.submit(self.process_single_item_three_fakes, item_data): item_data
                    for item_data in failed_items
                }

                for future in concurrent.futures.as_completed(future_to_item):
                    result = future.result()
                    results.append(result)

            # åº”ç”¨ç»“æœ
            self.apply_results(dataset, results)

            success_count = sum(1 for r in results if r['success'])
            print(f"å¤±è´¥é¡¹ç›®é‡å¤„ç†å®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")

    def collect_all_items(self, dataset: List[Dict]) -> List[Dict[str, Any]]:
        """
        æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„é¡¹ç›®æ•°æ®

        Args:
            dataset: æ•°æ®é›†

        Returns:
            æ‰€æœ‰é¡¹ç›®çš„æ•°æ®åˆ—è¡¨
        """
        all_items_data = []

        for item_idx, item in enumerate(dataset):
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            if ("question" in item and
                    "answers" in item and
                    "wrong_answer" in item):
                all_items_data.append({
                    'item_idx': item_idx,
                    'item': item
                })

        return all_items_data

    def process_dataset_optimized(self, input_file: str, output_file: str, retry_only: bool = False,
                                  missing_fields_only: bool = False):
        """
        ä¼˜åŒ–ç‰ˆæ•°æ®é›†å¤„ç†

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            retry_only: æ˜¯å¦ä»…æ‰§è¡Œé‡è¯•å¤±è´¥é¡¹ç›®å¤„ç†ï¼ˆè·³è¿‡åˆå§‹å¤„ç†ï¼‰
            missing_fields_only: æ˜¯å¦ä»…æ‰§è¡Œç¼ºå¤±ori_fakeå­—æ®µå¤„ç†ï¼ˆè·³è¿‡æ‰€æœ‰å…¶ä»–å¤„ç†ï¼‰
        """
        print(f"å¼€å§‹å¤„ç†æ•°æ®é›†: {input_file}")

        if missing_fields_only:
            print("ğŸ” å¯ç”¨ä»…ç¼ºå¤±å­—æ®µå¤„ç†æ¨¡å¼ï¼šè·³è¿‡æ‰€æœ‰å…¶ä»–å¤„ç†ï¼Œä»…å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®")
        elif retry_only:
            print("âš ï¸ å¯ç”¨ä»…é‡è¯•æ¨¡å¼ï¼šè·³è¿‡åˆå§‹å¤„ç†ï¼Œç›´æ¥å¤„ç†é»˜è®¤å€¼æˆ–ç©ºå€¼çš„å¤±è´¥é¡¹ç›®")
        else:
            print("ğŸ“ æ‰§è¡Œå®Œæ•´å¤„ç†ï¼šåŒ…å«åˆå§‹å¤„ç†ã€é‡è¯•å¤„ç†å’Œç¼ºå¤±å­—æ®µå¤„ç†")

        # è¯»å–è¾“å…¥æ–‡ä»¶
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
        except Exception as e:
            print(f"è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return

        if not isinstance(dataset, list):
            dataset = [dataset]

        # å¦‚æœæ˜¯ä»…ç¼ºå¤±å­—æ®µå¤„ç†æ¨¡å¼ï¼Œç›´æ¥è·³è½¬åˆ°ç¬¬ä¸‰é˜¶æ®µ
        if missing_fields_only:
            print("\n" + "=" * 60)
            print("ç›´æ¥æ‰§è¡Œï¼šå¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®")
            print("=" * 60)

            # å…ˆæ£€æŸ¥å½“å‰æ•°æ®é›†ä¸­çš„ç¼ºå¤±ori_fakeå­—æ®µæƒ…å†µ
            initial_missing_count = self.count_missing_ori_fake_fields(dataset)
            print(f"ğŸ“Š å½“å‰æ•°æ®é›†ä¸­ç¼ºå¤±ori_fakeå­—æ®µç»Ÿè®¡: {initial_missing_count} ä¸ª")

            if initial_missing_count == 0:
                print("âœ… æ•°æ®é›†ä¸­æ²¡æœ‰ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®ï¼Œæ— éœ€å¤„ç†")
            else:
                self.process_missing_ori_fake_fields_with_adaptive_config(
                    dataset, output_file, self.max_workers)

            # ä¿å­˜æœ€ç»ˆç»“æœ
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"âœ… ç¼ºå¤±å­—æ®µå¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

                # æœ€ç»ˆç»Ÿè®¡
                missing_count = self.count_missing_ori_fake_fields(dataset)
                print(f"ğŸ æœ€ç»ˆç»Ÿè®¡ - å‰©ä½™ç¼ºå¤±ori_fakeå­—æ®µ: {missing_count} ä¸ª")

            except Exception as e:
                print(f"ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

            return

        # å¦‚æœä¸æ˜¯ä»…é‡è¯•æ¨¡å¼ï¼Œæ‰§è¡Œå®Œæ•´çš„åˆå§‹å¤„ç†
        if not retry_only:
            print("=" * 60)
            print(f"ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†æ‰€æœ‰é—®é¢˜æ•°æ®ï¼ˆå¤šçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªé—®é¢˜ç”Ÿæˆ3ä¸ªå‡æ–°é—»ï¼‰")
            print("=" * 60)

            # ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†æ‰€æœ‰é—®é¢˜æ•°æ®
            all_items_data = self.collect_all_items(dataset)
            self.total_count = len(all_items_data)
            self.completed_count = 0

            print(f"æ€»å…±éœ€è¦å¤„ç† {self.total_count} ä¸ªé—®é¢˜")

            if self.total_count > 0:
                results = []
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_item = {
                        executor.submit(self.process_single_item_three_fakes, item_data): item_data
                        for item_data in all_items_data
                    }

                    for future in concurrent.futures.as_completed(future_to_item):
                        result = future.result()
                        results.append(result)

                # åº”ç”¨ç»“æœ
                self.apply_results(dataset, results)

                # ä¿å­˜åˆå§‹å¤„ç†è¿›åº¦
                self.save_progress(dataset, output_file, "initial")

                success_count = sum(1 for r in results if r['success'])
                print(f"åˆå§‹å¤„ç†å®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")

            # ä¿å­˜åˆå§‹å¤„ç†ç»“æœ
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"åˆå§‹å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                import os
                temp_file = f"{output_file}.initial.tmp"
                if os.path.exists(temp_file):
                    os.remove(temp_file)

            except Exception as e:
                print(f"ä¿å­˜è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
                return

        # ç¬¬äºŒé˜¶æ®µï¼šè‡ªé€‚åº”é‡è¯•å¤„ç†å¤±è´¥é¡¹ç›®ï¼ˆæ— è®ºæ˜¯å¦ä»…é‡è¯•æ¨¡å¼ï¼Œéƒ½ä¼šæ‰§è¡Œï¼‰
        print("\n" + "=" * 60)
        if retry_only:
            print("ç›´æ¥æ‰§è¡Œï¼šé‡è¯•å¤„ç†é»˜è®¤å€¼æˆ–ç©ºå€¼çš„å¤±è´¥é¡¹ç›®")
        else:
            print("ç¬¬äºŒé˜¶æ®µï¼šè‡ªé€‚åº”é‡è¯•å¤„ç†å¤±è´¥é¡¹ç›®")
        print("=" * 60)

        # å…ˆæ£€æŸ¥å½“å‰æ•°æ®é›†ä¸­çš„é»˜è®¤å€¼æˆ–ç©ºå€¼æƒ…å†µ
        initial_items_count, initial_fake_texts_count = self.count_default_or_empty_items(dataset)
        print(f"ğŸ“Š å½“å‰æ•°æ®é›†ä¸­é—®é¢˜é¡¹ç›®ç»Ÿè®¡ - é¡¹ç›®: {initial_items_count} ä¸ª, å‡æ–°é—»æ–‡æœ¬: {initial_fake_texts_count} ä¸ª")

        if initial_items_count == 0:
            print("âœ… æ•°æ®é›†ä¸­æ²¡æœ‰é»˜è®¤å€¼æˆ–ç©ºå€¼ï¼Œæ— éœ€é‡è¯•å¤„ç†")
        else:
            self.process_failed_items_with_adaptive_config(
                dataset, output_file, self.max_workers)

        # ç¬¬ä¸‰é˜¶æ®µï¼šå¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®
        print("\n" + "=" * 60)
        print("ç¬¬ä¸‰é˜¶æ®µï¼šå¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®")
        print("=" * 60)

        # å…ˆæ£€æŸ¥å½“å‰æ•°æ®é›†ä¸­çš„ç¼ºå¤±ori_fakeå­—æ®µæƒ…å†µ
        missing_count = self.count_missing_ori_fake_fields(dataset)
        print(f"ğŸ“Š å½“å‰æ•°æ®é›†ä¸­ç¼ºå¤±ori_fakeå­—æ®µç»Ÿè®¡: {missing_count} ä¸ª")

        if missing_count == 0:
            print("âœ… æ•°æ®é›†ä¸­æ²¡æœ‰ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®ï¼Œæ— éœ€å¤„ç†")
        else:
            self.process_missing_ori_fake_fields_with_adaptive_config(
                dataset, output_file, self.max_workers)

        # ä¿å­˜æœ€ç»ˆç»“æœ
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            print(f"âœ… æœ€ç»ˆå¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            # åˆ é™¤é‡è¯•é˜¶æ®µçš„ä¸´æ—¶æ–‡ä»¶
            import os
            for i in range(1, 11):
                temp_file = f"{output_file}.retry_round_{i}.tmp"
                if os.path.exists(temp_file):
                    os.remove(temp_file)

            # åˆ é™¤ç¼ºå¤±å­—æ®µå¤„ç†çš„ä¸´æ—¶æ–‡ä»¶
            temp_file = f"{output_file}.missing_fields_processed.tmp"
            if os.path.exists(temp_file):
                os.remove(temp_file)

            # æœ€ç»ˆç»Ÿè®¡
            default_items_count, default_fake_texts_count = self.count_default_or_empty_items(dataset)
            missing_count = self.count_missing_ori_fake_fields(dataset)
            print(f"ğŸ æœ€ç»ˆç»Ÿè®¡:")
            print(f"   - å‰©ä½™é—®é¢˜é¡¹ç›®: {default_items_count} ä¸ª, é—®é¢˜å‡æ–°é—»æ–‡æœ¬: {default_fake_texts_count} ä¸ª")
            print(f"   - å‰©ä½™ç¼ºå¤±ori_fakeå­—æ®µ: {missing_count} ä¸ª")

        except Exception as e:
            print(f"ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")


def main():
    """
    ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹
    """
    # é…ç½®å‚æ•°
    API_KEY = "05748176082447f483438dfd914cc299.NcsCifhTarCch6es"  # è¯·å¡«å†™æ‚¨çš„ZhipuAI API Key
    INPUT_FILE = "/home/jiangjp/trace-idea/data/2wikimultihopqa/wiki_test1000_add_wronganswer.json"
    OUTPUT_FILE = "/home/jiangjp/trace-idea/data/2wikimultihopqa/wiki_test1000_add_orifake.json"

    # å¹¶è¡Œå¤„ç†å‚æ•°
    MAX_WORKERS = 3000  # å¹¶å‘çº¿ç¨‹æ•°ï¼Œæ ¹æ®APIé™åˆ¶è°ƒæ•´

    # â­ æ§åˆ¶å‚æ•°ï¼šé€‰æ‹©æ‰§è¡Œæ¨¡å¼
    RETRY_ONLY = False  # è®¾ç½®ä¸ºTrueè¡¨ç¤ºä»…å¤„ç†é»˜è®¤å€¼æˆ–ç©ºå€¼çš„å¤±è´¥é¡¹ç›®
    MISSING_FIELDS_ONLY = False  # è®¾ç½®ä¸ºTrueè¡¨ç¤ºä»…å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®

    # æ³¨æ„ï¼šå¦‚æœMISSING_FIELDS_ONLY=Trueï¼Œåˆ™RETRY_ONLYçš„å€¼ä¼šè¢«å¿½ç•¥
    # ä¸‰ç§æ¨¡å¼ï¼š
    # 1. MISSING_FIELDS_ONLY=True: ä»…å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µ
    # 2. RETRY_ONLY=True, MISSING_FIELDS_ONLY=False: ä»…å¤„ç†é»˜è®¤å€¼æˆ–ç©ºå€¼çš„é¡¹ç›®
    # 3. ä¸¤è€…éƒ½ä¸ºFalse: æ‰§è¡Œå®Œæ•´æµç¨‹

    if not API_KEY:
        print("é”™è¯¯ï¼šè¯·å…ˆè®¾ç½®æ‚¨çš„ZhipuAI API Key")
        return

    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    generator = OptimizedFakeNewsGenerator(API_KEY, max_workers=MAX_WORKERS)

    # æ ¹æ®å‚æ•°æ‰§è¡Œä¸åŒçš„å¤„ç†æµç¨‹
    if MISSING_FIELDS_ONLY:
        print("ğŸ” å¯ç”¨ä»…ç¼ºå¤±å­—æ®µå¤„ç†æ¨¡å¼")
        print(f"ğŸ“‚ å°†ä»æ–‡ä»¶ {INPUT_FILE} ä¸­è¯»å–æ•°æ®ï¼Œä»…å¤„ç†ç¼ºå¤±ori_fakeå­—æ®µçš„é¡¹ç›®")
    elif RETRY_ONLY:
        print("ğŸ”„ å¯ç”¨ä»…é‡è¯•æ¨¡å¼")
        print(f"ğŸ“‚ å°†ä»æ–‡ä»¶ {INPUT_FILE} ä¸­è¯»å–æ•°æ®ï¼Œä»…å¤„ç†é»˜è®¤å€¼æˆ–ç©ºå€¼çš„é¡¹ç›®")
    else:
        print("ğŸš€ å¯ç”¨å®Œæ•´å¤„ç†æ¨¡å¼")
        print(f"ğŸ“‚ å°†å®Œæ•´å¤„ç†æ–‡ä»¶ {INPUT_FILE} ä¸­çš„æ‰€æœ‰æ•°æ®")

    # ä¼˜åŒ–å¤„ç†æ•°æ®é›†
    generator.process_dataset_optimized(
        INPUT_FILE,
        OUTPUT_FILE,
        retry_only=RETRY_ONLY,
        missing_fields_only=MISSING_FIELDS_ONLY
    )


if __name__ == "__main__":
    main()